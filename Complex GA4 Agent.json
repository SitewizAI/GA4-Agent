{"description":"Navigate the Linguistic Landscape, Discover Opportunities.","icon_bg_color":null,"updated_at":"2024-09-06T18:55:22+00:00","webhook":false,"id":"31bfacc4-3c1d-443f-8131-da5f4df72057","name":"GA4 Agent (1)","icon":null,"is_component":false,"endpoint_name":null,"data":{"nodes":[{"id":"ChatInput-ZoJNe","type":"genericNode","position":{"x":-2069.086124482587,"y":2419.255179720552},"data":{"type":"ChatInput","node":{"template":{"_type":"Component","files":{"trace_as_metadata":true,"file_path":"","fileTypes":["txt","md","mdx","csv","json","yaml","yml","xml","html","htm","pdf","docx","py","sh","sql","js","ts","tsx","jpg","jpeg","png","bmp","image"],"list":true,"required":false,"placeholder":"","show":true,"value":"","name":"files","display_name":"Files","advanced":true,"dynamic":false,"info":"Files to be sent with the message.","title_case":false,"type":"file","_input_type":"FileInput"},"code":{"type":"code","required":true,"placeholder":"","list":false,"show":true,"multiline":true,"value":"from langflow.base.data.utils import IMG_FILE_TYPES, TEXT_FILE_TYPES\nfrom langflow.base.io.chat import ChatComponent\nfrom langflow.inputs import BoolInput\nfrom langflow.io import DropdownInput, FileInput, MessageTextInput, MultilineInput, Output\nfrom langflow.memory import store_message\nfrom langflow.schema.message import Message\nfrom langflow.utils.constants import MESSAGE_SENDER_AI, MESSAGE_SENDER_USER, MESSAGE_SENDER_NAME_USER\n\n\nclass ChatInput(ChatComponent):\n    display_name = \"Chat Input\"\n    description = \"Get chat inputs from the Playground.\"\n    icon = \"ChatInput\"\n    name = \"ChatInput\"\n\n    inputs = [\n        MultilineInput(\n            name=\"input_value\",\n            display_name=\"Text\",\n            value=\"\",\n            info=\"Message to be passed as input.\",\n        ),\n        BoolInput(\n            name=\"should_store_message\",\n            display_name=\"Store Messages\",\n            info=\"Store the message in the history.\",\n            value=True,\n            advanced=True,\n        ),\n        DropdownInput(\n            name=\"sender\",\n            display_name=\"Sender Type\",\n            options=[MESSAGE_SENDER_AI, MESSAGE_SENDER_USER],\n            value=MESSAGE_SENDER_USER,\n            info=\"Type of sender.\",\n            advanced=True,\n        ),\n        MessageTextInput(\n            name=\"sender_name\",\n            display_name=\"Sender Name\",\n            info=\"Name of the sender.\",\n            value=MESSAGE_SENDER_NAME_USER,\n            advanced=True,\n        ),\n        MessageTextInput(\n            name=\"session_id\",\n            display_name=\"Session ID\",\n            info=\"The session ID of the chat. If empty, the current session ID parameter will be used.\",\n            advanced=True,\n        ),\n        FileInput(\n            name=\"files\",\n            display_name=\"Files\",\n            file_types=TEXT_FILE_TYPES + IMG_FILE_TYPES,\n            info=\"Files to be sent with the message.\",\n            advanced=True,\n            is_list=True,\n        ),\n    ]\n    outputs = [\n        Output(display_name=\"Message\", name=\"message\", method=\"message_response\"),\n    ]\n\n    def message_response(self) -> Message:\n        message = Message(\n            text=self.input_value,\n            sender=self.sender,\n            sender_name=self.sender_name,\n            session_id=self.session_id,\n            files=self.files,\n        )\n\n        if (\n            self.session_id\n            and isinstance(message, Message)\n            and isinstance(message.text, str)\n            and self.should_store_message\n        ):\n            store_message(\n                message,\n                flow_id=self.graph.flow_id,\n            )\n            self.message.value = message\n\n        self.status = message\n        return message\n","fileTypes":[],"file_path":"","password":false,"name":"code","advanced":true,"dynamic":true,"info":"","load_from_db":false,"title_case":false},"input_value":{"trace_as_input":true,"multiline":true,"trace_as_metadata":true,"load_from_db":false,"list":false,"required":false,"placeholder":"","show":true,"value":"Find a unique and actionablle insight?","name":"input_value","display_name":"Text","advanced":false,"input_types":["Message"],"dynamic":false,"info":"Message to be passed as input.","title_case":false,"type":"str","_input_type":"MultilineInput"},"sender":{"trace_as_metadata":true,"options":["Machine","User"],"combobox":false,"required":false,"placeholder":"","show":true,"value":"User","name":"sender","display_name":"Sender Type","advanced":true,"dynamic":false,"info":"Type of sender.","title_case":false,"type":"str","_input_type":"DropdownInput"},"sender_name":{"trace_as_input":true,"trace_as_metadata":true,"load_from_db":false,"list":false,"required":false,"placeholder":"","show":true,"value":"User","name":"sender_name","display_name":"Sender Name","advanced":true,"input_types":["Message"],"dynamic":false,"info":"Name of the sender.","title_case":false,"type":"str","_input_type":"MessageTextInput"},"session_id":{"trace_as_input":true,"trace_as_metadata":true,"load_from_db":false,"list":false,"required":false,"placeholder":"","show":true,"value":"","name":"session_id","display_name":"Session ID","advanced":true,"input_types":["Message"],"dynamic":false,"info":"The session ID of the chat. If empty, the current session ID parameter will be used.","title_case":false,"type":"str","_input_type":"MessageTextInput"},"should_store_message":{"trace_as_metadata":true,"list":false,"required":false,"placeholder":"","show":true,"value":true,"name":"should_store_message","display_name":"Store Messages","advanced":true,"dynamic":false,"info":"Store the message in the history.","title_case":false,"type":"bool","_input_type":"BoolInput"}},"description":"Get chat inputs from the Playground.","icon":"ChatInput","base_classes":["Message"],"display_name":"Chat Input","documentation":"","custom_fields":{},"output_types":[],"pinned":false,"conditional_paths":[],"frozen":false,"outputs":[{"types":["Message"],"selected":"Message","name":"message","display_name":"Message","method":"message_response","value":"__UNDEFINED__","cache":true}],"field_order":["input_value","should_store_message","sender","sender_name","session_id","files"],"beta":false,"edited":false,"lf_version":"1.0.17"},"id":"ChatInput-ZoJNe"},"selected":false,"width":384,"height":298,"dragging":false,"positionAbsolute":{"x":-2069.086124482587,"y":2419.255179720552}},{"id":"ChatOutput-BzHJd","type":"genericNode","position":{"x":-1302.9372617029771,"y":2414.9709300190107},"data":{"type":"ChatOutput","node":{"template":{"_type":"Component","code":{"type":"code","required":true,"placeholder":"","list":false,"show":true,"multiline":true,"value":"from langflow.base.io.chat import ChatComponent\nfrom langflow.inputs import BoolInput\nfrom langflow.io import DropdownInput, MessageTextInput, Output\nfrom langflow.memory import store_message\nfrom langflow.schema.message import Message\nfrom langflow.utils.constants import MESSAGE_SENDER_NAME_AI, MESSAGE_SENDER_USER, MESSAGE_SENDER_AI\n\n\nclass ChatOutput(ChatComponent):\n    display_name = \"Chat Output\"\n    description = \"Display a chat message in the Playground.\"\n    icon = \"ChatOutput\"\n    name = \"ChatOutput\"\n\n    inputs = [\n        MessageTextInput(\n            name=\"input_value\",\n            display_name=\"Text\",\n            info=\"Message to be passed as output.\",\n        ),\n        BoolInput(\n            name=\"should_store_message\",\n            display_name=\"Store Messages\",\n            info=\"Store the message in the history.\",\n            value=True,\n            advanced=True,\n        ),\n        DropdownInput(\n            name=\"sender\",\n            display_name=\"Sender Type\",\n            options=[MESSAGE_SENDER_AI, MESSAGE_SENDER_USER],\n            value=MESSAGE_SENDER_AI,\n            advanced=True,\n            info=\"Type of sender.\",\n        ),\n        MessageTextInput(\n            name=\"sender_name\",\n            display_name=\"Sender Name\",\n            info=\"Name of the sender.\",\n            value=MESSAGE_SENDER_NAME_AI,\n            advanced=True,\n        ),\n        MessageTextInput(\n            name=\"session_id\",\n            display_name=\"Session ID\",\n            info=\"The session ID of the chat. If empty, the current session ID parameter will be used.\",\n            advanced=True,\n        ),\n        MessageTextInput(\n            name=\"data_template\",\n            display_name=\"Data Template\",\n            value=\"{text}\",\n            advanced=True,\n            info=\"Template to convert Data to Text. If left empty, it will be dynamically set to the Data's text key.\",\n        ),\n    ]\n    outputs = [\n        Output(display_name=\"Message\", name=\"message\", method=\"message_response\"),\n    ]\n\n    def message_response(self) -> Message:\n        message = Message(\n            text=self.input_value,\n            sender=self.sender,\n            sender_name=self.sender_name,\n            session_id=self.session_id,\n        )\n        if (\n            self.session_id\n            and isinstance(message, Message)\n            and isinstance(message.text, str)\n            and self.should_store_message\n        ):\n            store_message(\n                message,\n                flow_id=self.graph.flow_id,\n            )\n            self.message.value = message\n\n        self.status = message\n        return message\n","fileTypes":[],"file_path":"","password":false,"name":"code","advanced":true,"dynamic":true,"info":"","load_from_db":false,"title_case":false},"data_template":{"trace_as_input":true,"trace_as_metadata":true,"load_from_db":false,"list":false,"required":false,"placeholder":"","show":true,"name":"data_template","value":"{text}","display_name":"Data Template","advanced":true,"input_types":["Message"],"dynamic":false,"info":"Template to convert Data to Text. If left empty, it will be dynamically set to the Data's text key.","title_case":false,"type":"str","_input_type":"MessageTextInput"},"input_value":{"trace_as_input":true,"trace_as_metadata":true,"load_from_db":false,"list":false,"required":false,"placeholder":"","show":true,"name":"input_value","value":"","display_name":"Text","advanced":false,"input_types":["Message"],"dynamic":false,"info":"Message to be passed as output.","title_case":false,"type":"str","_input_type":"MessageTextInput"},"sender":{"trace_as_metadata":true,"options":["Machine","User"],"combobox":false,"required":false,"placeholder":"","show":true,"name":"sender","value":"Machine","display_name":"Sender Type","advanced":true,"dynamic":false,"info":"Type of sender.","title_case":false,"type":"str","_input_type":"DropdownInput"},"sender_name":{"trace_as_input":true,"trace_as_metadata":true,"load_from_db":false,"list":false,"required":false,"placeholder":"","show":true,"name":"sender_name","value":"AI","display_name":"Sender Name","advanced":true,"input_types":["Message"],"dynamic":false,"info":"Name of the sender.","title_case":false,"type":"str","_input_type":"MessageTextInput"},"session_id":{"trace_as_input":true,"trace_as_metadata":true,"load_from_db":false,"list":false,"required":false,"placeholder":"","show":true,"name":"session_id","value":"","display_name":"Session ID","advanced":true,"input_types":["Message"],"dynamic":false,"info":"The session ID of the chat. If empty, the current session ID parameter will be used.","title_case":false,"type":"str","_input_type":"MessageTextInput"},"should_store_message":{"trace_as_metadata":true,"list":false,"required":false,"placeholder":"","show":true,"name":"should_store_message","value":true,"display_name":"Store Messages","advanced":true,"dynamic":false,"info":"Store the message in the history.","title_case":false,"type":"bool","_input_type":"BoolInput"}},"description":"Display a chat message in the Playground.","icon":"ChatOutput","base_classes":["Message"],"display_name":"Chat Output","documentation":"","custom_fields":{},"output_types":[],"pinned":false,"conditional_paths":[],"frozen":false,"outputs":[{"types":["Message"],"selected":"Message","name":"message","display_name":"Message","method":"message_response","value":"__UNDEFINED__","cache":true,"hidden":true}],"field_order":["input_value","should_store_message","sender","sender_name","session_id","data_template"],"beta":false,"edited":false,"lf_version":"1.0.17"},"id":"ChatOutput-BzHJd"},"selected":false,"width":384,"height":298,"positionAbsolute":{"x":-1302.9372617029771,"y":2414.9709300190107},"dragging":false},{"id":"ChatOutput-KIwTd","type":"genericNode","position":{"x":-802.8509319635515,"y":2397.090812360562},"data":{"type":"ChatOutput","node":{"template":{"_type":"Component","code":{"type":"code","required":true,"placeholder":"","list":false,"show":true,"multiline":true,"value":"from langflow.base.io.chat import ChatComponent\nfrom langflow.inputs import BoolInput\nfrom langflow.io import DropdownInput, MessageTextInput, Output\nfrom langflow.memory import store_message\nfrom langflow.schema.message import Message\nfrom langflow.utils.constants import MESSAGE_SENDER_NAME_AI, MESSAGE_SENDER_USER, MESSAGE_SENDER_AI\n\n\nclass ChatOutput(ChatComponent):\n    display_name = \"Chat Output\"\n    description = \"Display a chat message in the Playground.\"\n    icon = \"ChatOutput\"\n    name = \"ChatOutput\"\n\n    inputs = [\n        MessageTextInput(\n            name=\"input_value\",\n            display_name=\"Text\",\n            info=\"Message to be passed as output.\",\n        ),\n        BoolInput(\n            name=\"should_store_message\",\n            display_name=\"Store Messages\",\n            info=\"Store the message in the history.\",\n            value=True,\n            advanced=True,\n        ),\n        DropdownInput(\n            name=\"sender\",\n            display_name=\"Sender Type\",\n            options=[MESSAGE_SENDER_AI, MESSAGE_SENDER_USER],\n            value=MESSAGE_SENDER_AI,\n            advanced=True,\n            info=\"Type of sender.\",\n        ),\n        MessageTextInput(\n            name=\"sender_name\",\n            display_name=\"Sender Name\",\n            info=\"Name of the sender.\",\n            value=MESSAGE_SENDER_NAME_AI,\n            advanced=True,\n        ),\n        MessageTextInput(\n            name=\"session_id\",\n            display_name=\"Session ID\",\n            info=\"The session ID of the chat. If empty, the current session ID parameter will be used.\",\n            advanced=True,\n        ),\n        MessageTextInput(\n            name=\"data_template\",\n            display_name=\"Data Template\",\n            value=\"{text}\",\n            advanced=True,\n            info=\"Template to convert Data to Text. If left empty, it will be dynamically set to the Data's text key.\",\n        ),\n    ]\n    outputs = [\n        Output(display_name=\"Message\", name=\"message\", method=\"message_response\"),\n    ]\n\n    def message_response(self) -> Message:\n        message = Message(\n            text=self.input_value,\n            sender=self.sender,\n            sender_name=self.sender_name,\n            session_id=self.session_id,\n        )\n        if (\n            self.session_id\n            and isinstance(message, Message)\n            and isinstance(message.text, str)\n            and self.should_store_message\n        ):\n            store_message(\n                message,\n                flow_id=self.graph.flow_id,\n            )\n            self.message.value = message\n\n        self.status = message\n        return message\n","fileTypes":[],"file_path":"","password":false,"name":"code","advanced":true,"dynamic":true,"info":"","load_from_db":false,"title_case":false},"data_template":{"trace_as_input":true,"trace_as_metadata":true,"load_from_db":false,"list":false,"required":false,"placeholder":"","show":true,"name":"data_template","value":"{text}","display_name":"Data Template","advanced":true,"input_types":["Message"],"dynamic":false,"info":"Template to convert Data to Text. If left empty, it will be dynamically set to the Data's text key.","title_case":false,"type":"str","_input_type":"MessageTextInput"},"input_value":{"trace_as_input":true,"trace_as_metadata":true,"load_from_db":false,"list":false,"required":false,"placeholder":"","show":true,"name":"input_value","value":"","display_name":"Text","advanced":false,"input_types":["Message"],"dynamic":false,"info":"Message to be passed as output.","title_case":false,"type":"str","_input_type":"MessageTextInput"},"sender":{"trace_as_metadata":true,"options":["Machine","User"],"combobox":false,"required":false,"placeholder":"","show":true,"name":"sender","value":"Machine","display_name":"Sender Type","advanced":true,"dynamic":false,"info":"Type of sender.","title_case":false,"type":"str","_input_type":"DropdownInput"},"sender_name":{"trace_as_input":true,"trace_as_metadata":true,"load_from_db":false,"list":false,"required":false,"placeholder":"","show":true,"name":"sender_name","value":"AI","display_name":"Sender Name","advanced":true,"input_types":["Message"],"dynamic":false,"info":"Name of the sender.","title_case":false,"type":"str","_input_type":"MessageTextInput"},"session_id":{"trace_as_input":true,"trace_as_metadata":true,"load_from_db":false,"list":false,"required":false,"placeholder":"","show":true,"name":"session_id","value":"","display_name":"Session ID","advanced":true,"input_types":["Message"],"dynamic":false,"info":"The session ID of the chat. If empty, the current session ID parameter will be used.","title_case":false,"type":"str","_input_type":"MessageTextInput"},"should_store_message":{"trace_as_metadata":true,"list":false,"required":false,"placeholder":"","show":true,"name":"should_store_message","value":true,"display_name":"Store Messages","advanced":true,"dynamic":false,"info":"Store the message in the history.","title_case":false,"type":"bool","_input_type":"BoolInput"}},"description":"Display a chat message in the Playground.","icon":"ChatOutput","base_classes":["Message"],"display_name":"Chat Output","documentation":"","custom_fields":{},"output_types":[],"pinned":false,"conditional_paths":[],"frozen":false,"outputs":[{"types":["Message"],"selected":"Message","name":"message","display_name":"Message","method":"message_response","value":"__UNDEFINED__","cache":true,"hidden":true}],"field_order":["input_value","should_store_message","sender","sender_name","session_id","data_template"],"beta":false,"edited":false,"lf_version":"1.0.17"},"id":"ChatOutput-KIwTd"},"selected":false,"width":384,"height":298,"positionAbsolute":{"x":-802.8509319635515,"y":2397.090812360562},"dragging":false},{"id":"ChatOutput-mjrhT","type":"genericNode","position":{"x":-291.2587274742033,"y":2406.4714480692514},"data":{"type":"ChatOutput","node":{"template":{"_type":"Component","code":{"type":"code","required":true,"placeholder":"","list":false,"show":true,"multiline":true,"value":"from langflow.base.io.chat import ChatComponent\nfrom langflow.inputs import BoolInput\nfrom langflow.io import DropdownInput, MessageTextInput, Output\nfrom langflow.memory import store_message\nfrom langflow.schema.message import Message\nfrom langflow.utils.constants import MESSAGE_SENDER_NAME_AI, MESSAGE_SENDER_USER, MESSAGE_SENDER_AI\n\n\nclass ChatOutput(ChatComponent):\n    display_name = \"Chat Output\"\n    description = \"Display a chat message in the Playground.\"\n    icon = \"ChatOutput\"\n    name = \"ChatOutput\"\n\n    inputs = [\n        MessageTextInput(\n            name=\"input_value\",\n            display_name=\"Text\",\n            info=\"Message to be passed as output.\",\n        ),\n        BoolInput(\n            name=\"should_store_message\",\n            display_name=\"Store Messages\",\n            info=\"Store the message in the history.\",\n            value=True,\n            advanced=True,\n        ),\n        DropdownInput(\n            name=\"sender\",\n            display_name=\"Sender Type\",\n            options=[MESSAGE_SENDER_AI, MESSAGE_SENDER_USER],\n            value=MESSAGE_SENDER_AI,\n            advanced=True,\n            info=\"Type of sender.\",\n        ),\n        MessageTextInput(\n            name=\"sender_name\",\n            display_name=\"Sender Name\",\n            info=\"Name of the sender.\",\n            value=MESSAGE_SENDER_NAME_AI,\n            advanced=True,\n        ),\n        MessageTextInput(\n            name=\"session_id\",\n            display_name=\"Session ID\",\n            info=\"The session ID of the chat. If empty, the current session ID parameter will be used.\",\n            advanced=True,\n        ),\n        MessageTextInput(\n            name=\"data_template\",\n            display_name=\"Data Template\",\n            value=\"{text}\",\n            advanced=True,\n            info=\"Template to convert Data to Text. If left empty, it will be dynamically set to the Data's text key.\",\n        ),\n    ]\n    outputs = [\n        Output(display_name=\"Message\", name=\"message\", method=\"message_response\"),\n    ]\n\n    def message_response(self) -> Message:\n        message = Message(\n            text=self.input_value,\n            sender=self.sender,\n            sender_name=self.sender_name,\n            session_id=self.session_id,\n        )\n        if (\n            self.session_id\n            and isinstance(message, Message)\n            and isinstance(message.text, str)\n            and self.should_store_message\n        ):\n            store_message(\n                message,\n                flow_id=self.graph.flow_id,\n            )\n            self.message.value = message\n\n        self.status = message\n        return message\n","fileTypes":[],"file_path":"","password":false,"name":"code","advanced":true,"dynamic":true,"info":"","load_from_db":false,"title_case":false},"data_template":{"trace_as_input":true,"trace_as_metadata":true,"load_from_db":false,"list":false,"required":false,"placeholder":"","show":true,"name":"data_template","value":"{text}","display_name":"Data Template","advanced":true,"input_types":["Message"],"dynamic":false,"info":"Template to convert Data to Text. If left empty, it will be dynamically set to the Data's text key.","title_case":false,"type":"str","_input_type":"MessageTextInput"},"input_value":{"trace_as_input":true,"trace_as_metadata":true,"load_from_db":false,"list":false,"required":false,"placeholder":"","show":true,"name":"input_value","value":"","display_name":"Text","advanced":false,"input_types":["Message"],"dynamic":false,"info":"Message to be passed as output.","title_case":false,"type":"str","_input_type":"MessageTextInput"},"sender":{"trace_as_metadata":true,"options":["Machine","User"],"combobox":false,"required":false,"placeholder":"","show":true,"name":"sender","value":"Machine","display_name":"Sender Type","advanced":true,"dynamic":false,"info":"Type of sender.","title_case":false,"type":"str","_input_type":"DropdownInput"},"sender_name":{"trace_as_input":true,"trace_as_metadata":true,"load_from_db":false,"list":false,"required":false,"placeholder":"","show":true,"name":"sender_name","value":"AI","display_name":"Sender Name","advanced":true,"input_types":["Message"],"dynamic":false,"info":"Name of the sender.","title_case":false,"type":"str","_input_type":"MessageTextInput"},"session_id":{"trace_as_input":true,"trace_as_metadata":true,"load_from_db":false,"list":false,"required":false,"placeholder":"","show":true,"name":"session_id","value":"","display_name":"Session ID","advanced":true,"input_types":["Message"],"dynamic":false,"info":"The session ID of the chat. If empty, the current session ID parameter will be used.","title_case":false,"type":"str","_input_type":"MessageTextInput"},"should_store_message":{"trace_as_metadata":true,"list":false,"required":false,"placeholder":"","show":true,"name":"should_store_message","value":true,"display_name":"Store Messages","advanced":true,"dynamic":false,"info":"Store the message in the history.","title_case":false,"type":"bool","_input_type":"BoolInput"}},"description":"Display a chat message in the Playground.","icon":"ChatOutput","base_classes":["Message"],"display_name":"Chat Output","documentation":"","custom_fields":{},"output_types":[],"pinned":false,"conditional_paths":[],"frozen":false,"outputs":[{"types":["Message"],"selected":"Message","name":"message","display_name":"Message","method":"message_response","value":"__UNDEFINED__","cache":true}],"field_order":["input_value","should_store_message","sender","sender_name","session_id","data_template"],"beta":false,"edited":false,"lf_version":"1.0.17"},"id":"ChatOutput-mjrhT"},"selected":false,"width":384,"height":298,"positionAbsolute":{"x":-291.2587274742033,"y":2406.4714480692514},"dragging":false},{"id":"TextInput-hAmSp","type":"genericNode","position":{"x":-2215.2705298955498,"y":1282.4944465797469},"data":{"type":"TextInput","node":{"template":{"_type":"Component","code":{"type":"code","required":true,"placeholder":"","list":false,"show":true,"multiline":true,"value":"from langflow.base.io.text import TextComponent\nfrom langflow.io import MessageTextInput, Output\nfrom langflow.schema.message import Message\n\n\nclass TextInputComponent(TextComponent):\n    display_name = \"Text Input\"\n    description = \"Get text inputs from the Playground.\"\n    icon = \"type\"\n    name = \"TextInput\"\n\n    inputs = [\n        MessageTextInput(\n            name=\"input_value\",\n            display_name=\"Text\",\n            info=\"Text to be passed as input.\",\n        ),\n    ]\n    outputs = [\n        Output(display_name=\"Text\", name=\"text\", method=\"text_response\"),\n    ]\n\n    def text_response(self) -> Message:\n        message = Message(\n            text=self.input_value,\n        )\n        return message\n","fileTypes":[],"file_path":"","password":false,"name":"code","advanced":true,"dynamic":true,"info":"","load_from_db":false,"title_case":false},"input_value":{"trace_as_input":true,"trace_as_metadata":true,"load_from_db":true,"list":false,"required":false,"placeholder":"","show":true,"name":"input_value","value":"OPENAI_API_KEY","display_name":"Text","advanced":false,"input_types":["Message"],"dynamic":false,"info":"Text to be passed as input.","title_case":false,"type":"str","_input_type":"MessageTextInput"}},"description":"Get text inputs from the Playground.","icon":"type","base_classes":["Message"],"display_name":"Text Input","documentation":"","custom_fields":{},"output_types":[],"pinned":false,"conditional_paths":[],"frozen":false,"outputs":[{"types":["Message"],"selected":"Message","name":"text","display_name":"Text","method":"text_response","value":"__UNDEFINED__","cache":true}],"field_order":["input_value"],"beta":false,"edited":false,"lf_version":"1.0.17"},"id":"TextInput-hAmSp"},"selected":false,"width":384,"height":298,"positionAbsolute":{"x":-2215.2705298955498,"y":1282.4944465797469},"dragging":false},{"id":"TextInput-MSS8x","type":"genericNode","position":{"x":-2208.9010584339903,"y":1754.5773852457835},"data":{"type":"TextInput","node":{"template":{"_type":"Component","code":{"type":"code","required":true,"placeholder":"","list":false,"show":true,"multiline":true,"value":"from langflow.base.io.text import TextComponent\nfrom langflow.io import MessageTextInput, Output\nfrom langflow.schema.message import Message\n\n\nclass TextInputComponent(TextComponent):\n    display_name = \"Text Input\"\n    description = \"Get text inputs from the Playground.\"\n    icon = \"type\"\n    name = \"TextInput\"\n\n    inputs = [\n        MessageTextInput(\n            name=\"input_value\",\n            display_name=\"Text\",\n            info=\"Text to be passed as input.\",\n        ),\n    ]\n    outputs = [\n        Output(display_name=\"Text\", name=\"text\", method=\"text_response\"),\n    ]\n\n    def text_response(self) -> Message:\n        message = Message(\n            text=self.input_value,\n        )\n        return message\n","fileTypes":[],"file_path":"","password":false,"name":"code","advanced":true,"dynamic":true,"info":"","load_from_db":false,"title_case":false},"input_value":{"trace_as_input":true,"trace_as_metadata":true,"load_from_db":true,"list":false,"required":false,"placeholder":"","show":true,"name":"input_value","value":"BIGQUERY_API_KEY","display_name":"Text","advanced":false,"input_types":["Message"],"dynamic":false,"info":"Text to be passed as input.","title_case":false,"type":"str","_input_type":"MessageTextInput"}},"description":"Get text inputs from the Playground.","icon":"type","base_classes":["Message"],"display_name":"Text Input","documentation":"","custom_fields":{},"output_types":[],"pinned":false,"conditional_paths":[],"frozen":false,"outputs":[{"types":["Message"],"selected":"Message","name":"text","display_name":"Text","method":"text_response","value":"__UNDEFINED__","cache":true}],"field_order":["input_value"],"beta":false,"edited":false,"lf_version":"1.0.17"},"id":"TextInput-MSS8x"},"selected":false,"width":384,"height":298,"positionAbsolute":{"x":-2208.9010584339903,"y":1754.5773852457835},"dragging":false},{"data":{"id":"groupComponent-mAH3x","type":"GroupNode","node":{"display_name":"GA4 Agent","documentation":"","description":"","template":{"query_params_APIRequest-XsMPY":{"trace_as_input":true,"trace_as_metadata":true,"list":false,"required":false,"placeholder":"","show":true,"value":"","name":"query_params","display_name":"Query Parameters","advanced":false,"input_types":["Data"],"dynamic":false,"info":"The query parameters to append to the URL.","title_case":false,"type":"other","_input_type":"DataInput","proxy":{"id":"APIRequest-XsMPY","field":"query_params"}},"code_APIRequest-XsMPY":{"type":"code","required":true,"placeholder":"","list":false,"show":true,"multiline":true,"value":"import asyncio\nimport json\nfrom typing import Any, List, Optional\nfrom urllib.parse import parse_qsl, urlencode, urlparse, urlunparse\n\nimport httpx\nfrom loguru import logger\n\nfrom langflow.base.curl.parse import parse_context\nfrom langflow.custom import Component\nfrom langflow.io import DataInput, DropdownInput, IntInput, MessageTextInput, NestedDictInput, Output\nfrom langflow.schema import Data\nfrom langflow.schema.dotdict import dotdict\n\n\nclass APIRequestComponent(Component):\n    display_name = \"API Request\"\n    description = (\n        \"This component allows you to make HTTP requests to one or more URLs. \"\n        \"You can provide headers and body as either dictionaries or Data objects. \"\n        \"Additionally, you can append query parameters to the URLs.\\n\\n\"\n        \"**Note:** Check advanced options for more settings.\"\n    )\n    icon = \"Globe\"\n    name = \"APIRequest\"\n\n    inputs = [\n        MessageTextInput(\n            name=\"urls\",\n            display_name=\"URLs\",\n            is_list=True,\n            info=\"Enter one or more URLs, separated by commas.\",\n        ),\n        MessageTextInput(\n            name=\"curl\",\n            display_name=\"Curl\",\n            info=\"Paste a curl command to populate the fields. This will fill in the dictionary fields for headers and body.\",\n            advanced=False,\n            refresh_button=True,\n        ),\n        DropdownInput(\n            name=\"method\",\n            display_name=\"Method\",\n            options=[\"GET\", \"POST\", \"PATCH\", \"PUT\"],\n            value=\"GET\",\n            info=\"The HTTP method to use (GET, POST, PATCH, PUT).\",\n        ),\n        NestedDictInput(\n            name=\"headers\",\n            display_name=\"Headers\",\n            info=\"The headers to send with the request as a dictionary. This is populated when using the CURL field.\",\n            input_types=[\"Data\"],\n        ),\n        NestedDictInput(\n            name=\"body\",\n            display_name=\"Body\",\n            info=\"The body to send with the request as a dictionary (for POST, PATCH, PUT). This is populated when using the CURL field.\",\n            input_types=[\"Data\"],\n        ),\n        DataInput(\n            name=\"query_params\",\n            display_name=\"Query Parameters\",\n            info=\"The query parameters to append to the URL.\",\n        ),\n        IntInput(\n            name=\"timeout\",\n            display_name=\"Timeout\",\n            value=5,\n            info=\"The timeout to use for the request.\",\n        ),\n    ]\n\n    outputs = [\n        Output(display_name=\"Data\", name=\"data\", method=\"make_requests\"),\n    ]\n\n    def parse_curl(self, curl: str, build_config: dotdict) -> dotdict:\n        try:\n            parsed = parse_context(curl)\n            build_config[\"urls\"][\"value\"] = [parsed.url]\n            build_config[\"method\"][\"value\"] = parsed.method.upper()\n            build_config[\"headers\"][\"value\"] = dict(parsed.headers)\n\n            if parsed.data:\n                try:\n                    json_data = json.loads(parsed.data)\n                    build_config[\"body\"][\"value\"] = json_data\n                except json.JSONDecodeError as e:\n                    logger.error(f\"Error decoding JSON data: {e}\")\n            else:\n                build_config[\"body\"][\"value\"] = {}\n        except Exception as exc:\n            logger.error(f\"Error parsing curl: {exc}\")\n            raise ValueError(f\"Error parsing curl: {exc}\")\n        return build_config\n\n    def update_build_config(self, build_config: dotdict, field_value: Any, field_name: str | None = None):\n        if field_name == \"curl\" and field_value:\n            build_config = self.parse_curl(field_value, build_config)\n        return build_config\n\n    async def make_request(\n        self,\n        client: httpx.AsyncClient,\n        method: str,\n        url: str,\n        headers: Optional[dict] = None,\n        body: Optional[dict] = None,\n        timeout: int = 5,\n    ) -> Data:\n        method = method.upper()\n        if method not in [\"GET\", \"POST\", \"PATCH\", \"PUT\", \"DELETE\"]:\n            raise ValueError(f\"Unsupported method: {method}\")\n\n        if isinstance(body, str) and body:\n            try:\n                body = json.loads(body)\n            except Exception as e:\n                logger.error(f\"Error decoding JSON data: {e}\")\n                body = None\n                raise ValueError(f\"Error decoding JSON data: {e}\")\n\n        data = body if body else None\n\n        try:\n            response = await client.request(method, url, headers=headers, json=data, timeout=timeout)\n            try:\n                result = response.json()\n            except Exception:\n                result = response.text\n            return Data(\n                data={\n                    \"source\": url,\n                    \"headers\": headers,\n                    \"status_code\": response.status_code,\n                    \"result\": result,\n                },\n            )\n        except httpx.TimeoutException:\n            return Data(\n                data={\n                    \"source\": url,\n                    \"headers\": headers,\n                    \"status_code\": 408,\n                    \"error\": \"Request timed out\",\n                },\n            )\n        except Exception as exc:\n            return Data(\n                data={\n                    \"source\": url,\n                    \"headers\": headers,\n                    \"status_code\": 500,\n                    \"error\": str(exc),\n                },\n            )\n\n    def add_query_params(self, url: str, params: dict) -> str:\n        url_parts = list(urlparse(url))\n        query = dict(parse_qsl(url_parts[4]))\n        query.update(params)\n        url_parts[4] = urlencode(query)\n        return urlunparse(url_parts)\n\n    async def make_requests(self) -> List[Data]:\n        method = self.method\n        urls = [url.strip() for url in self.urls if url.strip()]\n        curl = self.curl\n        headers = self.headers or {}\n        body = self.body or {}\n        timeout = self.timeout\n        query_params = self.query_params.data if self.query_params else {}\n\n        if curl:\n            self._build_config = self.parse_curl(curl, dotdict())\n\n        if isinstance(headers, Data):\n            headers = headers.data\n\n        if isinstance(body, Data):\n            body = body.data\n\n        bodies = [body] * len(urls)\n\n        urls = [self.add_query_params(url, query_params) for url in urls]\n\n        async with httpx.AsyncClient() as client:\n            results = await asyncio.gather(\n                *[self.make_request(client, method, u, headers, rec, timeout) for u, rec in zip(urls, bodies)]\n            )\n        self.status = results\n        return results\n","fileTypes":[],"file_path":"","password":false,"name":"code","advanced":true,"dynamic":true,"info":"","load_from_db":false,"title_case":false,"display_name":"code","proxy":{"id":"APIRequest-XsMPY","field":"code"}},"curl_APIRequest-XsMPY":{"trace_as_input":true,"trace_as_metadata":true,"load_from_db":false,"list":false,"required":false,"placeholder":"","show":true,"value":"","name":"curl","display_name":"Curl","advanced":false,"input_types":["Message"],"dynamic":false,"info":"Paste a curl command to populate the fields. This will fill in the dictionary fields for headers and body.","refresh_button":true,"title_case":false,"type":"str","_input_type":"MessageTextInput","proxy":{"id":"APIRequest-XsMPY","field":"curl"}},"method_APIRequest-XsMPY":{"trace_as_metadata":true,"options":["GET","POST","PATCH","PUT"],"combobox":false,"required":false,"placeholder":"","show":true,"value":"POST","name":"method","display_name":"Method","advanced":true,"dynamic":false,"info":"The HTTP method to use (GET, POST, PATCH, PUT).","title_case":false,"type":"str","_input_type":"DropdownInput","proxy":{"id":"APIRequest-XsMPY","field":"method"}},"timeout_APIRequest-XsMPY":{"trace_as_metadata":true,"list":false,"required":false,"placeholder":"","show":true,"value":"5","name":"timeout","display_name":"Timeout","advanced":true,"dynamic":false,"info":"The timeout to use for the request.","title_case":false,"type":"int","_input_type":"IntInput","proxy":{"id":"APIRequest-XsMPY","field":"timeout"}},"urls_APIRequest-XsMPY":{"trace_as_input":true,"trace_as_metadata":true,"load_from_db":false,"list":true,"required":false,"placeholder":"","show":true,"value":["https://bigquery.googleapis.com/bigquery/v2/projects/spherical-proxy-424916-c9/queries"],"name":"urls","display_name":"URLs","advanced":false,"input_types":["Message"],"dynamic":false,"info":"Enter one or more URLs, separated by commas.","title_case":false,"type":"str","_input_type":"MessageTextInput","proxy":{"id":"APIRequest-XsMPY","field":"urls"}},"api_key_OpenAIModel-OHg1W":{"load_from_db":false,"required":false,"placeholder":"","show":true,"name":"api_key","value":null,"display_name":"OpenAI API Key","advanced":false,"input_types":["Message"],"dynamic":false,"info":"The OpenAI API Key to use for the OpenAI model.","title_case":false,"password":true,"type":"str","_input_type":"SecretStrInput","proxy":{"id":"OpenAIModel-OHg1W","field":"api_key"}},"code_OpenAIModel-OHg1W":{"type":"code","required":true,"placeholder":"","list":false,"show":true,"multiline":true,"value":"import operator\nfrom functools import reduce\n\nfrom langflow.field_typing.range_spec import RangeSpec\nfrom langchain_openai import ChatOpenAI\nfrom pydantic.v1 import SecretStr\n\nfrom langflow.base.models.model import LCModelComponent\nfrom langflow.base.models.openai_constants import OPENAI_MODEL_NAMES\nfrom langflow.field_typing import LanguageModel\nfrom langflow.inputs import (\n    BoolInput,\n    DictInput,\n    DropdownInput,\n    FloatInput,\n    IntInput,\n    SecretStrInput,\n    StrInput,\n)\n\n\nclass OpenAIModelComponent(LCModelComponent):\n    display_name = \"OpenAI\"\n    description = \"Generates text using OpenAI LLMs.\"\n    icon = \"OpenAI\"\n    name = \"OpenAIModel\"\n\n    inputs = LCModelComponent._base_inputs + [\n        IntInput(\n            name=\"max_tokens\",\n            display_name=\"Max Tokens\",\n            advanced=True,\n            info=\"The maximum number of tokens to generate. Set to 0 for unlimited tokens.\",\n            range_spec=RangeSpec(min=0, max=128000),\n        ),\n        DictInput(name=\"model_kwargs\", display_name=\"Model Kwargs\", advanced=True),\n        BoolInput(\n            name=\"json_mode\",\n            display_name=\"JSON Mode\",\n            advanced=True,\n            info=\"If True, it will output JSON regardless of passing a schema.\",\n        ),\n        DictInput(\n            name=\"output_schema\",\n            is_list=True,\n            display_name=\"Schema\",\n            advanced=True,\n            info=\"The schema for the Output of the model. You must pass the word JSON in the prompt. If left blank, JSON mode will be disabled.\",\n        ),\n        DropdownInput(\n            name=\"model_name\",\n            display_name=\"Model Name\",\n            advanced=False,\n            options=OPENAI_MODEL_NAMES,\n            value=OPENAI_MODEL_NAMES[0],\n        ),\n        StrInput(\n            name=\"openai_api_base\",\n            display_name=\"OpenAI API Base\",\n            advanced=True,\n            info=\"The base URL of the OpenAI API. Defaults to https://api.openai.com/v1. You can change this to use other APIs like JinaChat, LocalAI and Prem.\",\n        ),\n        SecretStrInput(\n            name=\"api_key\",\n            display_name=\"OpenAI API Key\",\n            info=\"The OpenAI API Key to use for the OpenAI model.\",\n            advanced=False,\n            value=\"OPENAI_API_KEY\",\n        ),\n        FloatInput(name=\"temperature\", display_name=\"Temperature\", value=0.1),\n        IntInput(\n            name=\"seed\",\n            display_name=\"Seed\",\n            info=\"The seed controls the reproducibility of the job.\",\n            advanced=True,\n            value=1,\n        ),\n    ]\n\n    def build_model(self) -> LanguageModel:  # type: ignore[type-var]\n        # self.output_schema is a list of dictionaries\n        # let's convert it to a dictionary\n        output_schema_dict: dict[str, str] = reduce(operator.ior, self.output_schema or {}, {})\n        openai_api_key = self.api_key\n        temperature = self.temperature\n        model_name: str = self.model_name\n        max_tokens = self.max_tokens\n        model_kwargs = self.model_kwargs or {}\n        openai_api_base = self.openai_api_base or \"https://api.openai.com/v1\"\n        json_mode = bool(output_schema_dict) or self.json_mode\n        seed = self.seed\n\n        if openai_api_key:\n            api_key = SecretStr(openai_api_key)\n        else:\n            api_key = None\n        output = ChatOpenAI(\n            max_tokens=max_tokens or None,\n            model_kwargs=model_kwargs,\n            model=model_name,\n            base_url=openai_api_base,\n            api_key=api_key,\n            temperature=temperature if temperature is not None else 0.1,\n            seed=seed,\n        )\n        if json_mode:\n            if output_schema_dict:\n                output = output.with_structured_output(schema=output_schema_dict, method=\"json_mode\")  # type: ignore\n            else:\n                output = output.bind(response_format={\"type\": \"json_object\"})  # type: ignore\n\n        return output  # type: ignore\n\n    def _get_exception_message(self, e: Exception):\n        \"\"\"\n        Get a message from an OpenAI exception.\n\n        Args:\n            exception (Exception): The exception to get the message from.\n\n        Returns:\n            str: The message from the exception.\n        \"\"\"\n\n        try:\n            from openai import BadRequestError\n        except ImportError:\n            return\n        if isinstance(e, BadRequestError):\n            message = e.body.get(\"message\")  # type: ignore\n            if message:\n                return message\n        return\n","fileTypes":[],"file_path":"","password":false,"name":"code","advanced":true,"dynamic":true,"info":"","load_from_db":false,"title_case":false,"display_name":"code","proxy":{"id":"OpenAIModel-OHg1W","field":"code"}},"json_mode_OpenAIModel-OHg1W":{"trace_as_metadata":true,"list":false,"required":false,"placeholder":"","show":true,"name":"json_mode","value":false,"display_name":"JSON Mode","advanced":true,"dynamic":false,"info":"If True, it will output JSON regardless of passing a schema.","title_case":false,"type":"bool","_input_type":"BoolInput","proxy":{"id":"OpenAIModel-OHg1W","field":"json_mode"}},"max_tokens_OpenAIModel-OHg1W":{"trace_as_metadata":true,"range_spec":{"step_type":"float","min":0,"max":128000,"step":0.1},"list":false,"required":false,"placeholder":"","show":true,"name":"max_tokens","value":"","display_name":"Max Tokens","advanced":true,"dynamic":false,"info":"The maximum number of tokens to generate. Set to 0 for unlimited tokens.","title_case":false,"type":"int","_input_type":"IntInput","proxy":{"id":"OpenAIModel-OHg1W","field":"max_tokens"}},"model_kwargs_OpenAIModel-OHg1W":{"trace_as_input":true,"list":false,"required":false,"placeholder":"","show":true,"name":"model_kwargs","value":{},"display_name":"Model Kwargs","advanced":true,"dynamic":false,"info":"","title_case":false,"type":"dict","_input_type":"DictInput","proxy":{"id":"OpenAIModel-OHg1W","field":"model_kwargs"}},"model_name_OpenAIModel-OHg1W":{"trace_as_metadata":true,"options":["gpt-4o-mini","gpt-4o","gpt-4-turbo","gpt-4-turbo-preview","gpt-4","gpt-3.5-turbo","gpt-3.5-turbo-0125"],"combobox":false,"required":false,"placeholder":"","show":true,"name":"model_name","value":"gpt-4o","display_name":"Model Name","advanced":true,"dynamic":false,"info":"","title_case":false,"type":"str","_input_type":"DropdownInput","proxy":{"id":"OpenAIModel-OHg1W","field":"model_name"}},"openai_api_base_OpenAIModel-OHg1W":{"trace_as_metadata":true,"load_from_db":false,"list":false,"required":false,"placeholder":"","show":true,"name":"openai_api_base","value":"","display_name":"OpenAI API Base","advanced":true,"dynamic":false,"info":"The base URL of the OpenAI API. Defaults to https://api.openai.com/v1. You can change this to use other APIs like JinaChat, LocalAI and Prem.","title_case":false,"type":"str","_input_type":"StrInput","proxy":{"id":"OpenAIModel-OHg1W","field":"openai_api_base"}},"output_schema_OpenAIModel-OHg1W":{"trace_as_input":true,"list":true,"required":false,"placeholder":"","show":true,"name":"output_schema","value":{},"display_name":"Schema","advanced":true,"dynamic":false,"info":"The schema for the Output of the model. You must pass the word JSON in the prompt. If left blank, JSON mode will be disabled.","title_case":false,"type":"dict","_input_type":"DictInput","proxy":{"id":"OpenAIModel-OHg1W","field":"output_schema"}},"seed_OpenAIModel-OHg1W":{"trace_as_metadata":true,"list":false,"required":false,"placeholder":"","show":true,"name":"seed","value":1,"display_name":"Seed","advanced":true,"dynamic":false,"info":"The seed controls the reproducibility of the job.","title_case":false,"type":"int","_input_type":"IntInput","proxy":{"id":"OpenAIModel-OHg1W","field":"seed"}},"stream_OpenAIModel-OHg1W":{"trace_as_metadata":true,"list":false,"required":false,"placeholder":"","show":true,"name":"stream","value":false,"display_name":"Stream","advanced":true,"dynamic":false,"info":"Stream the response from the model. Streaming works only in Chat.","title_case":false,"type":"bool","_input_type":"BoolInput","proxy":{"id":"OpenAIModel-OHg1W","field":"stream"}},"system_message_OpenAIModel-OHg1W":{"trace_as_input":true,"trace_as_metadata":true,"load_from_db":false,"list":false,"required":false,"placeholder":"","show":true,"name":"system_message","value":"","display_name":"System Message","advanced":true,"input_types":["Message"],"dynamic":false,"info":"System message to pass to the model.","title_case":false,"type":"str","_input_type":"MessageTextInput","proxy":{"id":"OpenAIModel-OHg1W","field":"system_message"}},"temperature_OpenAIModel-OHg1W":{"trace_as_metadata":true,"list":false,"required":false,"placeholder":"","show":true,"name":"temperature","value":0.1,"display_name":"Temperature","advanced":true,"dynamic":false,"info":"","title_case":false,"type":"float","_input_type":"FloatInput","proxy":{"id":"OpenAIModel-OHg1W","field":"temperature"}},"code_Prompt-AOko9":{"type":"code","required":true,"placeholder":"","list":false,"show":true,"multiline":true,"value":"from langflow.base.prompts.api_utils import process_prompt_template\nfrom langflow.custom import Component\nfrom langflow.inputs.inputs import DefaultPromptField\nfrom langflow.io import Output, PromptInput\nfrom langflow.schema.message import Message\nfrom langflow.template.utils import update_template_values\n\n\nclass PromptComponent(Component):\n    display_name: str = \"Prompt\"\n    description: str = \"Create a prompt template with dynamic variables.\"\n    icon = \"prompts\"\n    trace_type = \"prompt\"\n    name = \"Prompt\"\n\n    inputs = [\n        PromptInput(name=\"template\", display_name=\"Template\"),\n    ]\n\n    outputs = [\n        Output(display_name=\"Prompt Message\", name=\"prompt\", method=\"build_prompt\"),\n    ]\n\n    async def build_prompt(\n        self,\n    ) -> Message:\n        prompt = await Message.from_template_and_variables(**self._attributes)\n        self.status = prompt.text\n        return prompt\n\n    def _update_template(self, frontend_node: dict):\n        prompt_template = frontend_node[\"template\"][\"template\"][\"value\"]\n        custom_fields = frontend_node[\"custom_fields\"]\n        frontend_node_template = frontend_node[\"template\"]\n        _ = process_prompt_template(\n            template=prompt_template,\n            name=\"template\",\n            custom_fields=custom_fields,\n            frontend_node_template=frontend_node_template,\n        )\n        return frontend_node\n\n    def post_code_processing(self, new_frontend_node: dict, current_frontend_node: dict):\n        \"\"\"\n        This function is called after the code validation is done.\n        \"\"\"\n        frontend_node = super().post_code_processing(new_frontend_node, current_frontend_node)\n        template = frontend_node[\"template\"][\"template\"][\"value\"]\n        # Kept it duplicated for backwards compatibility\n        _ = process_prompt_template(\n            template=template,\n            name=\"template\",\n            custom_fields=frontend_node[\"custom_fields\"],\n            frontend_node_template=frontend_node[\"template\"],\n        )\n        # Now that template is updated, we need to grab any values that were set in the current_frontend_node\n        # and update the frontend_node with those values\n        update_template_values(new_template=frontend_node, previous_template=current_frontend_node[\"template\"])\n        return frontend_node\n\n    def _get_fallback_input(self, **kwargs):\n        return DefaultPromptField(**kwargs)\n","fileTypes":[],"file_path":"","password":false,"name":"code","advanced":true,"dynamic":true,"info":"","load_from_db":false,"title_case":false,"display_name":"code","proxy":{"id":"Prompt-AOko9","field":"code"}},"template_Prompt-AOko9":{"trace_as_input":true,"list":false,"required":false,"placeholder":"","show":true,"name":"template","value":"**Context** :\n{context}\n**User Query** :\n{user_prompt}\n\n---\n\n**Task** : Create a GA4 SQL query to the `bigquery-public-data.ga4_obfuscated_sample_ecommerce.events_20201209` table. Use the schema information provided below to ensure the query is valid. \n1. **Field Validation** :\n  - Only reference fields that exist in the schema provided below.\n \n  - Use the correct dot notation when accessing fields within complex data structures like `RECORD` (also known as `STRUCT`). For example, access `ecommerce.purchase_revenue_in_usd` directly if it’s a non-repeated field.\n \n  - Use `UNNEST` only for repeated fields (arrays). Do not use `UNNEST` for fields that are simple `STRUCT` (single-record) types. For example, `event_params` and `user_properties` require `UNNEST`, while `ecommerce` does not.\n \n2. **Query Structure** : \n  - Validate that the query structure is correct and does not produce errors related to data types, field names, or incorrect usage of `UNNEST`.\n \n  - Access fields within `STRUCT` (or `RECORD`) directly using dot notation without `UNNEST` if they are not repeated. For instance, fields within `ecommerce` can be accessed directly.\n\n  - Ensure that all referenced fields are appropriately handled, especially when dealing with nested or repeated fields.\n \n3. **Error Handling** :\n  - If a query cannot be formed due to a missing or invalid field in the schema, return an error message instead of a query.\n\n\n---\n\nSchema Information for `events_20201209` Table** :\n\n```json\ncolumn_name,data_type,is_nullable\nevent_date,STRING,YES\nevent_timestamp,INT64,YES\nevent_name,STRING,YES\nevent_params,\"ARRAY<STRUCT<key STRING, value STRUCT<string_value STRING, int_value INT64, float_value FLOAT64, double_value FLOAT64>>>\",NO\nevent_previous_timestamp,INT64,YES\nevent_value_in_usd,FLOAT64,YES\nevent_bundle_sequence_id,INT64,YES\nevent_server_timestamp_offset,INT64,YES\nuser_id,STRING,YES\nuser_pseudo_id,STRING,YES\nprivacy_info,\"STRUCT<analytics_storage INT64, ads_storage INT64, uses_transient_token STRING>\",YES\nuser_properties,\"ARRAY<STRUCT<key INT64, value STRUCT<string_value INT64, int_value INT64, float_value INT64, double_value INT64, set_timestamp_micros INT64>>>\",NO\nuser_first_touch_timestamp,INT64,YES\nuser_ltv,\"STRUCT<revenue FLOAT64, currency STRING>\",YES\ndevice,\"STRUCT<category STRING, mobile_brand_name STRING, mobile_model_name STRING, mobile_marketing_name STRING, mobile_os_hardware_model INT64, operating_system STRING, operating_system_version STRING, vendor_id INT64, advertising_id INT64, language STRING, is_limited_ad_tracking STRING, time_zone_offset_seconds INT64, web_info STRUCT<browser STRING, browser_version STRING>>\",YES\ngeo,\"STRUCT<continent STRING, sub_continent STRING, country STRING, region STRING, city STRING, metro STRING>\",YES\napp_info,\"STRUCT<id STRING, version STRING, install_store STRING, firebase_app_id STRING, install_source STRING>\",YES\ntraffic_source,\"STRUCT<medium STRING, name STRING, source STRING>\",YES\nstream_id,INT64,YES\nplatform,STRING,YES\nevent_dimensions,STRUCT<hostname STRING>,YES\necommerce,\"STRUCT<total_item_quantity INT64, purchase_revenue_in_usd FLOAT64, purchase_revenue FLOAT64, refund_value_in_usd FLOAT64, refund_value FLOAT64, shipping_value_in_usd FLOAT64, shipping_value FLOAT64, tax_value_in_usd FLOAT64, tax_value FLOAT64, unique_items INT64, transaction_id STRING>\",YES\nitems,\"ARRAY<STRUCT<item_id STRING, item_name STRING, item_brand STRING, item_variant STRING, item_category STRING, item_category2 STRING, item_category3 STRING, item_category4 STRING, item_category5 STRING, price_in_usd FLOAT64, price FLOAT64, quantity INT64, item_revenue_in_usd FLOAT64, item_revenue FLOAT64, item_refund_in_usd FLOAT64, item_refund FLOAT64, coupon STRING, affiliation STRING, location_id STRING, item_list_id STRING, item_list_name STRING, item_list_index STRING, promotion_id STRING, promotion_name STRING, creative_name STRING, creative_slot STRING>>\",NO\n```\n\n\n---\n\n**Output Requirement** : Your output must be **only**  the SQL query like the example provided:\n\n```sql\nSELECT * FROM `bigquery-public-data.ga4_obfuscated_sample_ecommerce.events_20201209` LIMIT 0\n```\n\nIf you cannot generate a valid query due to missing or invalid fields, return an error message instead. Ensure that you are running a unique query that was not ran before in the context.\n\n\n---\n\n**AI** :","display_name":"Template","advanced":true,"dynamic":false,"info":"","title_case":false,"type":"prompt","_input_type":"PromptInput","load_from_db":false,"proxy":{"id":"Prompt-AOko9","field":"template"}},"code_CustomComponent-NwxNM":{"type":"code","required":true,"placeholder":"","list":false,"show":true,"multiline":true,"value":"# from langflow.field_typing import Data\nfrom langflow.custom import Component\nfrom langflow.io import MessageTextInput, Output\nfrom langflow.schema import Data\nimport re  # Import regex module\n\nclass CustomComponent(Component):\n    display_name = \"Format Body\"\n    description = \"Turn OpenAI SQL output into a body to send with a fetch.\"\n    documentation: str = \"http://docs.langflow.org/components/custom\"\n    icon = \"custom_components\"\n    name = \"CustomComponent\"\n\n    inputs = [\n        MessageTextInput(name=\"input_value\", display_name=\"Input Value\", value=\"```sql\\nbigquery-public-data.ga4_obfuscated_sample_ecommerce.events_*\\n```\"),\n    ]\n\n    outputs = [\n        Output(display_name=\"Output\", name=\"output\", method=\"build_output\"),\n    ]\n\n    def build_output(self) -> Data:\n        # Remove the surrounding markdown and any trailing newline\n        input_value_cleaned = re.sub(r\"^```sql\\n|```$\", \"\", self.input_value.strip()).strip()\n\n        # Format the cleaned SQL string into the desired object\n        formatted_data = {\n            \"query\": input_value_cleaned,\n            \"maxResults\": 20,\n            \"timeoutMs\": 10000,\n            \"useLegacySql\": False\n        }\n        \n        # Return the formatted data as the output\n        data = Data(value=formatted_data)\n        self.status = data\n        \n        # Print the formatted data to the chat (for debugging purposes)\n        print(f\"Output Data: {formatted_data}\")\n        \n        return formatted_data\n","fileTypes":[],"file_path":"","password":false,"name":"code","advanced":true,"dynamic":true,"info":"","load_from_db":false,"title_case":false,"display_name":"code","proxy":{"id":"CustomComponent-NwxNM","field":"code"}},"code_FetchProcessorComponent-jzDpE":{"type":"code","required":true,"placeholder":"","list":false,"show":true,"multiline":true,"value":"from langflow.custom import Component\nfrom langflow.io import MessageTextInput, Output\nfrom langflow.schema import Data\nfrom langflow.schema.message import Message\nimport json\n\n\nclass FetchProcessorComponent(Component):\n    display_name = \"Fetch Processor\"\n    description = \"Process fetch output, print it, and stringify it for chat.\"\n    documentation: str = \"http://docs.langflow.org/components/custom\"\n    icon = \"custom_components\"\n    name = \"FetchProcessorComponent\"\n\n    inputs = [\n        HandleInput(\n            name=\"fetch_response\",\n            display_name=\"Fetch Response\",\n            input_types=[\"Data\"],\n        )\n    ]\n\n    outputs = [\n        Output(display_name=\"Processed Output\", name=\"processed_output\", method=\"process_output\"),\n    ]\n    \n    def process_output(self) -> Message:\n        try:\n            # Directly use the fetch_response as the input data\n            response_data = self.fetch_response\n\n            # Extract the response content\n            content = str(response_data[0].data[\"result\"])\n            \n            # Truncate the content to a specific length (e.g., 5000 characters)\n            max_length = 5000\n            truncated_content = content[:max_length]\n            \n            # Convert the truncated content to a string that can be printed\n            output_str = \"Processed Output: \" + str(truncated_content)+\"...\"\n            \n            \n            # Return the processed string as a Message\n            self.status = output_str\n            return Message(text=output_str)\n\n        except Exception as e:\n            # Handle any errors and print them\n            error_message = f\"Error processing fetch response: {str(e)}\"\n            print(error_message)\n            return Message(text=error_message)\n","fileTypes":[],"file_path":"","password":false,"name":"code","advanced":true,"dynamic":true,"info":"","load_from_db":false,"title_case":false,"display_name":"code","proxy":{"id":"FetchProcessorComponent-jzDpE","field":"code"}},"api_key_OpenAIModel-BxXKu":{"load_from_db":false,"required":false,"placeholder":"","show":true,"value":null,"name":"api_key","display_name":"OpenAI API Key","advanced":false,"input_types":["Message"],"dynamic":false,"info":"The OpenAI API Key to use for the OpenAI model.","title_case":false,"password":true,"type":"str","_input_type":"SecretStrInput","proxy":{"id":"OpenAIModel-BxXKu","field":"api_key"}},"code_OpenAIModel-BxXKu":{"type":"code","required":true,"placeholder":"","list":false,"show":true,"multiline":true,"value":"import operator\nfrom functools import reduce\n\nfrom langflow.field_typing.range_spec import RangeSpec\nfrom langchain_openai import ChatOpenAI\nfrom pydantic.v1 import SecretStr\n\nfrom langflow.base.models.model import LCModelComponent\nfrom langflow.base.models.openai_constants import OPENAI_MODEL_NAMES\nfrom langflow.field_typing import LanguageModel\nfrom langflow.inputs import (\n    BoolInput,\n    DictInput,\n    DropdownInput,\n    FloatInput,\n    IntInput,\n    SecretStrInput,\n    StrInput,\n)\n\n\nclass OpenAIModelComponent(LCModelComponent):\n    display_name = \"OpenAI\"\n    description = \"Generates text using OpenAI LLMs.\"\n    icon = \"OpenAI\"\n    name = \"OpenAIModel\"\n\n    inputs = LCModelComponent._base_inputs + [\n        IntInput(\n            name=\"max_tokens\",\n            display_name=\"Max Tokens\",\n            advanced=True,\n            info=\"The maximum number of tokens to generate. Set to 0 for unlimited tokens.\",\n            range_spec=RangeSpec(min=0, max=128000),\n        ),\n        DictInput(name=\"model_kwargs\", display_name=\"Model Kwargs\", advanced=True),\n        BoolInput(\n            name=\"json_mode\",\n            display_name=\"JSON Mode\",\n            advanced=True,\n            info=\"If True, it will output JSON regardless of passing a schema.\",\n        ),\n        DictInput(\n            name=\"output_schema\",\n            is_list=True,\n            display_name=\"Schema\",\n            advanced=True,\n            info=\"The schema for the Output of the model. You must pass the word JSON in the prompt. If left blank, JSON mode will be disabled.\",\n        ),\n        DropdownInput(\n            name=\"model_name\",\n            display_name=\"Model Name\",\n            advanced=False,\n            options=OPENAI_MODEL_NAMES,\n            value=OPENAI_MODEL_NAMES[0],\n        ),\n        StrInput(\n            name=\"openai_api_base\",\n            display_name=\"OpenAI API Base\",\n            advanced=True,\n            info=\"The base URL of the OpenAI API. Defaults to https://api.openai.com/v1. You can change this to use other APIs like JinaChat, LocalAI and Prem.\",\n        ),\n        SecretStrInput(\n            name=\"api_key\",\n            display_name=\"OpenAI API Key\",\n            info=\"The OpenAI API Key to use for the OpenAI model.\",\n            advanced=False,\n            value=\"OPENAI_API_KEY\",\n        ),\n        FloatInput(name=\"temperature\", display_name=\"Temperature\", value=0.1),\n        IntInput(\n            name=\"seed\",\n            display_name=\"Seed\",\n            info=\"The seed controls the reproducibility of the job.\",\n            advanced=True,\n            value=1,\n        ),\n    ]\n\n    def build_model(self) -> LanguageModel:  # type: ignore[type-var]\n        # self.output_schema is a list of dictionaries\n        # let's convert it to a dictionary\n        output_schema_dict: dict[str, str] = reduce(operator.ior, self.output_schema or {}, {})\n        openai_api_key = self.api_key\n        temperature = self.temperature\n        model_name: str = self.model_name\n        max_tokens = self.max_tokens\n        model_kwargs = self.model_kwargs or {}\n        openai_api_base = self.openai_api_base or \"https://api.openai.com/v1\"\n        json_mode = bool(output_schema_dict) or self.json_mode\n        seed = self.seed\n\n        if openai_api_key:\n            api_key = SecretStr(openai_api_key)\n        else:\n            api_key = None\n        output = ChatOpenAI(\n            max_tokens=max_tokens or None,\n            model_kwargs=model_kwargs,\n            model=model_name,\n            base_url=openai_api_base,\n            api_key=api_key,\n            temperature=temperature or 0.1,\n            seed=seed,\n        )\n        if json_mode:\n            if output_schema_dict:\n                output = output.with_structured_output(schema=output_schema_dict, method=\"json_mode\")  # type: ignore\n            else:\n                output = output.bind(response_format={\"type\": \"json_object\"})  # type: ignore\n\n        return output  # type: ignore\n\n    def _get_exception_message(self, e: Exception):\n        \"\"\"\n        Get a message from an OpenAI exception.\n\n        Args:\n            exception (Exception): The exception to get the message from.\n\n        Returns:\n            str: The message from the exception.\n        \"\"\"\n\n        try:\n            from openai import BadRequestError\n        except ImportError:\n            return\n        if isinstance(e, BadRequestError):\n            message = e.body.get(\"message\")  # type: ignore\n            if message:\n                return message\n        return\n","fileTypes":[],"file_path":"","password":false,"name":"code","advanced":true,"dynamic":true,"info":"","load_from_db":false,"title_case":false,"display_name":"code","proxy":{"id":"OpenAIModel-BxXKu","field":"code"}},"json_mode_OpenAIModel-BxXKu":{"trace_as_metadata":true,"list":false,"required":false,"placeholder":"","show":true,"value":false,"name":"json_mode","display_name":"JSON Mode","advanced":true,"dynamic":false,"info":"If True, it will output JSON regardless of passing a schema.","title_case":false,"type":"bool","_input_type":"BoolInput","proxy":{"id":"OpenAIModel-BxXKu","field":"json_mode"}},"max_tokens_OpenAIModel-BxXKu":{"trace_as_metadata":true,"range_spec":{"step_type":"float","min":0,"max":128000,"step":0.1},"list":false,"required":false,"placeholder":"","show":true,"value":"","name":"max_tokens","display_name":"Max Tokens","advanced":true,"dynamic":false,"info":"The maximum number of tokens to generate. Set to 0 for unlimited tokens.","title_case":false,"type":"int","_input_type":"IntInput","proxy":{"id":"OpenAIModel-BxXKu","field":"max_tokens"}},"model_kwargs_OpenAIModel-BxXKu":{"trace_as_input":true,"list":false,"required":false,"placeholder":"","show":true,"value":{},"name":"model_kwargs","display_name":"Model Kwargs","advanced":true,"dynamic":false,"info":"","title_case":false,"type":"dict","_input_type":"DictInput","proxy":{"id":"OpenAIModel-BxXKu","field":"model_kwargs"}},"model_name_OpenAIModel-BxXKu":{"trace_as_metadata":true,"options":["gpt-4o-mini","gpt-4o","gpt-4-turbo","gpt-4-turbo-preview","gpt-4","gpt-3.5-turbo","gpt-3.5-turbo-0125"],"combobox":false,"required":false,"placeholder":"","show":true,"value":"gpt-4o-mini","name":"model_name","display_name":"Model Name","advanced":true,"dynamic":false,"info":"","title_case":false,"type":"str","_input_type":"DropdownInput","proxy":{"id":"OpenAIModel-BxXKu","field":"model_name"}},"openai_api_base_OpenAIModel-BxXKu":{"trace_as_metadata":true,"load_from_db":false,"list":false,"required":false,"placeholder":"","show":true,"value":"","name":"openai_api_base","display_name":"OpenAI API Base","advanced":true,"dynamic":false,"info":"The base URL of the OpenAI API. Defaults to https://api.openai.com/v1. You can change this to use other APIs like JinaChat, LocalAI and Prem.","title_case":false,"type":"str","_input_type":"StrInput","proxy":{"id":"OpenAIModel-BxXKu","field":"openai_api_base"}},"output_schema_OpenAIModel-BxXKu":{"trace_as_input":true,"list":true,"required":false,"placeholder":"","show":true,"value":{},"name":"output_schema","display_name":"Schema","advanced":true,"dynamic":false,"info":"The schema for the Output of the model. You must pass the word JSON in the prompt. If left blank, JSON mode will be disabled.","title_case":false,"type":"dict","_input_type":"DictInput","proxy":{"id":"OpenAIModel-BxXKu","field":"output_schema"}},"seed_OpenAIModel-BxXKu":{"trace_as_metadata":true,"list":false,"required":false,"placeholder":"","show":true,"value":1,"name":"seed","display_name":"Seed","advanced":true,"dynamic":false,"info":"The seed controls the reproducibility of the job.","title_case":false,"type":"int","_input_type":"IntInput","proxy":{"id":"OpenAIModel-BxXKu","field":"seed"}},"stream_OpenAIModel-BxXKu":{"trace_as_metadata":true,"list":false,"required":false,"placeholder":"","show":true,"value":false,"name":"stream","display_name":"Stream","advanced":true,"dynamic":false,"info":"Stream the response from the model. Streaming works only in Chat.","title_case":false,"type":"bool","_input_type":"BoolInput","proxy":{"id":"OpenAIModel-BxXKu","field":"stream"}},"system_message_OpenAIModel-BxXKu":{"trace_as_input":true,"trace_as_metadata":true,"load_from_db":false,"list":false,"required":false,"placeholder":"","show":true,"value":"","name":"system_message","display_name":"System Message","advanced":true,"input_types":["Message"],"dynamic":false,"info":"System message to pass to the model.","title_case":false,"type":"str","_input_type":"MessageTextInput","proxy":{"id":"OpenAIModel-BxXKu","field":"system_message"}},"temperature_OpenAIModel-BxXKu":{"trace_as_metadata":true,"list":false,"required":false,"placeholder":"","show":true,"value":0.1,"name":"temperature","display_name":"Temperature","advanced":true,"dynamic":false,"info":"","title_case":false,"type":"float","_input_type":"FloatInput","proxy":{"id":"OpenAIModel-BxXKu","field":"temperature"}},"code_Prompt-NCppB":{"type":"code","required":true,"placeholder":"","list":false,"show":true,"multiline":true,"value":"from langflow.base.prompts.api_utils import process_prompt_template\nfrom langflow.custom import Component\nfrom langflow.inputs.inputs import DefaultPromptField\nfrom langflow.io import Output, PromptInput\nfrom langflow.schema.message import Message\nfrom langflow.template.utils import update_template_values\n\n\nclass PromptComponent(Component):\n    display_name: str = \"Prompt\"\n    description: str = \"Create a prompt template with dynamic variables.\"\n    icon = \"prompts\"\n    trace_type = \"prompt\"\n    name = \"Prompt\"\n\n    inputs = [\n        PromptInput(name=\"template\", display_name=\"Template\"),\n    ]\n\n    outputs = [\n        Output(display_name=\"Prompt Message\", name=\"prompt\", method=\"build_prompt\"),\n    ]\n\n    async def build_prompt(\n        self,\n    ) -> Message:\n        prompt = await Message.from_template_and_variables(**self._attributes)\n        self.status = prompt.text\n        return prompt\n\n    def _update_template(self, frontend_node: dict):\n        prompt_template = frontend_node[\"template\"][\"template\"][\"value\"]\n        custom_fields = frontend_node[\"custom_fields\"]\n        frontend_node_template = frontend_node[\"template\"]\n        _ = process_prompt_template(\n            template=prompt_template,\n            name=\"template\",\n            custom_fields=custom_fields,\n            frontend_node_template=frontend_node_template,\n        )\n        return frontend_node\n\n    def post_code_processing(self, new_frontend_node: dict, current_frontend_node: dict):\n        \"\"\"\n        This function is called after the code validation is done.\n        \"\"\"\n        frontend_node = super().post_code_processing(new_frontend_node, current_frontend_node)\n        template = frontend_node[\"template\"][\"template\"][\"value\"]\n        # Kept it duplicated for backwards compatibility\n        _ = process_prompt_template(\n            template=template,\n            name=\"template\",\n            custom_fields=frontend_node[\"custom_fields\"],\n            frontend_node_template=frontend_node[\"template\"],\n        )\n        # Now that template is updated, we need to grab any values that were set in the current_frontend_node\n        # and update the frontend_node with those values\n        update_template_values(new_template=frontend_node, previous_template=current_frontend_node[\"template\"])\n        return frontend_node\n\n    def _get_fallback_input(self, **kwargs):\n        return DefaultPromptField(**kwargs)\n","fileTypes":[],"file_path":"","password":false,"name":"code","advanced":true,"dynamic":true,"info":"","load_from_db":false,"title_case":false,"display_name":"code","proxy":{"id":"Prompt-NCppB","field":"code"}},"template_Prompt-NCppB":{"trace_as_input":true,"list":false,"required":false,"placeholder":"","show":true,"name":"template","value":"Context: {context}\n\nUser: I was given these instructions:\n\n```\n{instructions}\n```\n\n\nI gave this query to run:\n\n```\n{query}\n```\n\nI got this output:\n\n```\n{output}\n```\n\n\nDo the following:\n1. Output the query executed\n2. Output a portion of the results (human readable)\n3. Interpret the results in the context of the instructions\n4. Output the insights that you have learned\n5. Create an action for what you think you should do next using reflection.\n\nIf there was an error, output what the error was\n\nAI:","display_name":"Template","advanced":true,"dynamic":false,"info":"","title_case":false,"type":"prompt","_input_type":"PromptInput","load_from_db":false,"proxy":{"id":"Prompt-NCppB","field":"template"}},"code_Prompt-mw3zb":{"type":"code","required":true,"placeholder":"","list":false,"show":true,"multiline":true,"value":"from langflow.base.prompts.api_utils import process_prompt_template\nfrom langflow.custom import Component\nfrom langflow.inputs.inputs import DefaultPromptField\nfrom langflow.io import Output, PromptInput\nfrom langflow.schema.message import Message\nfrom langflow.template.utils import update_template_values\n\n\nclass PromptComponent(Component):\n    display_name: str = \"Prompt\"\n    description: str = \"Create a prompt template with dynamic variables.\"\n    icon = \"prompts\"\n    trace_type = \"prompt\"\n    name = \"Prompt\"\n\n    inputs = [\n        PromptInput(name=\"template\", display_name=\"Template\"),\n    ]\n\n    outputs = [\n        Output(display_name=\"Prompt Message\", name=\"prompt\", method=\"build_prompt\"),\n    ]\n\n    async def build_prompt(\n        self,\n    ) -> Message:\n        prompt = await Message.from_template_and_variables(**self._attributes)\n        self.status = prompt.text\n        return prompt\n\n    def _update_template(self, frontend_node: dict):\n        prompt_template = frontend_node[\"template\"][\"template\"][\"value\"]\n        custom_fields = frontend_node[\"custom_fields\"]\n        frontend_node_template = frontend_node[\"template\"]\n        _ = process_prompt_template(\n            template=prompt_template,\n            name=\"template\",\n            custom_fields=custom_fields,\n            frontend_node_template=frontend_node_template,\n        )\n        return frontend_node\n\n    def post_code_processing(self, new_frontend_node: dict, current_frontend_node: dict):\n        \"\"\"\n        This function is called after the code validation is done.\n        \"\"\"\n        frontend_node = super().post_code_processing(new_frontend_node, current_frontend_node)\n        template = frontend_node[\"template\"][\"template\"][\"value\"]\n        # Kept it duplicated for backwards compatibility\n        _ = process_prompt_template(\n            template=template,\n            name=\"template\",\n            custom_fields=frontend_node[\"custom_fields\"],\n            frontend_node_template=frontend_node[\"template\"],\n        )\n        # Now that template is updated, we need to grab any values that were set in the current_frontend_node\n        # and update the frontend_node with those values\n        update_template_values(new_template=frontend_node, previous_template=current_frontend_node[\"template\"])\n        return frontend_node\n\n    def _get_fallback_input(self, **kwargs):\n        return DefaultPromptField(**kwargs)\n","fileTypes":[],"file_path":"","password":false,"name":"code","advanced":true,"dynamic":true,"info":"","load_from_db":false,"title_case":false,"display_name":"code","proxy":{"id":"Prompt-mw3zb","field":"code"}},"template_Prompt-mw3zb":{"trace_as_input":true,"list":false,"required":false,"placeholder":"","show":true,"name":"template","value":"{user_prompt}","display_name":"Template","advanced":true,"dynamic":false,"info":"","title_case":false,"type":"prompt","_input_type":"PromptInput","proxy":{"id":"Prompt-mw3zb","field":"template"}},"user_prompt_Prompt-mw3zb":{"field_type":"str","required":false,"placeholder":"","list":false,"show":true,"multiline":true,"value":"","fileTypes":[],"file_path":"","password":false,"name":"user_prompt","display_name":"user_prompt","advanced":false,"input_types":["Message","Text"],"dynamic":false,"info":"","load_from_db":false,"title_case":false,"type":"str","proxy":{"id":"Prompt-mw3zb","field":"user_prompt"}},"code_Prompt-HrBgE":{"type":"code","required":true,"placeholder":"","list":false,"show":true,"multiline":true,"value":"from langflow.base.prompts.api_utils import process_prompt_template\nfrom langflow.custom import Component\nfrom langflow.inputs.inputs import DefaultPromptField\nfrom langflow.io import Output, PromptInput\nfrom langflow.schema.message import Message\nfrom langflow.template.utils import update_template_values\n\n\nclass PromptComponent(Component):\n    display_name: str = \"Prompt\"\n    description: str = \"Create a prompt template with dynamic variables.\"\n    icon = \"prompts\"\n    trace_type = \"prompt\"\n    name = \"Prompt\"\n\n    inputs = [\n        PromptInput(name=\"template\", display_name=\"Template\"),\n    ]\n\n    outputs = [\n        Output(display_name=\"Prompt Message\", name=\"prompt\", method=\"build_prompt\"),\n    ]\n\n    async def build_prompt(\n        self,\n    ) -> Message:\n        prompt = await Message.from_template_and_variables(**self._attributes)\n        self.status = prompt.text\n        return prompt\n\n    def _update_template(self, frontend_node: dict):\n        prompt_template = frontend_node[\"template\"][\"template\"][\"value\"]\n        custom_fields = frontend_node[\"custom_fields\"]\n        frontend_node_template = frontend_node[\"template\"]\n        _ = process_prompt_template(\n            template=prompt_template,\n            name=\"template\",\n            custom_fields=custom_fields,\n            frontend_node_template=frontend_node_template,\n        )\n        return frontend_node\n\n    def post_code_processing(self, new_frontend_node: dict, current_frontend_node: dict):\n        \"\"\"\n        This function is called after the code validation is done.\n        \"\"\"\n        frontend_node = super().post_code_processing(new_frontend_node, current_frontend_node)\n        template = frontend_node[\"template\"][\"template\"][\"value\"]\n        # Kept it duplicated for backwards compatibility\n        _ = process_prompt_template(\n            template=template,\n            name=\"template\",\n            custom_fields=frontend_node[\"custom_fields\"],\n            frontend_node_template=frontend_node[\"template\"],\n        )\n        # Now that template is updated, we need to grab any values that were set in the current_frontend_node\n        # and update the frontend_node with those values\n        update_template_values(new_template=frontend_node, previous_template=current_frontend_node[\"template\"])\n        return frontend_node\n\n    def _get_fallback_input(self, **kwargs):\n        return DefaultPromptField(**kwargs)\n","fileTypes":[],"file_path":"","password":false,"name":"code","advanced":true,"dynamic":true,"info":"","load_from_db":false,"title_case":false,"display_name":"code","proxy":{"id":"Prompt-HrBgE","field":"code"}},"template_Prompt-HrBgE":{"trace_as_input":true,"list":false,"required":false,"placeholder":"","show":true,"name":"template","value":"{context}","display_name":"Template","advanced":true,"dynamic":false,"info":"","title_case":false,"type":"prompt","_input_type":"PromptInput","proxy":{"id":"Prompt-HrBgE","field":"template"}},"context_Prompt-HrBgE":{"field_type":"str","required":false,"placeholder":"","list":false,"show":true,"multiline":true,"value":"","fileTypes":[],"file_path":"","password":false,"name":"context","display_name":"context","advanced":false,"input_types":["Message","Text"],"dynamic":false,"info":"","load_from_db":false,"title_case":false,"type":"str","proxy":{"id":"Prompt-HrBgE","field":"context"}},"code_CreateHeaders-Jw5wk":{"type":"code","required":true,"placeholder":"","list":false,"show":true,"multiline":true,"value":"from langflow.custom import Component\nfrom langflow.inputs import MessageTextInput\nfrom langflow.template import Output\nfrom pydantic.v1 import SecretStr\nfrom langflow.schema import Data\n\n\n\nclass CreateHeadersComponent(Component):\n    display_name = \"Create Headers\"\n    description = \"Creates a headers dictionary with a dynamic Authorization header.\"\n    icon = \"custom_components\"\n    name = \"CreateHeaders\"\n\n    inputs = [\n        MessageTextInput(\n            name=\"input_value\",\n            display_name=\"Authorization Token\",\n            info=\"Enter the token to be used in the Authorization header.\",\n        ),\n    ]\n\n    outputs = [\n        Output(display_name=\"Output\", name=\"output\", method=\"build_output\"),\n    ]\n\n    def build_output(self) -> Data:\n        authorization_header = f\"Bearer {self.input_value}\"\n        headers = {\n            \"Authorization\": authorization_header,\n            \"Content-Type\": \"application/json\",\n        }\n\n        data = Data(value=headers)\n        self.status = data\n        return headers\n        \n        ","fileTypes":[],"file_path":"","password":false,"name":"code","advanced":true,"dynamic":true,"info":"","load_from_db":false,"title_case":false,"display_name":"code","proxy":{"id":"CreateHeaders-Jw5wk","field":"code"}},"input_value_CreateHeaders-Jw5wk":{"trace_as_input":true,"trace_as_metadata":true,"load_from_db":false,"list":false,"required":false,"placeholder":"","show":true,"name":"input_value","value":"","display_name":"Authorization Token","advanced":false,"input_types":["Message"],"dynamic":false,"info":"Enter the token to be used in the Authorization header.","title_case":false,"type":"str","_input_type":"MessageTextInput","proxy":{"id":"CreateHeaders-Jw5wk","field":"input_value"}}},"flow":{"data":{"nodes":[{"id":"APIRequest-XsMPY","type":"genericNode","position":{"x":-160.1020978191558,"y":-152.81819864712116},"data":{"type":"APIRequest","node":{"template":{"_type":"Component","query_params":{"trace_as_input":true,"trace_as_metadata":true,"list":false,"required":false,"placeholder":"","show":true,"value":"","name":"query_params","display_name":"Query Parameters","advanced":false,"input_types":["Data"],"dynamic":false,"info":"The query parameters to append to the URL.","title_case":false,"type":"other","_input_type":"DataInput"},"body":{"trace_as_input":true,"trace_as_metadata":true,"list":false,"required":false,"placeholder":"","show":true,"value":{},"name":"body","display_name":"Body","advanced":false,"input_types":["Data"],"dynamic":false,"info":"The body to send with the request as a dictionary (for POST, PATCH, PUT). This is populated when using the CURL field.","title_case":false,"type":"NestedDict","_input_type":"NestedDictInput"},"code":{"type":"code","required":true,"placeholder":"","list":false,"show":true,"multiline":true,"value":"import asyncio\nimport json\nfrom typing import Any, List, Optional\nfrom urllib.parse import parse_qsl, urlencode, urlparse, urlunparse\n\nimport httpx\nfrom loguru import logger\n\nfrom langflow.base.curl.parse import parse_context\nfrom langflow.custom import Component\nfrom langflow.io import DataInput, DropdownInput, IntInput, MessageTextInput, NestedDictInput, Output\nfrom langflow.schema import Data\nfrom langflow.schema.dotdict import dotdict\n\n\nclass APIRequestComponent(Component):\n    display_name = \"API Request\"\n    description = (\n        \"This component allows you to make HTTP requests to one or more URLs. \"\n        \"You can provide headers and body as either dictionaries or Data objects. \"\n        \"Additionally, you can append query parameters to the URLs.\\n\\n\"\n        \"**Note:** Check advanced options for more settings.\"\n    )\n    icon = \"Globe\"\n    name = \"APIRequest\"\n\n    inputs = [\n        MessageTextInput(\n            name=\"urls\",\n            display_name=\"URLs\",\n            is_list=True,\n            info=\"Enter one or more URLs, separated by commas.\",\n        ),\n        MessageTextInput(\n            name=\"curl\",\n            display_name=\"Curl\",\n            info=\"Paste a curl command to populate the fields. This will fill in the dictionary fields for headers and body.\",\n            advanced=False,\n            refresh_button=True,\n        ),\n        DropdownInput(\n            name=\"method\",\n            display_name=\"Method\",\n            options=[\"GET\", \"POST\", \"PATCH\", \"PUT\"],\n            value=\"GET\",\n            info=\"The HTTP method to use (GET, POST, PATCH, PUT).\",\n        ),\n        NestedDictInput(\n            name=\"headers\",\n            display_name=\"Headers\",\n            info=\"The headers to send with the request as a dictionary. This is populated when using the CURL field.\",\n            input_types=[\"Data\"],\n        ),\n        NestedDictInput(\n            name=\"body\",\n            display_name=\"Body\",\n            info=\"The body to send with the request as a dictionary (for POST, PATCH, PUT). This is populated when using the CURL field.\",\n            input_types=[\"Data\"],\n        ),\n        DataInput(\n            name=\"query_params\",\n            display_name=\"Query Parameters\",\n            info=\"The query parameters to append to the URL.\",\n        ),\n        IntInput(\n            name=\"timeout\",\n            display_name=\"Timeout\",\n            value=5,\n            info=\"The timeout to use for the request.\",\n        ),\n    ]\n\n    outputs = [\n        Output(display_name=\"Data\", name=\"data\", method=\"make_requests\"),\n    ]\n\n    def parse_curl(self, curl: str, build_config: dotdict) -> dotdict:\n        try:\n            parsed = parse_context(curl)\n            build_config[\"urls\"][\"value\"] = [parsed.url]\n            build_config[\"method\"][\"value\"] = parsed.method.upper()\n            build_config[\"headers\"][\"value\"] = dict(parsed.headers)\n\n            if parsed.data:\n                try:\n                    json_data = json.loads(parsed.data)\n                    build_config[\"body\"][\"value\"] = json_data\n                except json.JSONDecodeError as e:\n                    logger.error(f\"Error decoding JSON data: {e}\")\n            else:\n                build_config[\"body\"][\"value\"] = {}\n        except Exception as exc:\n            logger.error(f\"Error parsing curl: {exc}\")\n            raise ValueError(f\"Error parsing curl: {exc}\")\n        return build_config\n\n    def update_build_config(self, build_config: dotdict, field_value: Any, field_name: str | None = None):\n        if field_name == \"curl\" and field_value:\n            build_config = self.parse_curl(field_value, build_config)\n        return build_config\n\n    async def make_request(\n        self,\n        client: httpx.AsyncClient,\n        method: str,\n        url: str,\n        headers: Optional[dict] = None,\n        body: Optional[dict] = None,\n        timeout: int = 5,\n    ) -> Data:\n        method = method.upper()\n        if method not in [\"GET\", \"POST\", \"PATCH\", \"PUT\", \"DELETE\"]:\n            raise ValueError(f\"Unsupported method: {method}\")\n\n        if isinstance(body, str) and body:\n            try:\n                body = json.loads(body)\n            except Exception as e:\n                logger.error(f\"Error decoding JSON data: {e}\")\n                body = None\n                raise ValueError(f\"Error decoding JSON data: {e}\")\n\n        data = body if body else None\n\n        try:\n            response = await client.request(method, url, headers=headers, json=data, timeout=timeout)\n            try:\n                result = response.json()\n            except Exception:\n                result = response.text\n            return Data(\n                data={\n                    \"source\": url,\n                    \"headers\": headers,\n                    \"status_code\": response.status_code,\n                    \"result\": result,\n                },\n            )\n        except httpx.TimeoutException:\n            return Data(\n                data={\n                    \"source\": url,\n                    \"headers\": headers,\n                    \"status_code\": 408,\n                    \"error\": \"Request timed out\",\n                },\n            )\n        except Exception as exc:\n            return Data(\n                data={\n                    \"source\": url,\n                    \"headers\": headers,\n                    \"status_code\": 500,\n                    \"error\": str(exc),\n                },\n            )\n\n    def add_query_params(self, url: str, params: dict) -> str:\n        url_parts = list(urlparse(url))\n        query = dict(parse_qsl(url_parts[4]))\n        query.update(params)\n        url_parts[4] = urlencode(query)\n        return urlunparse(url_parts)\n\n    async def make_requests(self) -> List[Data]:\n        method = self.method\n        urls = [url.strip() for url in self.urls if url.strip()]\n        curl = self.curl\n        headers = self.headers or {}\n        body = self.body or {}\n        timeout = self.timeout\n        query_params = self.query_params.data if self.query_params else {}\n\n        if curl:\n            self._build_config = self.parse_curl(curl, dotdict())\n\n        if isinstance(headers, Data):\n            headers = headers.data\n\n        if isinstance(body, Data):\n            body = body.data\n\n        bodies = [body] * len(urls)\n\n        urls = [self.add_query_params(url, query_params) for url in urls]\n\n        async with httpx.AsyncClient() as client:\n            results = await asyncio.gather(\n                *[self.make_request(client, method, u, headers, rec, timeout) for u, rec in zip(urls, bodies)]\n            )\n        self.status = results\n        return results\n","fileTypes":[],"file_path":"","password":false,"name":"code","advanced":true,"dynamic":true,"info":"","load_from_db":false,"title_case":false,"display_name":"code"},"curl":{"trace_as_input":true,"trace_as_metadata":true,"load_from_db":false,"list":false,"required":false,"placeholder":"","show":true,"value":"","name":"curl","display_name":"Curl","advanced":false,"input_types":["Message"],"dynamic":false,"info":"Paste a curl command to populate the fields. This will fill in the dictionary fields for headers and body.","refresh_button":true,"title_case":false,"type":"str","_input_type":"MessageTextInput"},"headers":{"trace_as_input":true,"trace_as_metadata":true,"list":false,"required":false,"placeholder":"","show":true,"value":{},"name":"headers","display_name":"Headers","advanced":false,"input_types":["Data"],"dynamic":false,"info":"The headers to send with the request as a dictionary. This is populated when using the CURL field.","title_case":false,"type":"NestedDict","_input_type":"NestedDictInput"},"method":{"trace_as_metadata":true,"options":["GET","POST","PATCH","PUT"],"combobox":false,"required":false,"placeholder":"","show":true,"value":"POST","name":"method","display_name":"Method","advanced":false,"dynamic":false,"info":"The HTTP method to use (GET, POST, PATCH, PUT).","title_case":false,"type":"str","_input_type":"DropdownInput"},"timeout":{"trace_as_metadata":true,"list":false,"required":false,"placeholder":"","show":true,"value":"5","name":"timeout","display_name":"Timeout","advanced":false,"dynamic":false,"info":"The timeout to use for the request.","title_case":false,"type":"int","_input_type":"IntInput"},"urls":{"trace_as_input":true,"trace_as_metadata":true,"load_from_db":false,"list":true,"required":false,"placeholder":"","show":true,"value":["https://bigquery.googleapis.com/bigquery/v2/projects/spherical-proxy-424916-c9/queries"],"name":"urls","display_name":"URLs","advanced":false,"input_types":["Message"],"dynamic":false,"info":"Enter one or more URLs, separated by commas.","title_case":false,"type":"str","_input_type":"MessageTextInput"}},"description":"This component allows you to make HTTP requests to one or more URLs. You can provide headers and body as either dictionaries or Data objects. Additionally, you can append query parameters to the URLs.\n\n**Note:** Check advanced options for more settings.","icon":"Globe","base_classes":["Data"],"display_name":"API Request","documentation":"","custom_fields":{},"output_types":[],"pinned":false,"conditional_paths":[],"frozen":false,"outputs":[{"types":["Data"],"selected":"Data","name":"data","display_name":"Data","method":"make_requests","value":"__UNDEFINED__","cache":true,"hidden":false}],"field_order":["urls","curl","method","headers","body","query_params","timeout"],"beta":false,"edited":false,"lf_version":"1.0.17"},"id":"APIRequest-XsMPY"},"selected":true,"width":384,"height":984,"positionAbsolute":{"x":-160.1020978191558,"y":-152.81819864712116},"dragging":false},{"id":"OpenAIModel-OHg1W","type":"genericNode","position":{"x":-1191.748050434836,"y":79.93314207599741},"data":{"type":"OpenAIModel","node":{"template":{"_type":"Component","api_key":{"load_from_db":false,"required":false,"placeholder":"","show":true,"name":"api_key","value":"","display_name":"OpenAI API Key","advanced":false,"input_types":["Message"],"dynamic":false,"info":"The OpenAI API Key to use for the OpenAI model.","title_case":false,"password":true,"type":"str","_input_type":"SecretStrInput"},"code":{"type":"code","required":true,"placeholder":"","list":false,"show":true,"multiline":true,"value":"import operator\nfrom functools import reduce\n\nfrom langflow.field_typing.range_spec import RangeSpec\nfrom langchain_openai import ChatOpenAI\nfrom pydantic.v1 import SecretStr\n\nfrom langflow.base.models.model import LCModelComponent\nfrom langflow.base.models.openai_constants import OPENAI_MODEL_NAMES\nfrom langflow.field_typing import LanguageModel\nfrom langflow.inputs import (\n    BoolInput,\n    DictInput,\n    DropdownInput,\n    FloatInput,\n    IntInput,\n    SecretStrInput,\n    StrInput,\n)\n\n\nclass OpenAIModelComponent(LCModelComponent):\n    display_name = \"OpenAI\"\n    description = \"Generates text using OpenAI LLMs.\"\n    icon = \"OpenAI\"\n    name = \"OpenAIModel\"\n\n    inputs = LCModelComponent._base_inputs + [\n        IntInput(\n            name=\"max_tokens\",\n            display_name=\"Max Tokens\",\n            advanced=True,\n            info=\"The maximum number of tokens to generate. Set to 0 for unlimited tokens.\",\n            range_spec=RangeSpec(min=0, max=128000),\n        ),\n        DictInput(name=\"model_kwargs\", display_name=\"Model Kwargs\", advanced=True),\n        BoolInput(\n            name=\"json_mode\",\n            display_name=\"JSON Mode\",\n            advanced=True,\n            info=\"If True, it will output JSON regardless of passing a schema.\",\n        ),\n        DictInput(\n            name=\"output_schema\",\n            is_list=True,\n            display_name=\"Schema\",\n            advanced=True,\n            info=\"The schema for the Output of the model. You must pass the word JSON in the prompt. If left blank, JSON mode will be disabled.\",\n        ),\n        DropdownInput(\n            name=\"model_name\",\n            display_name=\"Model Name\",\n            advanced=False,\n            options=OPENAI_MODEL_NAMES,\n            value=OPENAI_MODEL_NAMES[0],\n        ),\n        StrInput(\n            name=\"openai_api_base\",\n            display_name=\"OpenAI API Base\",\n            advanced=True,\n            info=\"The base URL of the OpenAI API. Defaults to https://api.openai.com/v1. You can change this to use other APIs like JinaChat, LocalAI and Prem.\",\n        ),\n        SecretStrInput(\n            name=\"api_key\",\n            display_name=\"OpenAI API Key\",\n            info=\"The OpenAI API Key to use for the OpenAI model.\",\n            advanced=False,\n            value=\"OPENAI_API_KEY\",\n        ),\n        FloatInput(name=\"temperature\", display_name=\"Temperature\", value=0.1),\n        IntInput(\n            name=\"seed\",\n            display_name=\"Seed\",\n            info=\"The seed controls the reproducibility of the job.\",\n            advanced=True,\n            value=1,\n        ),\n    ]\n\n    def build_model(self) -> LanguageModel:  # type: ignore[type-var]\n        # self.output_schema is a list of dictionaries\n        # let's convert it to a dictionary\n        output_schema_dict: dict[str, str] = reduce(operator.ior, self.output_schema or {}, {})\n        openai_api_key = self.api_key\n        temperature = self.temperature\n        model_name: str = self.model_name\n        max_tokens = self.max_tokens\n        model_kwargs = self.model_kwargs or {}\n        openai_api_base = self.openai_api_base or \"https://api.openai.com/v1\"\n        json_mode = bool(output_schema_dict) or self.json_mode\n        seed = self.seed\n\n        if openai_api_key:\n            api_key = SecretStr(openai_api_key)\n        else:\n            api_key = None\n        output = ChatOpenAI(\n            max_tokens=max_tokens or None,\n            model_kwargs=model_kwargs,\n            model=model_name,\n            base_url=openai_api_base,\n            api_key=api_key,\n            temperature=temperature if temperature is not None else 0.1,\n            seed=seed,\n        )\n        if json_mode:\n            if output_schema_dict:\n                output = output.with_structured_output(schema=output_schema_dict, method=\"json_mode\")  # type: ignore\n            else:\n                output = output.bind(response_format={\"type\": \"json_object\"})  # type: ignore\n\n        return output  # type: ignore\n\n    def _get_exception_message(self, e: Exception):\n        \"\"\"\n        Get a message from an OpenAI exception.\n\n        Args:\n            exception (Exception): The exception to get the message from.\n\n        Returns:\n            str: The message from the exception.\n        \"\"\"\n\n        try:\n            from openai import BadRequestError\n        except ImportError:\n            return\n        if isinstance(e, BadRequestError):\n            message = e.body.get(\"message\")  # type: ignore\n            if message:\n                return message\n        return\n","fileTypes":[],"file_path":"","password":false,"name":"code","advanced":true,"dynamic":true,"info":"","load_from_db":false,"title_case":false,"display_name":"code"},"input_value":{"trace_as_input":true,"trace_as_metadata":true,"load_from_db":false,"list":false,"required":false,"placeholder":"","show":true,"name":"input_value","value":"","display_name":"Input","advanced":false,"input_types":["Message"],"dynamic":false,"info":"","title_case":false,"type":"str","_input_type":"MessageInput"},"json_mode":{"trace_as_metadata":true,"list":false,"required":false,"placeholder":"","show":true,"name":"json_mode","value":false,"display_name":"JSON Mode","advanced":true,"dynamic":false,"info":"If True, it will output JSON regardless of passing a schema.","title_case":false,"type":"bool","_input_type":"BoolInput"},"max_tokens":{"trace_as_metadata":true,"range_spec":{"step_type":"float","min":0,"max":128000,"step":0.1},"list":false,"required":false,"placeholder":"","show":true,"name":"max_tokens","value":"","display_name":"Max Tokens","advanced":true,"dynamic":false,"info":"The maximum number of tokens to generate. Set to 0 for unlimited tokens.","title_case":false,"type":"int","_input_type":"IntInput"},"model_kwargs":{"trace_as_input":true,"list":false,"required":false,"placeholder":"","show":true,"name":"model_kwargs","value":{},"display_name":"Model Kwargs","advanced":true,"dynamic":false,"info":"","title_case":false,"type":"dict","_input_type":"DictInput"},"model_name":{"trace_as_metadata":true,"options":["gpt-4o-mini","gpt-4o","gpt-4-turbo","gpt-4-turbo-preview","gpt-4","gpt-3.5-turbo","gpt-3.5-turbo-0125"],"combobox":false,"required":false,"placeholder":"","show":true,"name":"model_name","value":"gpt-4o","display_name":"Model Name","advanced":false,"dynamic":false,"info":"","title_case":false,"type":"str","_input_type":"DropdownInput"},"openai_api_base":{"trace_as_metadata":true,"load_from_db":false,"list":false,"required":false,"placeholder":"","show":true,"name":"openai_api_base","value":"","display_name":"OpenAI API Base","advanced":true,"dynamic":false,"info":"The base URL of the OpenAI API. Defaults to https://api.openai.com/v1. You can change this to use other APIs like JinaChat, LocalAI and Prem.","title_case":false,"type":"str","_input_type":"StrInput"},"output_schema":{"trace_as_input":true,"list":true,"required":false,"placeholder":"","show":true,"name":"output_schema","value":{},"display_name":"Schema","advanced":true,"dynamic":false,"info":"The schema for the Output of the model. You must pass the word JSON in the prompt. If left blank, JSON mode will be disabled.","title_case":false,"type":"dict","_input_type":"DictInput"},"seed":{"trace_as_metadata":true,"list":false,"required":false,"placeholder":"","show":true,"name":"seed","value":1,"display_name":"Seed","advanced":true,"dynamic":false,"info":"The seed controls the reproducibility of the job.","title_case":false,"type":"int","_input_type":"IntInput"},"stream":{"trace_as_metadata":true,"list":false,"required":false,"placeholder":"","show":true,"name":"stream","value":false,"display_name":"Stream","advanced":true,"dynamic":false,"info":"Stream the response from the model. Streaming works only in Chat.","title_case":false,"type":"bool","_input_type":"BoolInput"},"system_message":{"trace_as_input":true,"trace_as_metadata":true,"load_from_db":false,"list":false,"required":false,"placeholder":"","show":true,"name":"system_message","value":"","display_name":"System Message","advanced":true,"input_types":["Message"],"dynamic":false,"info":"System message to pass to the model.","title_case":false,"type":"str","_input_type":"MessageTextInput"},"temperature":{"trace_as_metadata":true,"list":false,"required":false,"placeholder":"","show":true,"name":"temperature","value":0.1,"display_name":"Temperature","advanced":false,"dynamic":false,"info":"","title_case":false,"type":"float","_input_type":"FloatInput"}},"description":"Generates text using OpenAI LLMs.","icon":"OpenAI","base_classes":["LanguageModel","Message"],"display_name":"OpenAI","documentation":"","custom_fields":{},"output_types":[],"pinned":false,"conditional_paths":[],"frozen":false,"outputs":[{"types":["Message"],"selected":"Message","name":"text_output","display_name":"Text","method":"text_response","value":"__UNDEFINED__","cache":true},{"types":["LanguageModel"],"selected":"LanguageModel","name":"model_output","display_name":"Language Model","method":"build_model","value":"__UNDEFINED__","cache":true}],"field_order":["input_value","system_message","stream","max_tokens","model_kwargs","json_mode","output_schema","model_name","openai_api_base","api_key","temperature","seed"],"beta":false,"edited":false,"lf_version":"1.0.17"},"id":"OpenAIModel-OHg1W","description":"Generates text using OpenAI LLMs.","display_name":"OpenAI"},"selected":true,"width":384,"height":601,"positionAbsolute":{"x":-1191.748050434836,"y":79.93314207599741},"dragging":false},{"id":"Prompt-AOko9","type":"genericNode","position":{"x":-1652.4360176582995,"y":115.66517557500373},"data":{"type":"Prompt","node":{"template":{"_type":"Component","code":{"type":"code","required":true,"placeholder":"","list":false,"show":true,"multiline":true,"value":"from langflow.base.prompts.api_utils import process_prompt_template\nfrom langflow.custom import Component\nfrom langflow.inputs.inputs import DefaultPromptField\nfrom langflow.io import Output, PromptInput\nfrom langflow.schema.message import Message\nfrom langflow.template.utils import update_template_values\n\n\nclass PromptComponent(Component):\n    display_name: str = \"Prompt\"\n    description: str = \"Create a prompt template with dynamic variables.\"\n    icon = \"prompts\"\n    trace_type = \"prompt\"\n    name = \"Prompt\"\n\n    inputs = [\n        PromptInput(name=\"template\", display_name=\"Template\"),\n    ]\n\n    outputs = [\n        Output(display_name=\"Prompt Message\", name=\"prompt\", method=\"build_prompt\"),\n    ]\n\n    async def build_prompt(\n        self,\n    ) -> Message:\n        prompt = await Message.from_template_and_variables(**self._attributes)\n        self.status = prompt.text\n        return prompt\n\n    def _update_template(self, frontend_node: dict):\n        prompt_template = frontend_node[\"template\"][\"template\"][\"value\"]\n        custom_fields = frontend_node[\"custom_fields\"]\n        frontend_node_template = frontend_node[\"template\"]\n        _ = process_prompt_template(\n            template=prompt_template,\n            name=\"template\",\n            custom_fields=custom_fields,\n            frontend_node_template=frontend_node_template,\n        )\n        return frontend_node\n\n    def post_code_processing(self, new_frontend_node: dict, current_frontend_node: dict):\n        \"\"\"\n        This function is called after the code validation is done.\n        \"\"\"\n        frontend_node = super().post_code_processing(new_frontend_node, current_frontend_node)\n        template = frontend_node[\"template\"][\"template\"][\"value\"]\n        # Kept it duplicated for backwards compatibility\n        _ = process_prompt_template(\n            template=template,\n            name=\"template\",\n            custom_fields=frontend_node[\"custom_fields\"],\n            frontend_node_template=frontend_node[\"template\"],\n        )\n        # Now that template is updated, we need to grab any values that were set in the current_frontend_node\n        # and update the frontend_node with those values\n        update_template_values(new_template=frontend_node, previous_template=current_frontend_node[\"template\"])\n        return frontend_node\n\n    def _get_fallback_input(self, **kwargs):\n        return DefaultPromptField(**kwargs)\n","fileTypes":[],"file_path":"","password":false,"name":"code","advanced":true,"dynamic":true,"info":"","load_from_db":false,"title_case":false,"display_name":"code"},"template":{"trace_as_input":true,"list":false,"required":false,"placeholder":"","show":true,"name":"template","value":"**Context** :\n{context}\n**User Query** :\n{user_prompt}\n\n---\n\n**Task** : Create a GA4 SQL query to the `bigquery-public-data.ga4_obfuscated_sample_ecommerce.events_20201209` table. Use the schema information provided below to ensure the query is valid. \n1. **Field Validation** :\n  - Only reference fields that exist in the schema provided below.\n \n  - Use the correct dot notation when accessing fields within complex data structures like `RECORD` (also known as `STRUCT`). For example, access `ecommerce.purchase_revenue_in_usd` directly if it’s a non-repeated field.\n \n  - Use `UNNEST` only for repeated fields (arrays). Do not use `UNNEST` for fields that are simple `STRUCT` (single-record) types. For example, `event_params` and `user_properties` require `UNNEST`, while `ecommerce` does not.\n \n2. **Query Structure** : \n  - Validate that the query structure is correct and does not produce errors related to data types, field names, or incorrect usage of `UNNEST`.\n \n  - Access fields within `STRUCT` (or `RECORD`) directly using dot notation without `UNNEST` if they are not repeated. For instance, fields within `ecommerce` can be accessed directly.\n\n  - Ensure that all referenced fields are appropriately handled, especially when dealing with nested or repeated fields.\n \n3. **Error Handling** :\n  - If a query cannot be formed due to a missing or invalid field in the schema, return an error message instead of a query.\n\n\n---\n\nSchema Information for `events_20201209` Table** :\n\n```json\ncolumn_name,data_type,is_nullable\nevent_date,STRING,YES\nevent_timestamp,INT64,YES\nevent_name,STRING,YES\nevent_params,\"ARRAY<STRUCT<key STRING, value STRUCT<string_value STRING, int_value INT64, float_value FLOAT64, double_value FLOAT64>>>\",NO\nevent_previous_timestamp,INT64,YES\nevent_value_in_usd,FLOAT64,YES\nevent_bundle_sequence_id,INT64,YES\nevent_server_timestamp_offset,INT64,YES\nuser_id,STRING,YES\nuser_pseudo_id,STRING,YES\nprivacy_info,\"STRUCT<analytics_storage INT64, ads_storage INT64, uses_transient_token STRING>\",YES\nuser_properties,\"ARRAY<STRUCT<key INT64, value STRUCT<string_value INT64, int_value INT64, float_value INT64, double_value INT64, set_timestamp_micros INT64>>>\",NO\nuser_first_touch_timestamp,INT64,YES\nuser_ltv,\"STRUCT<revenue FLOAT64, currency STRING>\",YES\ndevice,\"STRUCT<category STRING, mobile_brand_name STRING, mobile_model_name STRING, mobile_marketing_name STRING, mobile_os_hardware_model INT64, operating_system STRING, operating_system_version STRING, vendor_id INT64, advertising_id INT64, language STRING, is_limited_ad_tracking STRING, time_zone_offset_seconds INT64, web_info STRUCT<browser STRING, browser_version STRING>>\",YES\ngeo,\"STRUCT<continent STRING, sub_continent STRING, country STRING, region STRING, city STRING, metro STRING>\",YES\napp_info,\"STRUCT<id STRING, version STRING, install_store STRING, firebase_app_id STRING, install_source STRING>\",YES\ntraffic_source,\"STRUCT<medium STRING, name STRING, source STRING>\",YES\nstream_id,INT64,YES\nplatform,STRING,YES\nevent_dimensions,STRUCT<hostname STRING>,YES\necommerce,\"STRUCT<total_item_quantity INT64, purchase_revenue_in_usd FLOAT64, purchase_revenue FLOAT64, refund_value_in_usd FLOAT64, refund_value FLOAT64, shipping_value_in_usd FLOAT64, shipping_value FLOAT64, tax_value_in_usd FLOAT64, tax_value FLOAT64, unique_items INT64, transaction_id STRING>\",YES\nitems,\"ARRAY<STRUCT<item_id STRING, item_name STRING, item_brand STRING, item_variant STRING, item_category STRING, item_category2 STRING, item_category3 STRING, item_category4 STRING, item_category5 STRING, price_in_usd FLOAT64, price FLOAT64, quantity INT64, item_revenue_in_usd FLOAT64, item_revenue FLOAT64, item_refund_in_usd FLOAT64, item_refund FLOAT64, coupon STRING, affiliation STRING, location_id STRING, item_list_id STRING, item_list_name STRING, item_list_index STRING, promotion_id STRING, promotion_name STRING, creative_name STRING, creative_slot STRING>>\",NO\n```\n\n\n---\n\n**Output Requirement** : Your output must be **only**  the SQL query like the example provided:\n\n```sql\nSELECT * FROM `bigquery-public-data.ga4_obfuscated_sample_ecommerce.events_20201209` LIMIT 0\n```\n\nIf you cannot generate a valid query due to missing or invalid fields, return an error message instead. Ensure that you are running a unique query that was not ran before in the context.\n\n\n---\n\n**AI** :","display_name":"Template","advanced":false,"dynamic":false,"info":"","title_case":false,"type":"prompt","_input_type":"PromptInput","load_from_db":false},"context":{"field_type":"str","required":false,"placeholder":"","list":false,"show":true,"multiline":true,"value":"","fileTypes":[],"file_path":"","password":false,"name":"context","display_name":"context","advanced":false,"input_types":["Message","Text"],"dynamic":false,"info":"","load_from_db":false,"title_case":false,"type":"str"},"user_prompt":{"field_type":"str","required":false,"placeholder":"","list":false,"show":true,"multiline":true,"value":"","fileTypes":[],"file_path":"","password":false,"name":"user_prompt","display_name":"user_prompt","advanced":false,"input_types":["Message","Text"],"dynamic":false,"info":"","load_from_db":false,"title_case":false,"type":"str"}},"description":"Create a prompt template with dynamic variables.","icon":"prompts","is_input":null,"is_output":null,"is_composition":null,"base_classes":["Message"],"name":"","display_name":"Prompt","documentation":"","custom_fields":{"template":["context","user_prompt"]},"output_types":[],"full_path":null,"pinned":false,"conditional_paths":[],"frozen":false,"outputs":[{"types":["Message"],"selected":"Message","name":"prompt","hidden":null,"display_name":"Prompt Message","method":"build_prompt","value":"__UNDEFINED__","cache":true}],"field_order":["template"],"beta":false,"error":null,"edited":false},"id":"Prompt-AOko9","description":"Create a prompt template with dynamic variables.","display_name":"Prompt"},"selected":true,"width":384,"height":498,"positionAbsolute":{"x":-1652.4360176582995,"y":115.66517557500373},"dragging":false},{"id":"CustomComponent-NwxNM","type":"genericNode","position":{"x":-722.2114864428826,"y":795.4262181316096},"data":{"type":"CustomComponent","node":{"template":{"_type":"Component","code":{"type":"code","required":true,"placeholder":"","list":false,"show":true,"multiline":true,"value":"# from langflow.field_typing import Data\nfrom langflow.custom import Component\nfrom langflow.io import MessageTextInput, Output\nfrom langflow.schema import Data\nimport re  # Import regex module\n\nclass CustomComponent(Component):\n    display_name = \"Format Body\"\n    description = \"Turn OpenAI SQL output into a body to send with a fetch.\"\n    documentation: str = \"http://docs.langflow.org/components/custom\"\n    icon = \"custom_components\"\n    name = \"CustomComponent\"\n\n    inputs = [\n        MessageTextInput(name=\"input_value\", display_name=\"Input Value\", value=\"```sql\\nbigquery-public-data.ga4_obfuscated_sample_ecommerce.events_*\\n```\"),\n    ]\n\n    outputs = [\n        Output(display_name=\"Output\", name=\"output\", method=\"build_output\"),\n    ]\n\n    def build_output(self) -> Data:\n        # Remove the surrounding markdown and any trailing newline\n        input_value_cleaned = re.sub(r\"^```sql\\n|```$\", \"\", self.input_value.strip()).strip()\n\n        # Format the cleaned SQL string into the desired object\n        formatted_data = {\n            \"query\": input_value_cleaned,\n            \"maxResults\": 20,\n            \"timeoutMs\": 10000,\n            \"useLegacySql\": False\n        }\n        \n        # Return the formatted data as the output\n        data = Data(value=formatted_data)\n        self.status = data\n        \n        # Print the formatted data to the chat (for debugging purposes)\n        print(f\"Output Data: {formatted_data}\")\n        \n        return formatted_data\n","fileTypes":[],"file_path":"","password":false,"name":"code","advanced":true,"dynamic":true,"info":"","load_from_db":false,"title_case":false,"display_name":"code"},"input_value":{"trace_as_input":true,"trace_as_metadata":true,"load_from_db":false,"list":false,"required":false,"placeholder":"","show":true,"name":"input_value","value":"","display_name":"Input Value","advanced":false,"input_types":["Message"],"dynamic":false,"info":"","title_case":false,"type":"str","_input_type":"MessageTextInput"}},"description":"Turn OpenAI SQL output into a body to send with a fetch.","icon":"custom_components","base_classes":["Data"],"display_name":"Custom Component","documentation":"http://docs.langflow.org/components/custom","custom_fields":{},"output_types":[],"pinned":false,"conditional_paths":[],"frozen":false,"outputs":[{"types":["Data"],"selected":"Data","name":"output","display_name":"Output","method":"build_output","value":"__UNDEFINED__","cache":true}],"field_order":["input_value"],"beta":false,"edited":true,"lf_version":"1.0.17"},"id":"CustomComponent-NwxNM"},"selected":true,"width":384,"height":326,"dragging":false,"positionAbsolute":{"x":-722.2114864428826,"y":795.4262181316096}},{"id":"FetchProcessorComponent-jzDpE","type":"genericNode","position":{"x":373.18673165075575,"y":648.0200366696015},"data":{"type":"FetchProcessorComponent","node":{"template":{"_type":"Component","fetch_response":{"trace_as_metadata":true,"list":false,"required":false,"placeholder":"","show":true,"name":"fetch_response","value":"","display_name":"Fetch Response","advanced":false,"input_types":["Data"],"dynamic":false,"info":"","title_case":false,"type":"other","_input_type":"HandleInput"},"code":{"type":"code","required":true,"placeholder":"","list":false,"show":true,"multiline":true,"value":"from langflow.custom import Component\nfrom langflow.io import MessageTextInput, Output\nfrom langflow.schema import Data\nfrom langflow.schema.message import Message\nimport json\n\n\nclass FetchProcessorComponent(Component):\n    display_name = \"Fetch Processor\"\n    description = \"Process fetch output, print it, and stringify it for chat.\"\n    documentation: str = \"http://docs.langflow.org/components/custom\"\n    icon = \"custom_components\"\n    name = \"FetchProcessorComponent\"\n\n    inputs = [\n        HandleInput(\n            name=\"fetch_response\",\n            display_name=\"Fetch Response\",\n            input_types=[\"Data\"],\n        )\n    ]\n\n    outputs = [\n        Output(display_name=\"Processed Output\", name=\"processed_output\", method=\"process_output\"),\n    ]\n    \n    def process_output(self) -> Message:\n        try:\n            # Directly use the fetch_response as the input data\n            response_data = self.fetch_response\n\n            # Extract the response content\n            content = str(response_data[0].data[\"result\"])\n            \n            # Truncate the content to a specific length (e.g., 5000 characters)\n            max_length = 5000\n            truncated_content = content[:max_length]\n            \n            # Convert the truncated content to a string that can be printed\n            output_str = \"Processed Output: \" + str(truncated_content)+\"...\"\n            \n            \n            # Return the processed string as a Message\n            self.status = output_str\n            return Message(text=output_str)\n\n        except Exception as e:\n            # Handle any errors and print them\n            error_message = f\"Error processing fetch response: {str(e)}\"\n            print(error_message)\n            return Message(text=error_message)\n","fileTypes":[],"file_path":"","password":false,"name":"code","advanced":true,"dynamic":true,"info":"","load_from_db":false,"title_case":false,"display_name":"code"}},"description":"Process fetch output, print it, and stringify it for chat.","icon":"custom_components","base_classes":["Message"],"display_name":"Custom Component","documentation":"http://docs.langflow.org/components/custom","custom_fields":{},"output_types":[],"pinned":false,"conditional_paths":[],"frozen":false,"outputs":[{"types":["Message"],"selected":"Message","name":"processed_output","display_name":"Processed Output","method":"process_output","value":"__UNDEFINED__","cache":true}],"field_order":["fetch_response"],"beta":false,"edited":true,"lf_version":"1.0.17"},"id":"FetchProcessorComponent-jzDpE"},"selected":true,"width":384,"height":288,"dragging":false,"positionAbsolute":{"x":373.18673165075575,"y":648.0200366696015}},{"id":"OpenAIModel-BxXKu","type":"genericNode","position":{"x":1420.9686803289817,"y":501.3038614984411},"data":{"type":"OpenAIModel","node":{"template":{"_type":"Component","api_key":{"load_from_db":false,"required":false,"placeholder":"","show":true,"value":"","name":"api_key","display_name":"OpenAI API Key","advanced":false,"input_types":["Message"],"dynamic":false,"info":"The OpenAI API Key to use for the OpenAI model.","title_case":false,"password":true,"type":"str","_input_type":"SecretStrInput"},"code":{"type":"code","required":true,"placeholder":"","list":false,"show":true,"multiline":true,"value":"import operator\nfrom functools import reduce\n\nfrom langflow.field_typing.range_spec import RangeSpec\nfrom langchain_openai import ChatOpenAI\nfrom pydantic.v1 import SecretStr\n\nfrom langflow.base.models.model import LCModelComponent\nfrom langflow.base.models.openai_constants import OPENAI_MODEL_NAMES\nfrom langflow.field_typing import LanguageModel\nfrom langflow.inputs import (\n    BoolInput,\n    DictInput,\n    DropdownInput,\n    FloatInput,\n    IntInput,\n    SecretStrInput,\n    StrInput,\n)\n\n\nclass OpenAIModelComponent(LCModelComponent):\n    display_name = \"OpenAI\"\n    description = \"Generates text using OpenAI LLMs.\"\n    icon = \"OpenAI\"\n    name = \"OpenAIModel\"\n\n    inputs = LCModelComponent._base_inputs + [\n        IntInput(\n            name=\"max_tokens\",\n            display_name=\"Max Tokens\",\n            advanced=True,\n            info=\"The maximum number of tokens to generate. Set to 0 for unlimited tokens.\",\n            range_spec=RangeSpec(min=0, max=128000),\n        ),\n        DictInput(name=\"model_kwargs\", display_name=\"Model Kwargs\", advanced=True),\n        BoolInput(\n            name=\"json_mode\",\n            display_name=\"JSON Mode\",\n            advanced=True,\n            info=\"If True, it will output JSON regardless of passing a schema.\",\n        ),\n        DictInput(\n            name=\"output_schema\",\n            is_list=True,\n            display_name=\"Schema\",\n            advanced=True,\n            info=\"The schema for the Output of the model. You must pass the word JSON in the prompt. If left blank, JSON mode will be disabled.\",\n        ),\n        DropdownInput(\n            name=\"model_name\",\n            display_name=\"Model Name\",\n            advanced=False,\n            options=OPENAI_MODEL_NAMES,\n            value=OPENAI_MODEL_NAMES[0],\n        ),\n        StrInput(\n            name=\"openai_api_base\",\n            display_name=\"OpenAI API Base\",\n            advanced=True,\n            info=\"The base URL of the OpenAI API. Defaults to https://api.openai.com/v1. You can change this to use other APIs like JinaChat, LocalAI and Prem.\",\n        ),\n        SecretStrInput(\n            name=\"api_key\",\n            display_name=\"OpenAI API Key\",\n            info=\"The OpenAI API Key to use for the OpenAI model.\",\n            advanced=False,\n            value=\"OPENAI_API_KEY\",\n        ),\n        FloatInput(name=\"temperature\", display_name=\"Temperature\", value=0.1),\n        IntInput(\n            name=\"seed\",\n            display_name=\"Seed\",\n            info=\"The seed controls the reproducibility of the job.\",\n            advanced=True,\n            value=1,\n        ),\n    ]\n\n    def build_model(self) -> LanguageModel:  # type: ignore[type-var]\n        # self.output_schema is a list of dictionaries\n        # let's convert it to a dictionary\n        output_schema_dict: dict[str, str] = reduce(operator.ior, self.output_schema or {}, {})\n        openai_api_key = self.api_key\n        temperature = self.temperature\n        model_name: str = self.model_name\n        max_tokens = self.max_tokens\n        model_kwargs = self.model_kwargs or {}\n        openai_api_base = self.openai_api_base or \"https://api.openai.com/v1\"\n        json_mode = bool(output_schema_dict) or self.json_mode\n        seed = self.seed\n\n        if openai_api_key:\n            api_key = SecretStr(openai_api_key)\n        else:\n            api_key = None\n        output = ChatOpenAI(\n            max_tokens=max_tokens or None,\n            model_kwargs=model_kwargs,\n            model=model_name,\n            base_url=openai_api_base,\n            api_key=api_key,\n            temperature=temperature or 0.1,\n            seed=seed,\n        )\n        if json_mode:\n            if output_schema_dict:\n                output = output.with_structured_output(schema=output_schema_dict, method=\"json_mode\")  # type: ignore\n            else:\n                output = output.bind(response_format={\"type\": \"json_object\"})  # type: ignore\n\n        return output  # type: ignore\n\n    def _get_exception_message(self, e: Exception):\n        \"\"\"\n        Get a message from an OpenAI exception.\n\n        Args:\n            exception (Exception): The exception to get the message from.\n\n        Returns:\n            str: The message from the exception.\n        \"\"\"\n\n        try:\n            from openai import BadRequestError\n        except ImportError:\n            return\n        if isinstance(e, BadRequestError):\n            message = e.body.get(\"message\")  # type: ignore\n            if message:\n                return message\n        return\n","fileTypes":[],"file_path":"","password":false,"name":"code","advanced":true,"dynamic":true,"info":"","load_from_db":false,"title_case":false,"display_name":"code"},"input_value":{"trace_as_input":true,"trace_as_metadata":true,"load_from_db":false,"list":false,"required":false,"placeholder":"","show":true,"value":"","name":"input_value","display_name":"Input","advanced":false,"input_types":["Message"],"dynamic":false,"info":"","title_case":false,"type":"str","_input_type":"MessageInput"},"json_mode":{"trace_as_metadata":true,"list":false,"required":false,"placeholder":"","show":true,"value":false,"name":"json_mode","display_name":"JSON Mode","advanced":true,"dynamic":false,"info":"If True, it will output JSON regardless of passing a schema.","title_case":false,"type":"bool","_input_type":"BoolInput"},"max_tokens":{"trace_as_metadata":true,"range_spec":{"step_type":"float","min":0,"max":128000,"step":0.1},"list":false,"required":false,"placeholder":"","show":true,"value":"","name":"max_tokens","display_name":"Max Tokens","advanced":true,"dynamic":false,"info":"The maximum number of tokens to generate. Set to 0 for unlimited tokens.","title_case":false,"type":"int","_input_type":"IntInput"},"model_kwargs":{"trace_as_input":true,"list":false,"required":false,"placeholder":"","show":true,"value":{},"name":"model_kwargs","display_name":"Model Kwargs","advanced":true,"dynamic":false,"info":"","title_case":false,"type":"dict","_input_type":"DictInput"},"model_name":{"trace_as_metadata":true,"options":["gpt-4o-mini","gpt-4o","gpt-4-turbo","gpt-4-turbo-preview","gpt-4","gpt-3.5-turbo","gpt-3.5-turbo-0125"],"combobox":false,"required":false,"placeholder":"","show":true,"value":"gpt-4o-mini","name":"model_name","display_name":"Model Name","advanced":false,"dynamic":false,"info":"","title_case":false,"type":"str","_input_type":"DropdownInput"},"openai_api_base":{"trace_as_metadata":true,"load_from_db":false,"list":false,"required":false,"placeholder":"","show":true,"value":"","name":"openai_api_base","display_name":"OpenAI API Base","advanced":true,"dynamic":false,"info":"The base URL of the OpenAI API. Defaults to https://api.openai.com/v1. You can change this to use other APIs like JinaChat, LocalAI and Prem.","title_case":false,"type":"str","_input_type":"StrInput"},"output_schema":{"trace_as_input":true,"list":true,"required":false,"placeholder":"","show":true,"value":{},"name":"output_schema","display_name":"Schema","advanced":true,"dynamic":false,"info":"The schema for the Output of the model. You must pass the word JSON in the prompt. If left blank, JSON mode will be disabled.","title_case":false,"type":"dict","_input_type":"DictInput"},"seed":{"trace_as_metadata":true,"list":false,"required":false,"placeholder":"","show":true,"value":1,"name":"seed","display_name":"Seed","advanced":true,"dynamic":false,"info":"The seed controls the reproducibility of the job.","title_case":false,"type":"int","_input_type":"IntInput"},"stream":{"trace_as_metadata":true,"list":false,"required":false,"placeholder":"","show":true,"value":false,"name":"stream","display_name":"Stream","advanced":true,"dynamic":false,"info":"Stream the response from the model. Streaming works only in Chat.","title_case":false,"type":"bool","_input_type":"BoolInput"},"system_message":{"trace_as_input":true,"trace_as_metadata":true,"load_from_db":false,"list":false,"required":false,"placeholder":"","show":true,"value":"","name":"system_message","display_name":"System Message","advanced":true,"input_types":["Message"],"dynamic":false,"info":"System message to pass to the model.","title_case":false,"type":"str","_input_type":"MessageTextInput"},"temperature":{"trace_as_metadata":true,"list":false,"required":false,"placeholder":"","show":true,"value":0.1,"name":"temperature","display_name":"Temperature","advanced":false,"dynamic":false,"info":"","title_case":false,"type":"float","_input_type":"FloatInput"}},"description":"Generates text using OpenAI LLMs.","icon":"OpenAI","base_classes":["LanguageModel","Message"],"display_name":"OpenAI","documentation":"","custom_fields":{},"output_types":[],"pinned":false,"conditional_paths":[],"frozen":false,"outputs":[{"types":["Message"],"selected":"Message","name":"text_output","display_name":"Text","method":"text_response","value":"__UNDEFINED__","cache":true},{"types":["LanguageModel"],"selected":"LanguageModel","name":"model_output","display_name":"Language Model","method":"build_model","value":"__UNDEFINED__","cache":true}],"field_order":["input_value","system_message","stream","max_tokens","model_kwargs","json_mode","output_schema","model_name","openai_api_base","api_key","temperature","seed"],"beta":false,"edited":false,"lf_version":"1.0.17"},"id":"OpenAIModel-BxXKu"},"selected":true,"width":384,"height":601,"positionAbsolute":{"x":1420.9686803289817,"y":501.3038614984411},"dragging":false},{"id":"Prompt-NCppB","type":"genericNode","position":{"x":914.8083947668401,"y":477.7711490769072},"data":{"type":"Prompt","node":{"template":{"_type":"Component","code":{"type":"code","required":true,"placeholder":"","list":false,"show":true,"multiline":true,"value":"from langflow.base.prompts.api_utils import process_prompt_template\nfrom langflow.custom import Component\nfrom langflow.inputs.inputs import DefaultPromptField\nfrom langflow.io import Output, PromptInput\nfrom langflow.schema.message import Message\nfrom langflow.template.utils import update_template_values\n\n\nclass PromptComponent(Component):\n    display_name: str = \"Prompt\"\n    description: str = \"Create a prompt template with dynamic variables.\"\n    icon = \"prompts\"\n    trace_type = \"prompt\"\n    name = \"Prompt\"\n\n    inputs = [\n        PromptInput(name=\"template\", display_name=\"Template\"),\n    ]\n\n    outputs = [\n        Output(display_name=\"Prompt Message\", name=\"prompt\", method=\"build_prompt\"),\n    ]\n\n    async def build_prompt(\n        self,\n    ) -> Message:\n        prompt = await Message.from_template_and_variables(**self._attributes)\n        self.status = prompt.text\n        return prompt\n\n    def _update_template(self, frontend_node: dict):\n        prompt_template = frontend_node[\"template\"][\"template\"][\"value\"]\n        custom_fields = frontend_node[\"custom_fields\"]\n        frontend_node_template = frontend_node[\"template\"]\n        _ = process_prompt_template(\n            template=prompt_template,\n            name=\"template\",\n            custom_fields=custom_fields,\n            frontend_node_template=frontend_node_template,\n        )\n        return frontend_node\n\n    def post_code_processing(self, new_frontend_node: dict, current_frontend_node: dict):\n        \"\"\"\n        This function is called after the code validation is done.\n        \"\"\"\n        frontend_node = super().post_code_processing(new_frontend_node, current_frontend_node)\n        template = frontend_node[\"template\"][\"template\"][\"value\"]\n        # Kept it duplicated for backwards compatibility\n        _ = process_prompt_template(\n            template=template,\n            name=\"template\",\n            custom_fields=frontend_node[\"custom_fields\"],\n            frontend_node_template=frontend_node[\"template\"],\n        )\n        # Now that template is updated, we need to grab any values that were set in the current_frontend_node\n        # and update the frontend_node with those values\n        update_template_values(new_template=frontend_node, previous_template=current_frontend_node[\"template\"])\n        return frontend_node\n\n    def _get_fallback_input(self, **kwargs):\n        return DefaultPromptField(**kwargs)\n","fileTypes":[],"file_path":"","password":false,"name":"code","advanced":true,"dynamic":true,"info":"","load_from_db":false,"title_case":false,"display_name":"code"},"template":{"trace_as_input":true,"list":false,"required":false,"placeholder":"","show":true,"name":"template","value":"Context: {context}\n\nUser: I was given these instructions:\n\n```\n{instructions}\n```\n\n\nI gave this query to run:\n\n```\n{query}\n```\n\nI got this output:\n\n```\n{output}\n```\n\n\nDo the following:\n1. Output the query executed\n2. Output a portion of the results (human readable)\n3. Interpret the results in the context of the instructions\n4. Output the insights that you have learned\n5. Create an action for what you think you should do next using reflection.\n\nIf there was an error, output what the error was\n\nAI:","display_name":"Template","advanced":false,"dynamic":false,"info":"","title_case":false,"type":"prompt","_input_type":"PromptInput","load_from_db":false},"context":{"field_type":"str","required":false,"placeholder":"","list":false,"show":true,"multiline":true,"value":"","fileTypes":[],"file_path":"","password":false,"name":"context","display_name":"context","advanced":false,"input_types":["Message","Text"],"dynamic":false,"info":"","load_from_db":false,"title_case":false,"type":"str"},"instructions":{"field_type":"str","required":false,"placeholder":"","list":false,"show":true,"multiline":true,"value":"","fileTypes":[],"file_path":"","password":false,"name":"instructions","display_name":"instructions","advanced":false,"input_types":["Message","Text"],"dynamic":false,"info":"","load_from_db":false,"title_case":false,"type":"str"},"query":{"field_type":"str","required":false,"placeholder":"","list":false,"show":true,"multiline":true,"value":"","fileTypes":[],"file_path":"","password":false,"name":"query","display_name":"query","advanced":false,"input_types":["Message","Text"],"dynamic":false,"info":"","load_from_db":false,"title_case":false,"type":"str"},"output":{"field_type":"str","required":false,"placeholder":"","list":false,"show":true,"multiline":true,"value":"","fileTypes":[],"file_path":"","password":false,"name":"output","display_name":"output","advanced":false,"input_types":["Message","Text"],"dynamic":false,"info":"","load_from_db":false,"title_case":false,"type":"str"}},"description":"Create a prompt template with dynamic variables.","icon":"prompts","is_input":null,"is_output":null,"is_composition":null,"base_classes":["Message"],"name":"","display_name":"Prompt","documentation":"","custom_fields":{"template":["context","instructions","query","output"]},"output_types":[],"full_path":null,"pinned":false,"conditional_paths":[],"frozen":false,"outputs":[{"types":["Message"],"selected":"Message","name":"prompt","hidden":null,"display_name":"Prompt Message","method":"build_prompt","value":"__UNDEFINED__","cache":true}],"field_order":["template"],"beta":false,"error":null,"edited":false},"id":"Prompt-NCppB","description":"Create a prompt template with dynamic variables.","display_name":"Prompt"},"selected":true,"width":384,"height":670,"dragging":false,"positionAbsolute":{"x":914.8083947668401,"y":477.7711490769072}},{"id":"Prompt-mw3zb","type":"genericNode","position":{"x":-2177.128851560763,"y":150.38359629552468},"data":{"type":"Prompt","node":{"template":{"_type":"Component","code":{"type":"code","required":true,"placeholder":"","list":false,"show":true,"multiline":true,"value":"from langflow.base.prompts.api_utils import process_prompt_template\nfrom langflow.custom import Component\nfrom langflow.inputs.inputs import DefaultPromptField\nfrom langflow.io import Output, PromptInput\nfrom langflow.schema.message import Message\nfrom langflow.template.utils import update_template_values\n\n\nclass PromptComponent(Component):\n    display_name: str = \"Prompt\"\n    description: str = \"Create a prompt template with dynamic variables.\"\n    icon = \"prompts\"\n    trace_type = \"prompt\"\n    name = \"Prompt\"\n\n    inputs = [\n        PromptInput(name=\"template\", display_name=\"Template\"),\n    ]\n\n    outputs = [\n        Output(display_name=\"Prompt Message\", name=\"prompt\", method=\"build_prompt\"),\n    ]\n\n    async def build_prompt(\n        self,\n    ) -> Message:\n        prompt = await Message.from_template_and_variables(**self._attributes)\n        self.status = prompt.text\n        return prompt\n\n    def _update_template(self, frontend_node: dict):\n        prompt_template = frontend_node[\"template\"][\"template\"][\"value\"]\n        custom_fields = frontend_node[\"custom_fields\"]\n        frontend_node_template = frontend_node[\"template\"]\n        _ = process_prompt_template(\n            template=prompt_template,\n            name=\"template\",\n            custom_fields=custom_fields,\n            frontend_node_template=frontend_node_template,\n        )\n        return frontend_node\n\n    def post_code_processing(self, new_frontend_node: dict, current_frontend_node: dict):\n        \"\"\"\n        This function is called after the code validation is done.\n        \"\"\"\n        frontend_node = super().post_code_processing(new_frontend_node, current_frontend_node)\n        template = frontend_node[\"template\"][\"template\"][\"value\"]\n        # Kept it duplicated for backwards compatibility\n        _ = process_prompt_template(\n            template=template,\n            name=\"template\",\n            custom_fields=frontend_node[\"custom_fields\"],\n            frontend_node_template=frontend_node[\"template\"],\n        )\n        # Now that template is updated, we need to grab any values that were set in the current_frontend_node\n        # and update the frontend_node with those values\n        update_template_values(new_template=frontend_node, previous_template=current_frontend_node[\"template\"])\n        return frontend_node\n\n    def _get_fallback_input(self, **kwargs):\n        return DefaultPromptField(**kwargs)\n","fileTypes":[],"file_path":"","password":false,"name":"code","advanced":true,"dynamic":true,"info":"","load_from_db":false,"title_case":false,"display_name":"code"},"template":{"trace_as_input":true,"list":false,"required":false,"placeholder":"","show":true,"name":"template","value":"{user_prompt}","display_name":"Template","advanced":false,"dynamic":false,"info":"","title_case":false,"type":"prompt","_input_type":"PromptInput"},"user_prompt":{"field_type":"str","required":false,"placeholder":"","list":false,"show":true,"multiline":true,"value":"","fileTypes":[],"file_path":"","password":false,"name":"user_prompt","display_name":"user_prompt","advanced":false,"input_types":["Message","Text"],"dynamic":false,"info":"","load_from_db":false,"title_case":false,"type":"str"}},"description":"Create a prompt template with dynamic variables.","icon":"prompts","is_input":null,"is_output":null,"is_composition":null,"base_classes":["Message"],"name":"","display_name":"Prompt","documentation":"","custom_fields":{"template":["user_prompt"]},"output_types":[],"full_path":null,"pinned":false,"conditional_paths":[],"frozen":false,"outputs":[{"types":["Message"],"selected":"Message","name":"prompt","hidden":null,"display_name":"Prompt Message","method":"build_prompt","value":"__UNDEFINED__","cache":true}],"field_order":["template"],"beta":false,"error":null,"edited":false,"lf_version":"1.0.17"},"id":"Prompt-mw3zb"},"selected":true,"width":384,"height":412,"positionAbsolute":{"x":-2177.128851560763,"y":150.38359629552468},"dragging":false},{"id":"Prompt-HrBgE","type":"genericNode","position":{"x":-2178.68201579404,"y":-325.42016585687594},"data":{"type":"Prompt","node":{"template":{"_type":"Component","code":{"type":"code","required":true,"placeholder":"","list":false,"show":true,"multiline":true,"value":"from langflow.base.prompts.api_utils import process_prompt_template\nfrom langflow.custom import Component\nfrom langflow.inputs.inputs import DefaultPromptField\nfrom langflow.io import Output, PromptInput\nfrom langflow.schema.message import Message\nfrom langflow.template.utils import update_template_values\n\n\nclass PromptComponent(Component):\n    display_name: str = \"Prompt\"\n    description: str = \"Create a prompt template with dynamic variables.\"\n    icon = \"prompts\"\n    trace_type = \"prompt\"\n    name = \"Prompt\"\n\n    inputs = [\n        PromptInput(name=\"template\", display_name=\"Template\"),\n    ]\n\n    outputs = [\n        Output(display_name=\"Prompt Message\", name=\"prompt\", method=\"build_prompt\"),\n    ]\n\n    async def build_prompt(\n        self,\n    ) -> Message:\n        prompt = await Message.from_template_and_variables(**self._attributes)\n        self.status = prompt.text\n        return prompt\n\n    def _update_template(self, frontend_node: dict):\n        prompt_template = frontend_node[\"template\"][\"template\"][\"value\"]\n        custom_fields = frontend_node[\"custom_fields\"]\n        frontend_node_template = frontend_node[\"template\"]\n        _ = process_prompt_template(\n            template=prompt_template,\n            name=\"template\",\n            custom_fields=custom_fields,\n            frontend_node_template=frontend_node_template,\n        )\n        return frontend_node\n\n    def post_code_processing(self, new_frontend_node: dict, current_frontend_node: dict):\n        \"\"\"\n        This function is called after the code validation is done.\n        \"\"\"\n        frontend_node = super().post_code_processing(new_frontend_node, current_frontend_node)\n        template = frontend_node[\"template\"][\"template\"][\"value\"]\n        # Kept it duplicated for backwards compatibility\n        _ = process_prompt_template(\n            template=template,\n            name=\"template\",\n            custom_fields=frontend_node[\"custom_fields\"],\n            frontend_node_template=frontend_node[\"template\"],\n        )\n        # Now that template is updated, we need to grab any values that were set in the current_frontend_node\n        # and update the frontend_node with those values\n        update_template_values(new_template=frontend_node, previous_template=current_frontend_node[\"template\"])\n        return frontend_node\n\n    def _get_fallback_input(self, **kwargs):\n        return DefaultPromptField(**kwargs)\n","fileTypes":[],"file_path":"","password":false,"name":"code","advanced":true,"dynamic":true,"info":"","load_from_db":false,"title_case":false,"display_name":"code"},"template":{"trace_as_input":true,"list":false,"required":false,"placeholder":"","show":true,"name":"template","value":"{context}","display_name":"Template","advanced":false,"dynamic":false,"info":"","title_case":false,"type":"prompt","_input_type":"PromptInput"},"context":{"field_type":"str","required":false,"placeholder":"","list":false,"show":true,"multiline":true,"value":"","fileTypes":[],"file_path":"","password":false,"name":"context","display_name":"context","advanced":false,"input_types":["Message","Text"],"dynamic":false,"info":"","load_from_db":false,"title_case":false,"type":"str"}},"description":"Create a prompt template with dynamic variables.","icon":"prompts","is_input":null,"is_output":null,"is_composition":null,"base_classes":["Message"],"name":"","display_name":"Prompt","documentation":"","custom_fields":{"template":["context"]},"output_types":[],"full_path":null,"pinned":false,"conditional_paths":[],"frozen":false,"outputs":[{"types":["Message"],"selected":"Message","name":"prompt","hidden":null,"display_name":"Prompt Message","method":"build_prompt","value":"__UNDEFINED__","cache":true}],"field_order":["template"],"beta":false,"error":null,"edited":false,"lf_version":"1.0.17"},"id":"Prompt-HrBgE"},"selected":true,"width":384,"height":412,"positionAbsolute":{"x":-2178.68201579404,"y":-325.42016585687594},"dragging":false},{"id":"CreateHeaders-Jw5wk","type":"genericNode","position":{"x":-657.0341125223471,"y":-128.46552887731997},"data":{"type":"CreateHeaders","node":{"template":{"_type":"Component","code":{"type":"code","required":true,"placeholder":"","list":false,"show":true,"multiline":true,"value":"from langflow.custom import Component\nfrom langflow.inputs import MessageTextInput\nfrom langflow.template import Output\nfrom pydantic.v1 import SecretStr\nfrom langflow.schema import Data\n\n\n\nclass CreateHeadersComponent(Component):\n    display_name = \"Create Headers\"\n    description = \"Creates a headers dictionary with a dynamic Authorization header.\"\n    icon = \"custom_components\"\n    name = \"CreateHeaders\"\n\n    inputs = [\n        MessageTextInput(\n            name=\"input_value\",\n            display_name=\"Authorization Token\",\n            info=\"Enter the token to be used in the Authorization header.\",\n        ),\n    ]\n\n    outputs = [\n        Output(display_name=\"Output\", name=\"output\", method=\"build_output\"),\n    ]\n\n    def build_output(self) -> Data:\n        authorization_header = f\"Bearer {self.input_value}\"\n        headers = {\n            \"Authorization\": authorization_header,\n            \"Content-Type\": \"application/json\",\n        }\n\n        data = Data(value=headers)\n        self.status = data\n        return headers\n        \n        ","fileTypes":[],"file_path":"","password":false,"name":"code","advanced":true,"dynamic":true,"info":"","load_from_db":false,"title_case":false,"display_name":"code"},"input_value":{"trace_as_input":true,"trace_as_metadata":true,"load_from_db":false,"list":false,"required":false,"placeholder":"","show":true,"name":"input_value","value":"","display_name":"Authorization Token","advanced":false,"input_types":["Message"],"dynamic":false,"info":"Enter the token to be used in the Authorization header.","title_case":false,"type":"str","_input_type":"MessageTextInput"}},"description":"Creates a headers dictionary with a dynamic Authorization header.","icon":"custom_components","base_classes":["Data"],"display_name":"Create List","documentation":"","custom_fields":{},"output_types":[],"pinned":false,"conditional_paths":[],"frozen":false,"outputs":[{"types":["Data"],"selected":"Data","name":"output","display_name":"Output","method":"build_output","value":"__UNDEFINED__","cache":true}],"field_order":["input_value"],"beta":false,"edited":true,"lf_version":"1.0.17"},"id":"CreateHeaders-Jw5wk"},"selected":true,"width":384,"height":326,"positionAbsolute":{"x":-657.0341125223471,"y":-128.46552887731997},"dragging":false}],"edges":[{"source":"OpenAIModel-OHg1W","sourceHandle":"{œdataTypeœ:œOpenAIModelœ,œidœ:œOpenAIModel-OHg1Wœ,œnameœ:œtext_outputœ,œoutput_typesœ:[œMessageœ]}","target":"CustomComponent-NwxNM","targetHandle":"{œfieldNameœ:œinput_valueœ,œidœ:œCustomComponent-NwxNMœ,œinputTypesœ:[œMessageœ],œtypeœ:œstrœ}","data":{"targetHandle":{"fieldName":"input_value","id":"CustomComponent-NwxNM","inputTypes":["Message"],"type":"str"},"sourceHandle":{"dataType":"OpenAIModel","id":"OpenAIModel-OHg1W","name":"text_output","output_types":["Message"]}},"id":"reactflow__edge-OpenAIModel-OHg1W{œdataTypeœ:œOpenAIModelœ,œidœ:œOpenAIModel-OHg1Wœ,œnameœ:œtext_outputœ,œoutput_typesœ:[œMessageœ]}-CustomComponent-NwxNM{œfieldNameœ:œinput_valueœ,œidœ:œCustomComponent-NwxNMœ,œinputTypesœ:[œMessageœ],œtypeœ:œstrœ}","className":"","selected":true},{"source":"APIRequest-XsMPY","sourceHandle":"{œdataTypeœ:œAPIRequestœ,œidœ:œAPIRequest-XsMPYœ,œnameœ:œdataœ,œoutput_typesœ:[œDataœ]}","target":"FetchProcessorComponent-jzDpE","targetHandle":"{œfieldNameœ:œfetch_responseœ,œidœ:œFetchProcessorComponent-jzDpEœ,œinputTypesœ:[œDataœ],œtypeœ:œotherœ}","data":{"targetHandle":{"fieldName":"fetch_response","id":"FetchProcessorComponent-jzDpE","inputTypes":["Data"],"type":"other"},"sourceHandle":{"dataType":"APIRequest","id":"APIRequest-XsMPY","name":"data","output_types":["Data"]}},"id":"reactflow__edge-APIRequest-XsMPY{œdataTypeœ:œAPIRequestœ,œidœ:œAPIRequest-XsMPYœ,œnameœ:œdataœ,œoutput_typesœ:[œDataœ]}-FetchProcessorComponent-jzDpE{œfieldNameœ:œfetch_responseœ,œidœ:œFetchProcessorComponent-jzDpEœ,œinputTypesœ:[œDataœ],œtypeœ:œotherœ}","className":"","selected":true},{"source":"FetchProcessorComponent-jzDpE","sourceHandle":"{œdataTypeœ:œFetchProcessorComponentœ,œidœ:œFetchProcessorComponent-jzDpEœ,œnameœ:œprocessed_outputœ,œoutput_typesœ:[œMessageœ]}","target":"Prompt-NCppB","targetHandle":"{œfieldNameœ:œoutputœ,œidœ:œPrompt-NCppBœ,œinputTypesœ:[œMessageœ,œTextœ],œtypeœ:œstrœ}","data":{"targetHandle":{"fieldName":"output","id":"Prompt-NCppB","inputTypes":["Message","Text"],"type":"str"},"sourceHandle":{"dataType":"FetchProcessorComponent","id":"FetchProcessorComponent-jzDpE","name":"processed_output","output_types":["Message"]}},"id":"reactflow__edge-FetchProcessorComponent-jzDpE{œdataTypeœ:œFetchProcessorComponentœ,œidœ:œFetchProcessorComponent-jzDpEœ,œnameœ:œprocessed_outputœ,œoutput_typesœ:[œMessageœ]}-Prompt-NCppB{œfieldNameœ:œoutputœ,œidœ:œPrompt-NCppBœ,œinputTypesœ:[œMessageœ,œTextœ],œtypeœ:œstrœ}","className":"","selected":true},{"source":"Prompt-NCppB","sourceHandle":"{œdataTypeœ:œPromptœ,œidœ:œPrompt-NCppBœ,œnameœ:œpromptœ,œoutput_typesœ:[œMessageœ]}","target":"OpenAIModel-BxXKu","targetHandle":"{œfieldNameœ:œinput_valueœ,œidœ:œOpenAIModel-BxXKuœ,œinputTypesœ:[œMessageœ],œtypeœ:œstrœ}","data":{"targetHandle":{"fieldName":"input_value","id":"OpenAIModel-BxXKu","inputTypes":["Message"],"type":"str"},"sourceHandle":{"dataType":"Prompt","id":"Prompt-NCppB","name":"prompt","output_types":["Message"]}},"id":"reactflow__edge-Prompt-NCppB{œdataTypeœ:œPromptœ,œidœ:œPrompt-NCppBœ,œnameœ:œpromptœ,œoutput_typesœ:[œMessageœ]}-OpenAIModel-BxXKu{œfieldNameœ:œinput_valueœ,œidœ:œOpenAIModel-BxXKuœ,œinputTypesœ:[œMessageœ],œtypeœ:œstrœ}","className":"","selected":true},{"source":"CustomComponent-NwxNM","sourceHandle":"{œdataTypeœ:œCustomComponentœ,œidœ:œCustomComponent-NwxNMœ,œnameœ:œoutputœ,œoutput_typesœ:[œDataœ]}","target":"APIRequest-XsMPY","targetHandle":"{œfieldNameœ:œbodyœ,œidœ:œAPIRequest-XsMPYœ,œinputTypesœ:[œDataœ],œtypeœ:œNestedDictœ}","data":{"targetHandle":{"fieldName":"body","id":"APIRequest-XsMPY","inputTypes":["Data"],"type":"NestedDict"},"sourceHandle":{"dataType":"CustomComponent","id":"CustomComponent-NwxNM","name":"output","output_types":["Data"]}},"id":"reactflow__edge-CustomComponent-NwxNM{œdataTypeœ:œCustomComponentœ,œidœ:œCustomComponent-NwxNMœ,œnameœ:œoutputœ,œoutput_typesœ:[œDataœ]}-APIRequest-XsMPY{œfieldNameœ:œbodyœ,œidœ:œAPIRequest-XsMPYœ,œinputTypesœ:[œDataœ],œtypeœ:œNestedDictœ}","className":"","selected":true},{"source":"OpenAIModel-OHg1W","sourceHandle":"{œdataTypeœ:œOpenAIModelœ,œidœ:œOpenAIModel-OHg1Wœ,œnameœ:œtext_outputœ,œoutput_typesœ:[œMessageœ]}","target":"Prompt-NCppB","targetHandle":"{œfieldNameœ:œqueryœ,œidœ:œPrompt-NCppBœ,œinputTypesœ:[œMessageœ,œTextœ],œtypeœ:œstrœ}","data":{"targetHandle":{"fieldName":"query","id":"Prompt-NCppB","inputTypes":["Message","Text"],"type":"str"},"sourceHandle":{"dataType":"OpenAIModel","id":"OpenAIModel-OHg1W","name":"text_output","output_types":["Message"]}},"id":"reactflow__edge-OpenAIModel-OHg1W{œdataTypeœ:œOpenAIModelœ,œidœ:œOpenAIModel-OHg1Wœ,œnameœ:œtext_outputœ,œoutput_typesœ:[œMessageœ]}-Prompt-NCppB{œfieldNameœ:œqueryœ,œidœ:œPrompt-NCppBœ,œinputTypesœ:[œMessageœ,œTextœ],œtypeœ:œstrœ}","className":"","selected":true},{"source":"Prompt-AOko9","sourceHandle":"{œdataTypeœ:œPromptœ,œidœ:œPrompt-AOko9œ,œnameœ:œpromptœ,œoutput_typesœ:[œMessageœ]}","target":"OpenAIModel-OHg1W","targetHandle":"{œfieldNameœ:œinput_valueœ,œidœ:œOpenAIModel-OHg1Wœ,œinputTypesœ:[œMessageœ],œtypeœ:œstrœ}","data":{"targetHandle":{"fieldName":"input_value","id":"OpenAIModel-OHg1W","inputTypes":["Message"],"type":"str"},"sourceHandle":{"dataType":"Prompt","id":"Prompt-AOko9","name":"prompt","output_types":["Message"]}},"id":"reactflow__edge-Prompt-AOko9{œdataTypeœ:œPromptœ,œidœ:œPrompt-AOko9œ,œnameœ:œpromptœ,œoutput_typesœ:[œMessageœ]}-OpenAIModel-OHg1W{œfieldNameœ:œinput_valueœ,œidœ:œOpenAIModel-OHg1Wœ,œinputTypesœ:[œMessageœ],œtypeœ:œstrœ}","selected":true,"className":""},{"source":"Prompt-mw3zb","sourceHandle":"{œdataTypeœ:œPromptœ,œidœ:œPrompt-mw3zbœ,œnameœ:œpromptœ,œoutput_typesœ:[œMessageœ]}","target":"Prompt-AOko9","targetHandle":"{œfieldNameœ:œuser_promptœ,œidœ:œPrompt-AOko9œ,œinputTypesœ:[œMessageœ,œTextœ],œtypeœ:œstrœ}","data":{"targetHandle":{"fieldName":"user_prompt","id":"Prompt-AOko9","inputTypes":["Message","Text"],"type":"str"},"sourceHandle":{"dataType":"Prompt","id":"Prompt-mw3zb","name":"prompt","output_types":["Message"]}},"id":"reactflow__edge-Prompt-mw3zb{œdataTypeœ:œPromptœ,œidœ:œPrompt-mw3zbœ,œnameœ:œpromptœ,œoutput_typesœ:[œMessageœ]}-Prompt-AOko9{œfieldNameœ:œuser_promptœ,œidœ:œPrompt-AOko9œ,œinputTypesœ:[œMessageœ,œTextœ],œtypeœ:œstrœ}","selected":true},{"source":"Prompt-mw3zb","sourceHandle":"{œdataTypeœ:œPromptœ,œidœ:œPrompt-mw3zbœ,œnameœ:œpromptœ,œoutput_typesœ:[œMessageœ]}","target":"Prompt-NCppB","targetHandle":"{œfieldNameœ:œinstructionsœ,œidœ:œPrompt-NCppBœ,œinputTypesœ:[œMessageœ,œTextœ],œtypeœ:œstrœ}","data":{"targetHandle":{"fieldName":"instructions","id":"Prompt-NCppB","inputTypes":["Message","Text"],"type":"str"},"sourceHandle":{"dataType":"Prompt","id":"Prompt-mw3zb","name":"prompt","output_types":["Message"]}},"id":"reactflow__edge-Prompt-mw3zb{œdataTypeœ:œPromptœ,œidœ:œPrompt-mw3zbœ,œnameœ:œpromptœ,œoutput_typesœ:[œMessageœ]}-Prompt-NCppB{œfieldNameœ:œinstructionsœ,œidœ:œPrompt-NCppBœ,œinputTypesœ:[œMessageœ,œTextœ],œtypeœ:œstrœ}","selected":true},{"source":"Prompt-HrBgE","sourceHandle":"{œdataTypeœ:œPromptœ,œidœ:œPrompt-HrBgEœ,œnameœ:œpromptœ,œoutput_typesœ:[œMessageœ]}","target":"Prompt-NCppB","targetHandle":"{œfieldNameœ:œcontextœ,œidœ:œPrompt-NCppBœ,œinputTypesœ:[œMessageœ,œTextœ],œtypeœ:œstrœ}","data":{"targetHandle":{"fieldName":"context","id":"Prompt-NCppB","inputTypes":["Message","Text"],"type":"str"},"sourceHandle":{"dataType":"Prompt","id":"Prompt-HrBgE","name":"prompt","output_types":["Message"]}},"id":"reactflow__edge-Prompt-HrBgE{œdataTypeœ:œPromptœ,œidœ:œPrompt-HrBgEœ,œnameœ:œpromptœ,œoutput_typesœ:[œMessageœ]}-Prompt-NCppB{œfieldNameœ:œcontextœ,œidœ:œPrompt-NCppBœ,œinputTypesœ:[œMessageœ,œTextœ],œtypeœ:œstrœ}","selected":true},{"source":"Prompt-HrBgE","sourceHandle":"{œdataTypeœ:œPromptœ,œidœ:œPrompt-HrBgEœ,œnameœ:œpromptœ,œoutput_typesœ:[œMessageœ]}","target":"Prompt-AOko9","targetHandle":"{œfieldNameœ:œcontextœ,œidœ:œPrompt-AOko9œ,œinputTypesœ:[œMessageœ,œTextœ],œtypeœ:œstrœ}","data":{"targetHandle":{"fieldName":"context","id":"Prompt-AOko9","inputTypes":["Message","Text"],"type":"str"},"sourceHandle":{"dataType":"Prompt","id":"Prompt-HrBgE","name":"prompt","output_types":["Message"]}},"id":"reactflow__edge-Prompt-HrBgE{œdataTypeœ:œPromptœ,œidœ:œPrompt-HrBgEœ,œnameœ:œpromptœ,œoutput_typesœ:[œMessageœ]}-Prompt-AOko9{œfieldNameœ:œcontextœ,œidœ:œPrompt-AOko9œ,œinputTypesœ:[œMessageœ,œTextœ],œtypeœ:œstrœ}","selected":true},{"source":"CreateHeaders-Jw5wk","sourceHandle":"{œdataTypeœ:œCreateHeadersœ,œidœ:œCreateHeaders-Jw5wkœ,œnameœ:œoutputœ,œoutput_typesœ:[œDataœ]}","target":"APIRequest-XsMPY","targetHandle":"{œfieldNameœ:œheadersœ,œidœ:œAPIRequest-XsMPYœ,œinputTypesœ:[œDataœ],œtypeœ:œNestedDictœ}","data":{"targetHandle":{"fieldName":"headers","id":"APIRequest-XsMPY","inputTypes":["Data"],"type":"NestedDict"},"sourceHandle":{"dataType":"CreateHeaders","id":"CreateHeaders-Jw5wk","name":"output","output_types":["Data"]}},"id":"reactflow__edge-CreateHeaders-Jw5wk{œdataTypeœ:œCreateHeadersœ,œidœ:œCreateHeaders-Jw5wkœ,œnameœ:œoutputœ,œoutput_typesœ:[œDataœ]}-APIRequest-XsMPY{œfieldNameœ:œheadersœ,œidœ:œAPIRequest-XsMPYœ,œinputTypesœ:[œDataœ],œtypeœ:œNestedDictœ}","selected":true}],"viewport":{"zoom":1,"x":0,"y":0}},"is_component":false,"name":"Dazzling Volhard","description":"","id":"A8EHc"},"outputs":[{"types":["LanguageModel"],"selected":"LanguageModel","name":"OpenAIModel-OHg1W_model_output","display_name":"Language Model","method":"build_model","value":"__UNDEFINED__","cache":true,"proxy":{"id":"OpenAIModel-OHg1W","name":"model_output","nodeDisplayName":"OpenAI"}},{"types":["Message"],"selected":"Message","name":"OpenAIModel-BxXKu_text_output","display_name":"Text","method":"text_response","value":"__UNDEFINED__","cache":true,"proxy":{"id":"OpenAIModel-BxXKu","name":"text_output","nodeDisplayName":"OpenAI"}},{"types":["LanguageModel"],"selected":"LanguageModel","name":"OpenAIModel-BxXKu_model_output","display_name":"Language Model","method":"build_model","value":"__UNDEFINED__","cache":true,"proxy":{"id":"OpenAIModel-BxXKu","name":"model_output","nodeDisplayName":"OpenAI"}}]}},"id":"groupComponent-mAH3x","position":{"x":-1609.7443089307212,"y":1339.5057307790316},"type":"genericNode","width":384,"height":944,"selected":false,"positionAbsolute":{"x":-1609.7443089307212,"y":1339.5057307790316},"dragging":false},{"id":"GroupNode-zl0DH","type":"genericNode","position":{"x":-1023.8468916717701,"y":1331.818463507503},"data":{"id":"GroupNode-zl0DH","type":"GroupNode","node":{"display_name":"GA4 Agent","documentation":"","description":"","template":{"query_params_APIRequest-XsMPY":{"trace_as_input":true,"trace_as_metadata":true,"list":false,"required":false,"placeholder":"","show":true,"value":"","name":"query_params","display_name":"Query Parameters","advanced":false,"input_types":["Data"],"dynamic":false,"info":"The query parameters to append to the URL.","title_case":false,"type":"other","_input_type":"DataInput","proxy":{"id":"APIRequest-PrH39","field":"query_params"}},"code_APIRequest-XsMPY":{"type":"code","required":true,"placeholder":"","list":false,"show":true,"multiline":true,"value":"import asyncio\nimport json\nfrom typing import Any, List, Optional\nfrom urllib.parse import parse_qsl, urlencode, urlparse, urlunparse\n\nimport httpx\nfrom loguru import logger\n\nfrom langflow.base.curl.parse import parse_context\nfrom langflow.custom import Component\nfrom langflow.io import DataInput, DropdownInput, IntInput, MessageTextInput, NestedDictInput, Output\nfrom langflow.schema import Data\nfrom langflow.schema.dotdict import dotdict\n\n\nclass APIRequestComponent(Component):\n    display_name = \"API Request\"\n    description = (\n        \"This component allows you to make HTTP requests to one or more URLs. \"\n        \"You can provide headers and body as either dictionaries or Data objects. \"\n        \"Additionally, you can append query parameters to the URLs.\\n\\n\"\n        \"**Note:** Check advanced options for more settings.\"\n    )\n    icon = \"Globe\"\n    name = \"APIRequest\"\n\n    inputs = [\n        MessageTextInput(\n            name=\"urls\",\n            display_name=\"URLs\",\n            is_list=True,\n            info=\"Enter one or more URLs, separated by commas.\",\n        ),\n        MessageTextInput(\n            name=\"curl\",\n            display_name=\"Curl\",\n            info=\"Paste a curl command to populate the fields. This will fill in the dictionary fields for headers and body.\",\n            advanced=False,\n            refresh_button=True,\n        ),\n        DropdownInput(\n            name=\"method\",\n            display_name=\"Method\",\n            options=[\"GET\", \"POST\", \"PATCH\", \"PUT\"],\n            value=\"GET\",\n            info=\"The HTTP method to use (GET, POST, PATCH, PUT).\",\n        ),\n        NestedDictInput(\n            name=\"headers\",\n            display_name=\"Headers\",\n            info=\"The headers to send with the request as a dictionary. This is populated when using the CURL field.\",\n            input_types=[\"Data\"],\n        ),\n        NestedDictInput(\n            name=\"body\",\n            display_name=\"Body\",\n            info=\"The body to send with the request as a dictionary (for POST, PATCH, PUT). This is populated when using the CURL field.\",\n            input_types=[\"Data\"],\n        ),\n        DataInput(\n            name=\"query_params\",\n            display_name=\"Query Parameters\",\n            info=\"The query parameters to append to the URL.\",\n        ),\n        IntInput(\n            name=\"timeout\",\n            display_name=\"Timeout\",\n            value=5,\n            info=\"The timeout to use for the request.\",\n        ),\n    ]\n\n    outputs = [\n        Output(display_name=\"Data\", name=\"data\", method=\"make_requests\"),\n    ]\n\n    def parse_curl(self, curl: str, build_config: dotdict) -> dotdict:\n        try:\n            parsed = parse_context(curl)\n            build_config[\"urls\"][\"value\"] = [parsed.url]\n            build_config[\"method\"][\"value\"] = parsed.method.upper()\n            build_config[\"headers\"][\"value\"] = dict(parsed.headers)\n\n            if parsed.data:\n                try:\n                    json_data = json.loads(parsed.data)\n                    build_config[\"body\"][\"value\"] = json_data\n                except json.JSONDecodeError as e:\n                    logger.error(f\"Error decoding JSON data: {e}\")\n            else:\n                build_config[\"body\"][\"value\"] = {}\n        except Exception as exc:\n            logger.error(f\"Error parsing curl: {exc}\")\n            raise ValueError(f\"Error parsing curl: {exc}\")\n        return build_config\n\n    def update_build_config(self, build_config: dotdict, field_value: Any, field_name: str | None = None):\n        if field_name == \"curl\" and field_value:\n            build_config = self.parse_curl(field_value, build_config)\n        return build_config\n\n    async def make_request(\n        self,\n        client: httpx.AsyncClient,\n        method: str,\n        url: str,\n        headers: Optional[dict] = None,\n        body: Optional[dict] = None,\n        timeout: int = 5,\n    ) -> Data:\n        method = method.upper()\n        if method not in [\"GET\", \"POST\", \"PATCH\", \"PUT\", \"DELETE\"]:\n            raise ValueError(f\"Unsupported method: {method}\")\n\n        if isinstance(body, str) and body:\n            try:\n                body = json.loads(body)\n            except Exception as e:\n                logger.error(f\"Error decoding JSON data: {e}\")\n                body = None\n                raise ValueError(f\"Error decoding JSON data: {e}\")\n\n        data = body if body else None\n\n        try:\n            response = await client.request(method, url, headers=headers, json=data, timeout=timeout)\n            try:\n                result = response.json()\n            except Exception:\n                result = response.text\n            return Data(\n                data={\n                    \"source\": url,\n                    \"headers\": headers,\n                    \"status_code\": response.status_code,\n                    \"result\": result,\n                },\n            )\n        except httpx.TimeoutException:\n            return Data(\n                data={\n                    \"source\": url,\n                    \"headers\": headers,\n                    \"status_code\": 408,\n                    \"error\": \"Request timed out\",\n                },\n            )\n        except Exception as exc:\n            return Data(\n                data={\n                    \"source\": url,\n                    \"headers\": headers,\n                    \"status_code\": 500,\n                    \"error\": str(exc),\n                },\n            )\n\n    def add_query_params(self, url: str, params: dict) -> str:\n        url_parts = list(urlparse(url))\n        query = dict(parse_qsl(url_parts[4]))\n        query.update(params)\n        url_parts[4] = urlencode(query)\n        return urlunparse(url_parts)\n\n    async def make_requests(self) -> List[Data]:\n        method = self.method\n        urls = [url.strip() for url in self.urls if url.strip()]\n        curl = self.curl\n        headers = self.headers or {}\n        body = self.body or {}\n        timeout = self.timeout\n        query_params = self.query_params.data if self.query_params else {}\n\n        if curl:\n            self._build_config = self.parse_curl(curl, dotdict())\n\n        if isinstance(headers, Data):\n            headers = headers.data\n\n        if isinstance(body, Data):\n            body = body.data\n\n        bodies = [body] * len(urls)\n\n        urls = [self.add_query_params(url, query_params) for url in urls]\n\n        async with httpx.AsyncClient() as client:\n            results = await asyncio.gather(\n                *[self.make_request(client, method, u, headers, rec, timeout) for u, rec in zip(urls, bodies)]\n            )\n        self.status = results\n        return results\n","fileTypes":[],"file_path":"","password":false,"name":"code","advanced":true,"dynamic":true,"info":"","load_from_db":false,"title_case":false,"display_name":"code","proxy":{"id":"APIRequest-PrH39","field":"code"}},"curl_APIRequest-XsMPY":{"trace_as_input":true,"trace_as_metadata":true,"load_from_db":false,"list":false,"required":false,"placeholder":"","show":true,"value":"","name":"curl","display_name":"Curl","advanced":false,"input_types":["Message"],"dynamic":false,"info":"Paste a curl command to populate the fields. This will fill in the dictionary fields for headers and body.","refresh_button":true,"title_case":false,"type":"str","_input_type":"MessageTextInput","proxy":{"id":"APIRequest-PrH39","field":"curl"}},"method_APIRequest-XsMPY":{"trace_as_metadata":true,"options":["GET","POST","PATCH","PUT"],"combobox":false,"required":false,"placeholder":"","show":true,"value":"POST","name":"method","display_name":"Method","advanced":true,"dynamic":false,"info":"The HTTP method to use (GET, POST, PATCH, PUT).","title_case":false,"type":"str","_input_type":"DropdownInput","proxy":{"id":"APIRequest-PrH39","field":"method"}},"timeout_APIRequest-XsMPY":{"trace_as_metadata":true,"list":false,"required":false,"placeholder":"","show":true,"value":"5","name":"timeout","display_name":"Timeout","advanced":true,"dynamic":false,"info":"The timeout to use for the request.","title_case":false,"type":"int","_input_type":"IntInput","proxy":{"id":"APIRequest-PrH39","field":"timeout"}},"urls_APIRequest-XsMPY":{"trace_as_input":true,"trace_as_metadata":true,"load_from_db":false,"list":true,"required":false,"placeholder":"","show":true,"value":["https://bigquery.googleapis.com/bigquery/v2/projects/spherical-proxy-424916-c9/queries"],"name":"urls","display_name":"URLs","advanced":false,"input_types":["Message"],"dynamic":false,"info":"Enter one or more URLs, separated by commas.","title_case":false,"type":"str","_input_type":"MessageTextInput","proxy":{"id":"APIRequest-PrH39","field":"urls"}},"api_key_OpenAIModel-OHg1W":{"load_from_db":false,"required":false,"placeholder":"","show":true,"name":"api_key","value":null,"display_name":"OpenAI API Key","advanced":false,"input_types":["Message"],"dynamic":false,"info":"The OpenAI API Key to use for the OpenAI model.","title_case":false,"password":true,"type":"str","_input_type":"SecretStrInput","proxy":{"id":"OpenAIModel-joKQi","field":"api_key"}},"code_OpenAIModel-OHg1W":{"type":"code","required":true,"placeholder":"","list":false,"show":true,"multiline":true,"value":"import operator\nfrom functools import reduce\n\nfrom langflow.field_typing.range_spec import RangeSpec\nfrom langchain_openai import ChatOpenAI\nfrom pydantic.v1 import SecretStr\n\nfrom langflow.base.models.model import LCModelComponent\nfrom langflow.base.models.openai_constants import OPENAI_MODEL_NAMES\nfrom langflow.field_typing import LanguageModel\nfrom langflow.inputs import (\n    BoolInput,\n    DictInput,\n    DropdownInput,\n    FloatInput,\n    IntInput,\n    SecretStrInput,\n    StrInput,\n)\n\n\nclass OpenAIModelComponent(LCModelComponent):\n    display_name = \"OpenAI\"\n    description = \"Generates text using OpenAI LLMs.\"\n    icon = \"OpenAI\"\n    name = \"OpenAIModel\"\n\n    inputs = LCModelComponent._base_inputs + [\n        IntInput(\n            name=\"max_tokens\",\n            display_name=\"Max Tokens\",\n            advanced=True,\n            info=\"The maximum number of tokens to generate. Set to 0 for unlimited tokens.\",\n            range_spec=RangeSpec(min=0, max=128000),\n        ),\n        DictInput(name=\"model_kwargs\", display_name=\"Model Kwargs\", advanced=True),\n        BoolInput(\n            name=\"json_mode\",\n            display_name=\"JSON Mode\",\n            advanced=True,\n            info=\"If True, it will output JSON regardless of passing a schema.\",\n        ),\n        DictInput(\n            name=\"output_schema\",\n            is_list=True,\n            display_name=\"Schema\",\n            advanced=True,\n            info=\"The schema for the Output of the model. You must pass the word JSON in the prompt. If left blank, JSON mode will be disabled.\",\n        ),\n        DropdownInput(\n            name=\"model_name\",\n            display_name=\"Model Name\",\n            advanced=False,\n            options=OPENAI_MODEL_NAMES,\n            value=OPENAI_MODEL_NAMES[0],\n        ),\n        StrInput(\n            name=\"openai_api_base\",\n            display_name=\"OpenAI API Base\",\n            advanced=True,\n            info=\"The base URL of the OpenAI API. Defaults to https://api.openai.com/v1. You can change this to use other APIs like JinaChat, LocalAI and Prem.\",\n        ),\n        SecretStrInput(\n            name=\"api_key\",\n            display_name=\"OpenAI API Key\",\n            info=\"The OpenAI API Key to use for the OpenAI model.\",\n            advanced=False,\n            value=\"OPENAI_API_KEY\",\n        ),\n        FloatInput(name=\"temperature\", display_name=\"Temperature\", value=0.1),\n        IntInput(\n            name=\"seed\",\n            display_name=\"Seed\",\n            info=\"The seed controls the reproducibility of the job.\",\n            advanced=True,\n            value=1,\n        ),\n    ]\n\n    def build_model(self) -> LanguageModel:  # type: ignore[type-var]\n        # self.output_schema is a list of dictionaries\n        # let's convert it to a dictionary\n        output_schema_dict: dict[str, str] = reduce(operator.ior, self.output_schema or {}, {})\n        openai_api_key = self.api_key\n        temperature = self.temperature\n        model_name: str = self.model_name\n        max_tokens = self.max_tokens\n        model_kwargs = self.model_kwargs or {}\n        openai_api_base = self.openai_api_base or \"https://api.openai.com/v1\"\n        json_mode = bool(output_schema_dict) or self.json_mode\n        seed = self.seed\n\n        if openai_api_key:\n            api_key = SecretStr(openai_api_key)\n        else:\n            api_key = None\n        output = ChatOpenAI(\n            max_tokens=max_tokens or None,\n            model_kwargs=model_kwargs,\n            model=model_name,\n            base_url=openai_api_base,\n            api_key=api_key,\n            temperature=temperature if temperature is not None else 0.1,\n            seed=seed,\n        )\n        if json_mode:\n            if output_schema_dict:\n                output = output.with_structured_output(schema=output_schema_dict, method=\"json_mode\")  # type: ignore\n            else:\n                output = output.bind(response_format={\"type\": \"json_object\"})  # type: ignore\n\n        return output  # type: ignore\n\n    def _get_exception_message(self, e: Exception):\n        \"\"\"\n        Get a message from an OpenAI exception.\n\n        Args:\n            exception (Exception): The exception to get the message from.\n\n        Returns:\n            str: The message from the exception.\n        \"\"\"\n\n        try:\n            from openai import BadRequestError\n        except ImportError:\n            return\n        if isinstance(e, BadRequestError):\n            message = e.body.get(\"message\")  # type: ignore\n            if message:\n                return message\n        return\n","fileTypes":[],"file_path":"","password":false,"name":"code","advanced":true,"dynamic":true,"info":"","load_from_db":false,"title_case":false,"display_name":"code","proxy":{"id":"OpenAIModel-joKQi","field":"code"}},"json_mode_OpenAIModel-OHg1W":{"trace_as_metadata":true,"list":false,"required":false,"placeholder":"","show":true,"name":"json_mode","value":false,"display_name":"JSON Mode","advanced":true,"dynamic":false,"info":"If True, it will output JSON regardless of passing a schema.","title_case":false,"type":"bool","_input_type":"BoolInput","proxy":{"id":"OpenAIModel-joKQi","field":"json_mode"}},"max_tokens_OpenAIModel-OHg1W":{"trace_as_metadata":true,"range_spec":{"step_type":"float","min":0,"max":128000,"step":0.1},"list":false,"required":false,"placeholder":"","show":true,"name":"max_tokens","value":"","display_name":"Max Tokens","advanced":true,"dynamic":false,"info":"The maximum number of tokens to generate. Set to 0 for unlimited tokens.","title_case":false,"type":"int","_input_type":"IntInput","proxy":{"id":"OpenAIModel-joKQi","field":"max_tokens"}},"model_kwargs_OpenAIModel-OHg1W":{"trace_as_input":true,"list":false,"required":false,"placeholder":"","show":true,"name":"model_kwargs","value":{},"display_name":"Model Kwargs","advanced":true,"dynamic":false,"info":"","title_case":false,"type":"dict","_input_type":"DictInput","proxy":{"id":"OpenAIModel-joKQi","field":"model_kwargs"}},"model_name_OpenAIModel-OHg1W":{"trace_as_metadata":true,"options":["gpt-4o-mini","gpt-4o","gpt-4-turbo","gpt-4-turbo-preview","gpt-4","gpt-3.5-turbo","gpt-3.5-turbo-0125"],"combobox":false,"required":false,"placeholder":"","show":true,"name":"model_name","value":"gpt-4o","display_name":"Model Name","advanced":true,"dynamic":false,"info":"","title_case":false,"type":"str","_input_type":"DropdownInput","proxy":{"id":"OpenAIModel-joKQi","field":"model_name"}},"openai_api_base_OpenAIModel-OHg1W":{"trace_as_metadata":true,"load_from_db":false,"list":false,"required":false,"placeholder":"","show":true,"name":"openai_api_base","value":"","display_name":"OpenAI API Base","advanced":true,"dynamic":false,"info":"The base URL of the OpenAI API. Defaults to https://api.openai.com/v1. You can change this to use other APIs like JinaChat, LocalAI and Prem.","title_case":false,"type":"str","_input_type":"StrInput","proxy":{"id":"OpenAIModel-joKQi","field":"openai_api_base"}},"output_schema_OpenAIModel-OHg1W":{"trace_as_input":true,"list":true,"required":false,"placeholder":"","show":true,"name":"output_schema","value":{},"display_name":"Schema","advanced":true,"dynamic":false,"info":"The schema for the Output of the model. You must pass the word JSON in the prompt. If left blank, JSON mode will be disabled.","title_case":false,"type":"dict","_input_type":"DictInput","proxy":{"id":"OpenAIModel-joKQi","field":"output_schema"}},"seed_OpenAIModel-OHg1W":{"trace_as_metadata":true,"list":false,"required":false,"placeholder":"","show":true,"name":"seed","value":1,"display_name":"Seed","advanced":true,"dynamic":false,"info":"The seed controls the reproducibility of the job.","title_case":false,"type":"int","_input_type":"IntInput","proxy":{"id":"OpenAIModel-joKQi","field":"seed"}},"stream_OpenAIModel-OHg1W":{"trace_as_metadata":true,"list":false,"required":false,"placeholder":"","show":true,"name":"stream","value":false,"display_name":"Stream","advanced":true,"dynamic":false,"info":"Stream the response from the model. Streaming works only in Chat.","title_case":false,"type":"bool","_input_type":"BoolInput","proxy":{"id":"OpenAIModel-joKQi","field":"stream"}},"system_message_OpenAIModel-OHg1W":{"trace_as_input":true,"trace_as_metadata":true,"load_from_db":false,"list":false,"required":false,"placeholder":"","show":true,"name":"system_message","value":"","display_name":"System Message","advanced":true,"input_types":["Message"],"dynamic":false,"info":"System message to pass to the model.","title_case":false,"type":"str","_input_type":"MessageTextInput","proxy":{"id":"OpenAIModel-joKQi","field":"system_message"}},"temperature_OpenAIModel-OHg1W":{"trace_as_metadata":true,"list":false,"required":false,"placeholder":"","show":true,"name":"temperature","value":0.1,"display_name":"Temperature","advanced":true,"dynamic":false,"info":"","title_case":false,"type":"float","_input_type":"FloatInput","proxy":{"id":"OpenAIModel-joKQi","field":"temperature"}},"code_Prompt-AOko9":{"type":"code","required":true,"placeholder":"","list":false,"show":true,"multiline":true,"value":"from langflow.base.prompts.api_utils import process_prompt_template\nfrom langflow.custom import Component\nfrom langflow.inputs.inputs import DefaultPromptField\nfrom langflow.io import Output, PromptInput\nfrom langflow.schema.message import Message\nfrom langflow.template.utils import update_template_values\n\n\nclass PromptComponent(Component):\n    display_name: str = \"Prompt\"\n    description: str = \"Create a prompt template with dynamic variables.\"\n    icon = \"prompts\"\n    trace_type = \"prompt\"\n    name = \"Prompt\"\n\n    inputs = [\n        PromptInput(name=\"template\", display_name=\"Template\"),\n    ]\n\n    outputs = [\n        Output(display_name=\"Prompt Message\", name=\"prompt\", method=\"build_prompt\"),\n    ]\n\n    async def build_prompt(\n        self,\n    ) -> Message:\n        prompt = await Message.from_template_and_variables(**self._attributes)\n        self.status = prompt.text\n        return prompt\n\n    def _update_template(self, frontend_node: dict):\n        prompt_template = frontend_node[\"template\"][\"template\"][\"value\"]\n        custom_fields = frontend_node[\"custom_fields\"]\n        frontend_node_template = frontend_node[\"template\"]\n        _ = process_prompt_template(\n            template=prompt_template,\n            name=\"template\",\n            custom_fields=custom_fields,\n            frontend_node_template=frontend_node_template,\n        )\n        return frontend_node\n\n    def post_code_processing(self, new_frontend_node: dict, current_frontend_node: dict):\n        \"\"\"\n        This function is called after the code validation is done.\n        \"\"\"\n        frontend_node = super().post_code_processing(new_frontend_node, current_frontend_node)\n        template = frontend_node[\"template\"][\"template\"][\"value\"]\n        # Kept it duplicated for backwards compatibility\n        _ = process_prompt_template(\n            template=template,\n            name=\"template\",\n            custom_fields=frontend_node[\"custom_fields\"],\n            frontend_node_template=frontend_node[\"template\"],\n        )\n        # Now that template is updated, we need to grab any values that were set in the current_frontend_node\n        # and update the frontend_node with those values\n        update_template_values(new_template=frontend_node, previous_template=current_frontend_node[\"template\"])\n        return frontend_node\n\n    def _get_fallback_input(self, **kwargs):\n        return DefaultPromptField(**kwargs)\n","fileTypes":[],"file_path":"","password":false,"name":"code","advanced":true,"dynamic":true,"info":"","load_from_db":false,"title_case":false,"display_name":"code","proxy":{"id":"Prompt-CiXqc","field":"code"}},"template_Prompt-AOko9":{"trace_as_input":true,"list":false,"required":false,"placeholder":"","show":true,"name":"template","value":"**Context** :\n{context}\n**User Query** :\n{user_prompt}\n\n---\n\n**Task** : Create a GA4 SQL query to the `bigquery-public-data.ga4_obfuscated_sample_ecommerce.events_20201209` table. Use the schema information provided below to ensure the query is valid. \n1. **Field Validation** :\n  - Only reference fields that exist in the schema provided below.\n \n  - Use the correct dot notation when accessing fields within complex data structures like `RECORD` (also known as `STRUCT`). For example, access `ecommerce.purchase_revenue_in_usd` directly if it’s a non-repeated field.\n \n  - Use `UNNEST` only for repeated fields (arrays). Do not use `UNNEST` for fields that are simple `STRUCT` (single-record) types. For example, `event_params` and `user_properties` require `UNNEST`, while `ecommerce` does not.\n \n2. **Query Structure** : \n  - Validate that the query structure is correct and does not produce errors related to data types, field names, or incorrect usage of `UNNEST`.\n \n  - Access fields within `STRUCT` (or `RECORD`) directly using dot notation without `UNNEST` if they are not repeated. For instance, fields within `ecommerce` can be accessed directly.\n\n  - Ensure that all referenced fields are appropriately handled, especially when dealing with nested or repeated fields.\n \n3. **Error Handling** :\n  - If a query cannot be formed due to a missing or invalid field in the schema, return an error message instead of a query.\n\n\n---\n\nSchema Information for `events_20201209` Table** :\n\n```json\ncolumn_name,data_type,is_nullable\nevent_date,STRING,YES\nevent_timestamp,INT64,YES\nevent_name,STRING,YES\nevent_params,\"ARRAY<STRUCT<key STRING, value STRUCT<string_value STRING, int_value INT64, float_value FLOAT64, double_value FLOAT64>>>\",NO\nevent_previous_timestamp,INT64,YES\nevent_value_in_usd,FLOAT64,YES\nevent_bundle_sequence_id,INT64,YES\nevent_server_timestamp_offset,INT64,YES\nuser_id,STRING,YES\nuser_pseudo_id,STRING,YES\nprivacy_info,\"STRUCT<analytics_storage INT64, ads_storage INT64, uses_transient_token STRING>\",YES\nuser_properties,\"ARRAY<STRUCT<key INT64, value STRUCT<string_value INT64, int_value INT64, float_value INT64, double_value INT64, set_timestamp_micros INT64>>>\",NO\nuser_first_touch_timestamp,INT64,YES\nuser_ltv,\"STRUCT<revenue FLOAT64, currency STRING>\",YES\ndevice,\"STRUCT<category STRING, mobile_brand_name STRING, mobile_model_name STRING, mobile_marketing_name STRING, mobile_os_hardware_model INT64, operating_system STRING, operating_system_version STRING, vendor_id INT64, advertising_id INT64, language STRING, is_limited_ad_tracking STRING, time_zone_offset_seconds INT64, web_info STRUCT<browser STRING, browser_version STRING>>\",YES\ngeo,\"STRUCT<continent STRING, sub_continent STRING, country STRING, region STRING, city STRING, metro STRING>\",YES\napp_info,\"STRUCT<id STRING, version STRING, install_store STRING, firebase_app_id STRING, install_source STRING>\",YES\ntraffic_source,\"STRUCT<medium STRING, name STRING, source STRING>\",YES\nstream_id,INT64,YES\nplatform,STRING,YES\nevent_dimensions,STRUCT<hostname STRING>,YES\necommerce,\"STRUCT<total_item_quantity INT64, purchase_revenue_in_usd FLOAT64, purchase_revenue FLOAT64, refund_value_in_usd FLOAT64, refund_value FLOAT64, shipping_value_in_usd FLOAT64, shipping_value FLOAT64, tax_value_in_usd FLOAT64, tax_value FLOAT64, unique_items INT64, transaction_id STRING>\",YES\nitems,\"ARRAY<STRUCT<item_id STRING, item_name STRING, item_brand STRING, item_variant STRING, item_category STRING, item_category2 STRING, item_category3 STRING, item_category4 STRING, item_category5 STRING, price_in_usd FLOAT64, price FLOAT64, quantity INT64, item_revenue_in_usd FLOAT64, item_revenue FLOAT64, item_refund_in_usd FLOAT64, item_refund FLOAT64, coupon STRING, affiliation STRING, location_id STRING, item_list_id STRING, item_list_name STRING, item_list_index STRING, promotion_id STRING, promotion_name STRING, creative_name STRING, creative_slot STRING>>\",NO\n```\n\n\n---\n\n**Output Requirement** : Your output must be **only**  the SQL query like the example provided:\n\n```sql\nSELECT * FROM `bigquery-public-data.ga4_obfuscated_sample_ecommerce.events_20201209` LIMIT 0\n```\n\nIf you cannot generate a valid query due to missing or invalid fields, return an error message instead. Ensure that you are running a unique query that was not ran before in the context.\n\n\n---\n\n**AI** :","display_name":"Template","advanced":true,"dynamic":false,"info":"","title_case":false,"type":"prompt","_input_type":"PromptInput","load_from_db":false,"proxy":{"id":"Prompt-CiXqc","field":"template"}},"code_CustomComponent-NwxNM":{"type":"code","required":true,"placeholder":"","list":false,"show":true,"multiline":true,"value":"# from langflow.field_typing import Data\nfrom langflow.custom import Component\nfrom langflow.io import MessageTextInput, Output\nfrom langflow.schema import Data\nimport re  # Import regex module\n\nclass CustomComponent(Component):\n    display_name = \"Format Body\"\n    description = \"Turn OpenAI SQL output into a body to send with a fetch.\"\n    documentation: str = \"http://docs.langflow.org/components/custom\"\n    icon = \"custom_components\"\n    name = \"CustomComponent\"\n\n    inputs = [\n        MessageTextInput(name=\"input_value\", display_name=\"Input Value\", value=\"```sql\\nbigquery-public-data.ga4_obfuscated_sample_ecommerce.events_*\\n```\"),\n    ]\n\n    outputs = [\n        Output(display_name=\"Output\", name=\"output\", method=\"build_output\"),\n    ]\n\n    def build_output(self) -> Data:\n        # Remove the surrounding markdown and any trailing newline\n        input_value_cleaned = re.sub(r\"^```sql\\n|```$\", \"\", self.input_value.strip()).strip()\n\n        # Format the cleaned SQL string into the desired object\n        formatted_data = {\n            \"query\": input_value_cleaned,\n            \"maxResults\": 20,\n            \"timeoutMs\": 10000,\n            \"useLegacySql\": False\n        }\n        \n        # Return the formatted data as the output\n        data = Data(value=formatted_data)\n        self.status = data\n        \n        # Print the formatted data to the chat (for debugging purposes)\n        print(f\"Output Data: {formatted_data}\")\n        \n        return formatted_data\n","fileTypes":[],"file_path":"","password":false,"name":"code","advanced":true,"dynamic":true,"info":"","load_from_db":false,"title_case":false,"display_name":"code","proxy":{"id":"CustomComponent-PAxID","field":"code"}},"code_FetchProcessorComponent-jzDpE":{"type":"code","required":true,"placeholder":"","list":false,"show":true,"multiline":true,"value":"from langflow.custom import Component\nfrom langflow.io import MessageTextInput, Output\nfrom langflow.schema import Data\nfrom langflow.schema.message import Message\nimport json\n\n\nclass FetchProcessorComponent(Component):\n    display_name = \"Fetch Processor\"\n    description = \"Process fetch output, print it, and stringify it for chat.\"\n    documentation: str = \"http://docs.langflow.org/components/custom\"\n    icon = \"custom_components\"\n    name = \"FetchProcessorComponent\"\n\n    inputs = [\n        HandleInput(\n            name=\"fetch_response\",\n            display_name=\"Fetch Response\",\n            input_types=[\"Data\"],\n        )\n    ]\n\n    outputs = [\n        Output(display_name=\"Processed Output\", name=\"processed_output\", method=\"process_output\"),\n    ]\n    \n    def process_output(self) -> Message:\n        try:\n            # Directly use the fetch_response as the input data\n            response_data = self.fetch_response\n\n            # Extract the response content\n            content = str(response_data[0].data[\"result\"])\n            \n            # Truncate the content to a specific length (e.g., 5000 characters)\n            max_length = 5000\n            truncated_content = content[:max_length]\n            \n            # Convert the truncated content to a string that can be printed\n            output_str = \"Processed Output: \" + str(truncated_content)+\"...\"\n            \n            \n            # Return the processed string as a Message\n            self.status = output_str\n            return Message(text=output_str)\n\n        except Exception as e:\n            # Handle any errors and print them\n            error_message = f\"Error processing fetch response: {str(e)}\"\n            print(error_message)\n            return Message(text=error_message)\n","fileTypes":[],"file_path":"","password":false,"name":"code","advanced":true,"dynamic":true,"info":"","load_from_db":false,"title_case":false,"display_name":"code","proxy":{"id":"FetchProcessorComponent-toRll","field":"code"}},"api_key_OpenAIModel-BxXKu":{"load_from_db":false,"required":false,"placeholder":"","show":true,"value":null,"name":"api_key","display_name":"OpenAI API Key","advanced":false,"input_types":["Message"],"dynamic":false,"info":"The OpenAI API Key to use for the OpenAI model.","title_case":false,"password":true,"type":"str","_input_type":"SecretStrInput","proxy":{"id":"OpenAIModel-fJtPg","field":"api_key"}},"code_OpenAIModel-BxXKu":{"type":"code","required":true,"placeholder":"","list":false,"show":true,"multiline":true,"value":"import operator\nfrom functools import reduce\n\nfrom langflow.field_typing.range_spec import RangeSpec\nfrom langchain_openai import ChatOpenAI\nfrom pydantic.v1 import SecretStr\n\nfrom langflow.base.models.model import LCModelComponent\nfrom langflow.base.models.openai_constants import OPENAI_MODEL_NAMES\nfrom langflow.field_typing import LanguageModel\nfrom langflow.inputs import (\n    BoolInput,\n    DictInput,\n    DropdownInput,\n    FloatInput,\n    IntInput,\n    SecretStrInput,\n    StrInput,\n)\n\n\nclass OpenAIModelComponent(LCModelComponent):\n    display_name = \"OpenAI\"\n    description = \"Generates text using OpenAI LLMs.\"\n    icon = \"OpenAI\"\n    name = \"OpenAIModel\"\n\n    inputs = LCModelComponent._base_inputs + [\n        IntInput(\n            name=\"max_tokens\",\n            display_name=\"Max Tokens\",\n            advanced=True,\n            info=\"The maximum number of tokens to generate. Set to 0 for unlimited tokens.\",\n            range_spec=RangeSpec(min=0, max=128000),\n        ),\n        DictInput(name=\"model_kwargs\", display_name=\"Model Kwargs\", advanced=True),\n        BoolInput(\n            name=\"json_mode\",\n            display_name=\"JSON Mode\",\n            advanced=True,\n            info=\"If True, it will output JSON regardless of passing a schema.\",\n        ),\n        DictInput(\n            name=\"output_schema\",\n            is_list=True,\n            display_name=\"Schema\",\n            advanced=True,\n            info=\"The schema for the Output of the model. You must pass the word JSON in the prompt. If left blank, JSON mode will be disabled.\",\n        ),\n        DropdownInput(\n            name=\"model_name\",\n            display_name=\"Model Name\",\n            advanced=False,\n            options=OPENAI_MODEL_NAMES,\n            value=OPENAI_MODEL_NAMES[0],\n        ),\n        StrInput(\n            name=\"openai_api_base\",\n            display_name=\"OpenAI API Base\",\n            advanced=True,\n            info=\"The base URL of the OpenAI API. Defaults to https://api.openai.com/v1. You can change this to use other APIs like JinaChat, LocalAI and Prem.\",\n        ),\n        SecretStrInput(\n            name=\"api_key\",\n            display_name=\"OpenAI API Key\",\n            info=\"The OpenAI API Key to use for the OpenAI model.\",\n            advanced=False,\n            value=\"OPENAI_API_KEY\",\n        ),\n        FloatInput(name=\"temperature\", display_name=\"Temperature\", value=0.1),\n        IntInput(\n            name=\"seed\",\n            display_name=\"Seed\",\n            info=\"The seed controls the reproducibility of the job.\",\n            advanced=True,\n            value=1,\n        ),\n    ]\n\n    def build_model(self) -> LanguageModel:  # type: ignore[type-var]\n        # self.output_schema is a list of dictionaries\n        # let's convert it to a dictionary\n        output_schema_dict: dict[str, str] = reduce(operator.ior, self.output_schema or {}, {})\n        openai_api_key = self.api_key\n        temperature = self.temperature\n        model_name: str = self.model_name\n        max_tokens = self.max_tokens\n        model_kwargs = self.model_kwargs or {}\n        openai_api_base = self.openai_api_base or \"https://api.openai.com/v1\"\n        json_mode = bool(output_schema_dict) or self.json_mode\n        seed = self.seed\n\n        if openai_api_key:\n            api_key = SecretStr(openai_api_key)\n        else:\n            api_key = None\n        output = ChatOpenAI(\n            max_tokens=max_tokens or None,\n            model_kwargs=model_kwargs,\n            model=model_name,\n            base_url=openai_api_base,\n            api_key=api_key,\n            temperature=temperature or 0.1,\n            seed=seed,\n        )\n        if json_mode:\n            if output_schema_dict:\n                output = output.with_structured_output(schema=output_schema_dict, method=\"json_mode\")  # type: ignore\n            else:\n                output = output.bind(response_format={\"type\": \"json_object\"})  # type: ignore\n\n        return output  # type: ignore\n\n    def _get_exception_message(self, e: Exception):\n        \"\"\"\n        Get a message from an OpenAI exception.\n\n        Args:\n            exception (Exception): The exception to get the message from.\n\n        Returns:\n            str: The message from the exception.\n        \"\"\"\n\n        try:\n            from openai import BadRequestError\n        except ImportError:\n            return\n        if isinstance(e, BadRequestError):\n            message = e.body.get(\"message\")  # type: ignore\n            if message:\n                return message\n        return\n","fileTypes":[],"file_path":"","password":false,"name":"code","advanced":true,"dynamic":true,"info":"","load_from_db":false,"title_case":false,"display_name":"code","proxy":{"id":"OpenAIModel-fJtPg","field":"code"}},"json_mode_OpenAIModel-BxXKu":{"trace_as_metadata":true,"list":false,"required":false,"placeholder":"","show":true,"value":false,"name":"json_mode","display_name":"JSON Mode","advanced":true,"dynamic":false,"info":"If True, it will output JSON regardless of passing a schema.","title_case":false,"type":"bool","_input_type":"BoolInput","proxy":{"id":"OpenAIModel-fJtPg","field":"json_mode"}},"max_tokens_OpenAIModel-BxXKu":{"trace_as_metadata":true,"range_spec":{"step_type":"float","min":0,"max":128000,"step":0.1},"list":false,"required":false,"placeholder":"","show":true,"value":"","name":"max_tokens","display_name":"Max Tokens","advanced":true,"dynamic":false,"info":"The maximum number of tokens to generate. Set to 0 for unlimited tokens.","title_case":false,"type":"int","_input_type":"IntInput","proxy":{"id":"OpenAIModel-fJtPg","field":"max_tokens"}},"model_kwargs_OpenAIModel-BxXKu":{"trace_as_input":true,"list":false,"required":false,"placeholder":"","show":true,"value":{},"name":"model_kwargs","display_name":"Model Kwargs","advanced":true,"dynamic":false,"info":"","title_case":false,"type":"dict","_input_type":"DictInput","proxy":{"id":"OpenAIModel-fJtPg","field":"model_kwargs"}},"model_name_OpenAIModel-BxXKu":{"trace_as_metadata":true,"options":["gpt-4o-mini","gpt-4o","gpt-4-turbo","gpt-4-turbo-preview","gpt-4","gpt-3.5-turbo","gpt-3.5-turbo-0125"],"combobox":false,"required":false,"placeholder":"","show":true,"value":"gpt-4o-mini","name":"model_name","display_name":"Model Name","advanced":true,"dynamic":false,"info":"","title_case":false,"type":"str","_input_type":"DropdownInput","proxy":{"id":"OpenAIModel-fJtPg","field":"model_name"}},"openai_api_base_OpenAIModel-BxXKu":{"trace_as_metadata":true,"load_from_db":false,"list":false,"required":false,"placeholder":"","show":true,"value":"","name":"openai_api_base","display_name":"OpenAI API Base","advanced":true,"dynamic":false,"info":"The base URL of the OpenAI API. Defaults to https://api.openai.com/v1. You can change this to use other APIs like JinaChat, LocalAI and Prem.","title_case":false,"type":"str","_input_type":"StrInput","proxy":{"id":"OpenAIModel-fJtPg","field":"openai_api_base"}},"output_schema_OpenAIModel-BxXKu":{"trace_as_input":true,"list":true,"required":false,"placeholder":"","show":true,"value":{},"name":"output_schema","display_name":"Schema","advanced":true,"dynamic":false,"info":"The schema for the Output of the model. You must pass the word JSON in the prompt. If left blank, JSON mode will be disabled.","title_case":false,"type":"dict","_input_type":"DictInput","proxy":{"id":"OpenAIModel-fJtPg","field":"output_schema"}},"seed_OpenAIModel-BxXKu":{"trace_as_metadata":true,"list":false,"required":false,"placeholder":"","show":true,"value":1,"name":"seed","display_name":"Seed","advanced":true,"dynamic":false,"info":"The seed controls the reproducibility of the job.","title_case":false,"type":"int","_input_type":"IntInput","proxy":{"id":"OpenAIModel-fJtPg","field":"seed"}},"stream_OpenAIModel-BxXKu":{"trace_as_metadata":true,"list":false,"required":false,"placeholder":"","show":true,"value":false,"name":"stream","display_name":"Stream","advanced":true,"dynamic":false,"info":"Stream the response from the model. Streaming works only in Chat.","title_case":false,"type":"bool","_input_type":"BoolInput","proxy":{"id":"OpenAIModel-fJtPg","field":"stream"}},"system_message_OpenAIModel-BxXKu":{"trace_as_input":true,"trace_as_metadata":true,"load_from_db":false,"list":false,"required":false,"placeholder":"","show":true,"value":"","name":"system_message","display_name":"System Message","advanced":true,"input_types":["Message"],"dynamic":false,"info":"System message to pass to the model.","title_case":false,"type":"str","_input_type":"MessageTextInput","proxy":{"id":"OpenAIModel-fJtPg","field":"system_message"}},"temperature_OpenAIModel-BxXKu":{"trace_as_metadata":true,"list":false,"required":false,"placeholder":"","show":true,"value":0.1,"name":"temperature","display_name":"Temperature","advanced":true,"dynamic":false,"info":"","title_case":false,"type":"float","_input_type":"FloatInput","proxy":{"id":"OpenAIModel-fJtPg","field":"temperature"}},"code_Prompt-NCppB":{"type":"code","required":true,"placeholder":"","list":false,"show":true,"multiline":true,"value":"from langflow.base.prompts.api_utils import process_prompt_template\nfrom langflow.custom import Component\nfrom langflow.inputs.inputs import DefaultPromptField\nfrom langflow.io import Output, PromptInput\nfrom langflow.schema.message import Message\nfrom langflow.template.utils import update_template_values\n\n\nclass PromptComponent(Component):\n    display_name: str = \"Prompt\"\n    description: str = \"Create a prompt template with dynamic variables.\"\n    icon = \"prompts\"\n    trace_type = \"prompt\"\n    name = \"Prompt\"\n\n    inputs = [\n        PromptInput(name=\"template\", display_name=\"Template\"),\n    ]\n\n    outputs = [\n        Output(display_name=\"Prompt Message\", name=\"prompt\", method=\"build_prompt\"),\n    ]\n\n    async def build_prompt(\n        self,\n    ) -> Message:\n        prompt = await Message.from_template_and_variables(**self._attributes)\n        self.status = prompt.text\n        return prompt\n\n    def _update_template(self, frontend_node: dict):\n        prompt_template = frontend_node[\"template\"][\"template\"][\"value\"]\n        custom_fields = frontend_node[\"custom_fields\"]\n        frontend_node_template = frontend_node[\"template\"]\n        _ = process_prompt_template(\n            template=prompt_template,\n            name=\"template\",\n            custom_fields=custom_fields,\n            frontend_node_template=frontend_node_template,\n        )\n        return frontend_node\n\n    def post_code_processing(self, new_frontend_node: dict, current_frontend_node: dict):\n        \"\"\"\n        This function is called after the code validation is done.\n        \"\"\"\n        frontend_node = super().post_code_processing(new_frontend_node, current_frontend_node)\n        template = frontend_node[\"template\"][\"template\"][\"value\"]\n        # Kept it duplicated for backwards compatibility\n        _ = process_prompt_template(\n            template=template,\n            name=\"template\",\n            custom_fields=frontend_node[\"custom_fields\"],\n            frontend_node_template=frontend_node[\"template\"],\n        )\n        # Now that template is updated, we need to grab any values that were set in the current_frontend_node\n        # and update the frontend_node with those values\n        update_template_values(new_template=frontend_node, previous_template=current_frontend_node[\"template\"])\n        return frontend_node\n\n    def _get_fallback_input(self, **kwargs):\n        return DefaultPromptField(**kwargs)\n","fileTypes":[],"file_path":"","password":false,"name":"code","advanced":true,"dynamic":true,"info":"","load_from_db":false,"title_case":false,"display_name":"code","proxy":{"id":"Prompt-r2X9G","field":"code"}},"template_Prompt-NCppB":{"trace_as_input":true,"list":false,"required":false,"placeholder":"","show":true,"name":"template","value":"Context: {context}\n\nUser: I was given these instructions:\n\n```\n{instructions}\n```\n\n\nI gave this query to run:\n\n```\n{query}\n```\n\nI got this output:\n\n```\n{output}\n```\n\n\nDo the following:\n1. Output the query executed\n2. Output a portion of the results (human readable)\n3. Interpret the results in the context of the instructions\n4. Output the insights that you have learned\n5. Create an action for what you think you should do next using reflection.\n\nIf there was an error, output what the error was\n\nAI:","display_name":"Template","advanced":true,"dynamic":false,"info":"","title_case":false,"type":"prompt","_input_type":"PromptInput","load_from_db":false,"proxy":{"id":"Prompt-r2X9G","field":"template"}},"code_Prompt-mw3zb":{"type":"code","required":true,"placeholder":"","list":false,"show":true,"multiline":true,"value":"from langflow.base.prompts.api_utils import process_prompt_template\nfrom langflow.custom import Component\nfrom langflow.inputs.inputs import DefaultPromptField\nfrom langflow.io import Output, PromptInput\nfrom langflow.schema.message import Message\nfrom langflow.template.utils import update_template_values\n\n\nclass PromptComponent(Component):\n    display_name: str = \"Prompt\"\n    description: str = \"Create a prompt template with dynamic variables.\"\n    icon = \"prompts\"\n    trace_type = \"prompt\"\n    name = \"Prompt\"\n\n    inputs = [\n        PromptInput(name=\"template\", display_name=\"Template\"),\n    ]\n\n    outputs = [\n        Output(display_name=\"Prompt Message\", name=\"prompt\", method=\"build_prompt\"),\n    ]\n\n    async def build_prompt(\n        self,\n    ) -> Message:\n        prompt = await Message.from_template_and_variables(**self._attributes)\n        self.status = prompt.text\n        return prompt\n\n    def _update_template(self, frontend_node: dict):\n        prompt_template = frontend_node[\"template\"][\"template\"][\"value\"]\n        custom_fields = frontend_node[\"custom_fields\"]\n        frontend_node_template = frontend_node[\"template\"]\n        _ = process_prompt_template(\n            template=prompt_template,\n            name=\"template\",\n            custom_fields=custom_fields,\n            frontend_node_template=frontend_node_template,\n        )\n        return frontend_node\n\n    def post_code_processing(self, new_frontend_node: dict, current_frontend_node: dict):\n        \"\"\"\n        This function is called after the code validation is done.\n        \"\"\"\n        frontend_node = super().post_code_processing(new_frontend_node, current_frontend_node)\n        template = frontend_node[\"template\"][\"template\"][\"value\"]\n        # Kept it duplicated for backwards compatibility\n        _ = process_prompt_template(\n            template=template,\n            name=\"template\",\n            custom_fields=frontend_node[\"custom_fields\"],\n            frontend_node_template=frontend_node[\"template\"],\n        )\n        # Now that template is updated, we need to grab any values that were set in the current_frontend_node\n        # and update the frontend_node with those values\n        update_template_values(new_template=frontend_node, previous_template=current_frontend_node[\"template\"])\n        return frontend_node\n\n    def _get_fallback_input(self, **kwargs):\n        return DefaultPromptField(**kwargs)\n","fileTypes":[],"file_path":"","password":false,"name":"code","advanced":true,"dynamic":true,"info":"","load_from_db":false,"title_case":false,"display_name":"code","proxy":{"id":"Prompt-1lyud","field":"code"}},"template_Prompt-mw3zb":{"trace_as_input":true,"list":false,"required":false,"placeholder":"","show":true,"name":"template","value":"{user_prompt}","display_name":"Template","advanced":true,"dynamic":false,"info":"","title_case":false,"type":"prompt","_input_type":"PromptInput","proxy":{"id":"Prompt-1lyud","field":"template"}},"user_prompt_Prompt-mw3zb":{"field_type":"str","required":false,"placeholder":"","list":false,"show":true,"multiline":true,"value":"","fileTypes":[],"file_path":"","password":false,"name":"user_prompt","display_name":"user_prompt","advanced":false,"input_types":["Message","Text"],"dynamic":false,"info":"","load_from_db":false,"title_case":false,"type":"str","proxy":{"id":"Prompt-1lyud","field":"user_prompt"}},"code_Prompt-HrBgE":{"type":"code","required":true,"placeholder":"","list":false,"show":true,"multiline":true,"value":"from langflow.base.prompts.api_utils import process_prompt_template\nfrom langflow.custom import Component\nfrom langflow.inputs.inputs import DefaultPromptField\nfrom langflow.io import Output, PromptInput\nfrom langflow.schema.message import Message\nfrom langflow.template.utils import update_template_values\n\n\nclass PromptComponent(Component):\n    display_name: str = \"Prompt\"\n    description: str = \"Create a prompt template with dynamic variables.\"\n    icon = \"prompts\"\n    trace_type = \"prompt\"\n    name = \"Prompt\"\n\n    inputs = [\n        PromptInput(name=\"template\", display_name=\"Template\"),\n    ]\n\n    outputs = [\n        Output(display_name=\"Prompt Message\", name=\"prompt\", method=\"build_prompt\"),\n    ]\n\n    async def build_prompt(\n        self,\n    ) -> Message:\n        prompt = await Message.from_template_and_variables(**self._attributes)\n        self.status = prompt.text\n        return prompt\n\n    def _update_template(self, frontend_node: dict):\n        prompt_template = frontend_node[\"template\"][\"template\"][\"value\"]\n        custom_fields = frontend_node[\"custom_fields\"]\n        frontend_node_template = frontend_node[\"template\"]\n        _ = process_prompt_template(\n            template=prompt_template,\n            name=\"template\",\n            custom_fields=custom_fields,\n            frontend_node_template=frontend_node_template,\n        )\n        return frontend_node\n\n    def post_code_processing(self, new_frontend_node: dict, current_frontend_node: dict):\n        \"\"\"\n        This function is called after the code validation is done.\n        \"\"\"\n        frontend_node = super().post_code_processing(new_frontend_node, current_frontend_node)\n        template = frontend_node[\"template\"][\"template\"][\"value\"]\n        # Kept it duplicated for backwards compatibility\n        _ = process_prompt_template(\n            template=template,\n            name=\"template\",\n            custom_fields=frontend_node[\"custom_fields\"],\n            frontend_node_template=frontend_node[\"template\"],\n        )\n        # Now that template is updated, we need to grab any values that were set in the current_frontend_node\n        # and update the frontend_node with those values\n        update_template_values(new_template=frontend_node, previous_template=current_frontend_node[\"template\"])\n        return frontend_node\n\n    def _get_fallback_input(self, **kwargs):\n        return DefaultPromptField(**kwargs)\n","fileTypes":[],"file_path":"","password":false,"name":"code","advanced":true,"dynamic":true,"info":"","load_from_db":false,"title_case":false,"display_name":"code","proxy":{"id":"Prompt-CcCEo","field":"code"}},"template_Prompt-HrBgE":{"trace_as_input":true,"list":false,"required":false,"placeholder":"","show":true,"name":"template","value":"{context}","display_name":"Template","advanced":true,"dynamic":false,"info":"","title_case":false,"type":"prompt","_input_type":"PromptInput","proxy":{"id":"Prompt-CcCEo","field":"template"}},"context_Prompt-HrBgE":{"field_type":"str","required":false,"placeholder":"","list":false,"show":true,"multiline":true,"value":"","fileTypes":[],"file_path":"","password":false,"name":"context","display_name":"context","advanced":false,"input_types":["Message","Text"],"dynamic":false,"info":"","load_from_db":false,"title_case":false,"type":"str","proxy":{"id":"Prompt-CcCEo","field":"context"}},"code_CreateHeaders-Jw5wk":{"type":"code","required":true,"placeholder":"","list":false,"show":true,"multiline":true,"value":"from langflow.custom import Component\nfrom langflow.inputs import MessageTextInput\nfrom langflow.template import Output\nfrom pydantic.v1 import SecretStr\nfrom langflow.schema import Data\n\n\n\nclass CreateHeadersComponent(Component):\n    display_name = \"Create Headers\"\n    description = \"Creates a headers dictionary with a dynamic Authorization header.\"\n    icon = \"custom_components\"\n    name = \"CreateHeaders\"\n\n    inputs = [\n        MessageTextInput(\n            name=\"input_value\",\n            display_name=\"Authorization Token\",\n            info=\"Enter the token to be used in the Authorization header.\",\n        ),\n    ]\n\n    outputs = [\n        Output(display_name=\"Output\", name=\"output\", method=\"build_output\"),\n    ]\n\n    def build_output(self) -> Data:\n        authorization_header = f\"Bearer {self.input_value}\"\n        headers = {\n            \"Authorization\": authorization_header,\n            \"Content-Type\": \"application/json\",\n        }\n\n        data = Data(value=headers)\n        self.status = data\n        return headers\n        \n        ","fileTypes":[],"file_path":"","password":false,"name":"code","advanced":true,"dynamic":true,"info":"","load_from_db":false,"title_case":false,"display_name":"code","proxy":{"id":"CreateHeaders-zkVPz","field":"code"}},"input_value_CreateHeaders-Jw5wk":{"trace_as_input":true,"trace_as_metadata":true,"load_from_db":false,"list":false,"required":false,"placeholder":"","show":true,"name":"input_value","value":"","display_name":"Authorization Token","advanced":false,"input_types":["Message"],"dynamic":false,"info":"Enter the token to be used in the Authorization header.","title_case":false,"type":"str","_input_type":"MessageTextInput","proxy":{"id":"CreateHeaders-zkVPz","field":"input_value"}}},"flow":{"data":{"nodes":[{"id":"APIRequest-PrH39","type":"genericNode","position":{"x":-160.1020978191558,"y":-152.81819864712116},"data":{"type":"APIRequest","node":{"template":{"_type":"Component","query_params":{"trace_as_input":true,"trace_as_metadata":true,"list":false,"required":false,"placeholder":"","show":true,"value":"","name":"query_params","display_name":"Query Parameters","advanced":false,"input_types":["Data"],"dynamic":false,"info":"The query parameters to append to the URL.","title_case":false,"type":"other","_input_type":"DataInput"},"body":{"trace_as_input":true,"trace_as_metadata":true,"list":false,"required":false,"placeholder":"","show":true,"value":{},"name":"body","display_name":"Body","advanced":false,"input_types":["Data"],"dynamic":false,"info":"The body to send with the request as a dictionary (for POST, PATCH, PUT). This is populated when using the CURL field.","title_case":false,"type":"NestedDict","_input_type":"NestedDictInput"},"code":{"type":"code","required":true,"placeholder":"","list":false,"show":true,"multiline":true,"value":"import asyncio\nimport json\nfrom typing import Any, List, Optional\nfrom urllib.parse import parse_qsl, urlencode, urlparse, urlunparse\n\nimport httpx\nfrom loguru import logger\n\nfrom langflow.base.curl.parse import parse_context\nfrom langflow.custom import Component\nfrom langflow.io import DataInput, DropdownInput, IntInput, MessageTextInput, NestedDictInput, Output\nfrom langflow.schema import Data\nfrom langflow.schema.dotdict import dotdict\n\n\nclass APIRequestComponent(Component):\n    display_name = \"API Request\"\n    description = (\n        \"This component allows you to make HTTP requests to one or more URLs. \"\n        \"You can provide headers and body as either dictionaries or Data objects. \"\n        \"Additionally, you can append query parameters to the URLs.\\n\\n\"\n        \"**Note:** Check advanced options for more settings.\"\n    )\n    icon = \"Globe\"\n    name = \"APIRequest\"\n\n    inputs = [\n        MessageTextInput(\n            name=\"urls\",\n            display_name=\"URLs\",\n            is_list=True,\n            info=\"Enter one or more URLs, separated by commas.\",\n        ),\n        MessageTextInput(\n            name=\"curl\",\n            display_name=\"Curl\",\n            info=\"Paste a curl command to populate the fields. This will fill in the dictionary fields for headers and body.\",\n            advanced=False,\n            refresh_button=True,\n        ),\n        DropdownInput(\n            name=\"method\",\n            display_name=\"Method\",\n            options=[\"GET\", \"POST\", \"PATCH\", \"PUT\"],\n            value=\"GET\",\n            info=\"The HTTP method to use (GET, POST, PATCH, PUT).\",\n        ),\n        NestedDictInput(\n            name=\"headers\",\n            display_name=\"Headers\",\n            info=\"The headers to send with the request as a dictionary. This is populated when using the CURL field.\",\n            input_types=[\"Data\"],\n        ),\n        NestedDictInput(\n            name=\"body\",\n            display_name=\"Body\",\n            info=\"The body to send with the request as a dictionary (for POST, PATCH, PUT). This is populated when using the CURL field.\",\n            input_types=[\"Data\"],\n        ),\n        DataInput(\n            name=\"query_params\",\n            display_name=\"Query Parameters\",\n            info=\"The query parameters to append to the URL.\",\n        ),\n        IntInput(\n            name=\"timeout\",\n            display_name=\"Timeout\",\n            value=5,\n            info=\"The timeout to use for the request.\",\n        ),\n    ]\n\n    outputs = [\n        Output(display_name=\"Data\", name=\"data\", method=\"make_requests\"),\n    ]\n\n    def parse_curl(self, curl: str, build_config: dotdict) -> dotdict:\n        try:\n            parsed = parse_context(curl)\n            build_config[\"urls\"][\"value\"] = [parsed.url]\n            build_config[\"method\"][\"value\"] = parsed.method.upper()\n            build_config[\"headers\"][\"value\"] = dict(parsed.headers)\n\n            if parsed.data:\n                try:\n                    json_data = json.loads(parsed.data)\n                    build_config[\"body\"][\"value\"] = json_data\n                except json.JSONDecodeError as e:\n                    logger.error(f\"Error decoding JSON data: {e}\")\n            else:\n                build_config[\"body\"][\"value\"] = {}\n        except Exception as exc:\n            logger.error(f\"Error parsing curl: {exc}\")\n            raise ValueError(f\"Error parsing curl: {exc}\")\n        return build_config\n\n    def update_build_config(self, build_config: dotdict, field_value: Any, field_name: str | None = None):\n        if field_name == \"curl\" and field_value:\n            build_config = self.parse_curl(field_value, build_config)\n        return build_config\n\n    async def make_request(\n        self,\n        client: httpx.AsyncClient,\n        method: str,\n        url: str,\n        headers: Optional[dict] = None,\n        body: Optional[dict] = None,\n        timeout: int = 5,\n    ) -> Data:\n        method = method.upper()\n        if method not in [\"GET\", \"POST\", \"PATCH\", \"PUT\", \"DELETE\"]:\n            raise ValueError(f\"Unsupported method: {method}\")\n\n        if isinstance(body, str) and body:\n            try:\n                body = json.loads(body)\n            except Exception as e:\n                logger.error(f\"Error decoding JSON data: {e}\")\n                body = None\n                raise ValueError(f\"Error decoding JSON data: {e}\")\n\n        data = body if body else None\n\n        try:\n            response = await client.request(method, url, headers=headers, json=data, timeout=timeout)\n            try:\n                result = response.json()\n            except Exception:\n                result = response.text\n            return Data(\n                data={\n                    \"source\": url,\n                    \"headers\": headers,\n                    \"status_code\": response.status_code,\n                    \"result\": result,\n                },\n            )\n        except httpx.TimeoutException:\n            return Data(\n                data={\n                    \"source\": url,\n                    \"headers\": headers,\n                    \"status_code\": 408,\n                    \"error\": \"Request timed out\",\n                },\n            )\n        except Exception as exc:\n            return Data(\n                data={\n                    \"source\": url,\n                    \"headers\": headers,\n                    \"status_code\": 500,\n                    \"error\": str(exc),\n                },\n            )\n\n    def add_query_params(self, url: str, params: dict) -> str:\n        url_parts = list(urlparse(url))\n        query = dict(parse_qsl(url_parts[4]))\n        query.update(params)\n        url_parts[4] = urlencode(query)\n        return urlunparse(url_parts)\n\n    async def make_requests(self) -> List[Data]:\n        method = self.method\n        urls = [url.strip() for url in self.urls if url.strip()]\n        curl = self.curl\n        headers = self.headers or {}\n        body = self.body or {}\n        timeout = self.timeout\n        query_params = self.query_params.data if self.query_params else {}\n\n        if curl:\n            self._build_config = self.parse_curl(curl, dotdict())\n\n        if isinstance(headers, Data):\n            headers = headers.data\n\n        if isinstance(body, Data):\n            body = body.data\n\n        bodies = [body] * len(urls)\n\n        urls = [self.add_query_params(url, query_params) for url in urls]\n\n        async with httpx.AsyncClient() as client:\n            results = await asyncio.gather(\n                *[self.make_request(client, method, u, headers, rec, timeout) for u, rec in zip(urls, bodies)]\n            )\n        self.status = results\n        return results\n","fileTypes":[],"file_path":"","password":false,"name":"code","advanced":true,"dynamic":true,"info":"","load_from_db":false,"title_case":false,"display_name":"code"},"curl":{"trace_as_input":true,"trace_as_metadata":true,"load_from_db":false,"list":false,"required":false,"placeholder":"","show":true,"value":"","name":"curl","display_name":"Curl","advanced":false,"input_types":["Message"],"dynamic":false,"info":"Paste a curl command to populate the fields. This will fill in the dictionary fields for headers and body.","refresh_button":true,"title_case":false,"type":"str","_input_type":"MessageTextInput"},"headers":{"trace_as_input":true,"trace_as_metadata":true,"list":false,"required":false,"placeholder":"","show":true,"value":{},"name":"headers","display_name":"Headers","advanced":false,"input_types":["Data"],"dynamic":false,"info":"The headers to send with the request as a dictionary. This is populated when using the CURL field.","title_case":false,"type":"NestedDict","_input_type":"NestedDictInput"},"method":{"trace_as_metadata":true,"options":["GET","POST","PATCH","PUT"],"combobox":false,"required":false,"placeholder":"","show":true,"value":"POST","name":"method","display_name":"Method","advanced":false,"dynamic":false,"info":"The HTTP method to use (GET, POST, PATCH, PUT).","title_case":false,"type":"str","_input_type":"DropdownInput"},"timeout":{"trace_as_metadata":true,"list":false,"required":false,"placeholder":"","show":true,"value":"5","name":"timeout","display_name":"Timeout","advanced":false,"dynamic":false,"info":"The timeout to use for the request.","title_case":false,"type":"int","_input_type":"IntInput"},"urls":{"trace_as_input":true,"trace_as_metadata":true,"load_from_db":false,"list":true,"required":false,"placeholder":"","show":true,"value":["https://bigquery.googleapis.com/bigquery/v2/projects/spherical-proxy-424916-c9/queries"],"name":"urls","display_name":"URLs","advanced":false,"input_types":["Message"],"dynamic":false,"info":"Enter one or more URLs, separated by commas.","title_case":false,"type":"str","_input_type":"MessageTextInput"}},"description":"This component allows you to make HTTP requests to one or more URLs. You can provide headers and body as either dictionaries or Data objects. Additionally, you can append query parameters to the URLs.\n\n**Note:** Check advanced options for more settings.","icon":"Globe","base_classes":["Data"],"display_name":"API Request","documentation":"","custom_fields":{},"output_types":[],"pinned":false,"conditional_paths":[],"frozen":false,"outputs":[{"types":["Data"],"selected":"Data","name":"data","display_name":"Data","method":"make_requests","value":"__UNDEFINED__","cache":true,"hidden":false}],"field_order":["urls","curl","method","headers","body","query_params","timeout"],"beta":false,"edited":false,"lf_version":"1.0.17"},"id":"APIRequest-PrH39"},"selected":true,"width":384,"height":984,"positionAbsolute":{"x":-160.1020978191558,"y":-152.81819864712116},"dragging":false},{"id":"OpenAIModel-joKQi","type":"genericNode","position":{"x":-1191.748050434836,"y":79.93314207599741},"data":{"type":"OpenAIModel","node":{"template":{"_type":"Component","api_key":{"load_from_db":false,"required":false,"placeholder":"","show":true,"name":"api_key","value":"","display_name":"OpenAI API Key","advanced":false,"input_types":["Message"],"dynamic":false,"info":"The OpenAI API Key to use for the OpenAI model.","title_case":false,"password":true,"type":"str","_input_type":"SecretStrInput"},"code":{"type":"code","required":true,"placeholder":"","list":false,"show":true,"multiline":true,"value":"import operator\nfrom functools import reduce\n\nfrom langflow.field_typing.range_spec import RangeSpec\nfrom langchain_openai import ChatOpenAI\nfrom pydantic.v1 import SecretStr\n\nfrom langflow.base.models.model import LCModelComponent\nfrom langflow.base.models.openai_constants import OPENAI_MODEL_NAMES\nfrom langflow.field_typing import LanguageModel\nfrom langflow.inputs import (\n    BoolInput,\n    DictInput,\n    DropdownInput,\n    FloatInput,\n    IntInput,\n    SecretStrInput,\n    StrInput,\n)\n\n\nclass OpenAIModelComponent(LCModelComponent):\n    display_name = \"OpenAI\"\n    description = \"Generates text using OpenAI LLMs.\"\n    icon = \"OpenAI\"\n    name = \"OpenAIModel\"\n\n    inputs = LCModelComponent._base_inputs + [\n        IntInput(\n            name=\"max_tokens\",\n            display_name=\"Max Tokens\",\n            advanced=True,\n            info=\"The maximum number of tokens to generate. Set to 0 for unlimited tokens.\",\n            range_spec=RangeSpec(min=0, max=128000),\n        ),\n        DictInput(name=\"model_kwargs\", display_name=\"Model Kwargs\", advanced=True),\n        BoolInput(\n            name=\"json_mode\",\n            display_name=\"JSON Mode\",\n            advanced=True,\n            info=\"If True, it will output JSON regardless of passing a schema.\",\n        ),\n        DictInput(\n            name=\"output_schema\",\n            is_list=True,\n            display_name=\"Schema\",\n            advanced=True,\n            info=\"The schema for the Output of the model. You must pass the word JSON in the prompt. If left blank, JSON mode will be disabled.\",\n        ),\n        DropdownInput(\n            name=\"model_name\",\n            display_name=\"Model Name\",\n            advanced=False,\n            options=OPENAI_MODEL_NAMES,\n            value=OPENAI_MODEL_NAMES[0],\n        ),\n        StrInput(\n            name=\"openai_api_base\",\n            display_name=\"OpenAI API Base\",\n            advanced=True,\n            info=\"The base URL of the OpenAI API. Defaults to https://api.openai.com/v1. You can change this to use other APIs like JinaChat, LocalAI and Prem.\",\n        ),\n        SecretStrInput(\n            name=\"api_key\",\n            display_name=\"OpenAI API Key\",\n            info=\"The OpenAI API Key to use for the OpenAI model.\",\n            advanced=False,\n            value=\"OPENAI_API_KEY\",\n        ),\n        FloatInput(name=\"temperature\", display_name=\"Temperature\", value=0.1),\n        IntInput(\n            name=\"seed\",\n            display_name=\"Seed\",\n            info=\"The seed controls the reproducibility of the job.\",\n            advanced=True,\n            value=1,\n        ),\n    ]\n\n    def build_model(self) -> LanguageModel:  # type: ignore[type-var]\n        # self.output_schema is a list of dictionaries\n        # let's convert it to a dictionary\n        output_schema_dict: dict[str, str] = reduce(operator.ior, self.output_schema or {}, {})\n        openai_api_key = self.api_key\n        temperature = self.temperature\n        model_name: str = self.model_name\n        max_tokens = self.max_tokens\n        model_kwargs = self.model_kwargs or {}\n        openai_api_base = self.openai_api_base or \"https://api.openai.com/v1\"\n        json_mode = bool(output_schema_dict) or self.json_mode\n        seed = self.seed\n\n        if openai_api_key:\n            api_key = SecretStr(openai_api_key)\n        else:\n            api_key = None\n        output = ChatOpenAI(\n            max_tokens=max_tokens or None,\n            model_kwargs=model_kwargs,\n            model=model_name,\n            base_url=openai_api_base,\n            api_key=api_key,\n            temperature=temperature if temperature is not None else 0.1,\n            seed=seed,\n        )\n        if json_mode:\n            if output_schema_dict:\n                output = output.with_structured_output(schema=output_schema_dict, method=\"json_mode\")  # type: ignore\n            else:\n                output = output.bind(response_format={\"type\": \"json_object\"})  # type: ignore\n\n        return output  # type: ignore\n\n    def _get_exception_message(self, e: Exception):\n        \"\"\"\n        Get a message from an OpenAI exception.\n\n        Args:\n            exception (Exception): The exception to get the message from.\n\n        Returns:\n            str: The message from the exception.\n        \"\"\"\n\n        try:\n            from openai import BadRequestError\n        except ImportError:\n            return\n        if isinstance(e, BadRequestError):\n            message = e.body.get(\"message\")  # type: ignore\n            if message:\n                return message\n        return\n","fileTypes":[],"file_path":"","password":false,"name":"code","advanced":true,"dynamic":true,"info":"","load_from_db":false,"title_case":false,"display_name":"code"},"input_value":{"trace_as_input":true,"trace_as_metadata":true,"load_from_db":false,"list":false,"required":false,"placeholder":"","show":true,"name":"input_value","value":"","display_name":"Input","advanced":false,"input_types":["Message"],"dynamic":false,"info":"","title_case":false,"type":"str","_input_type":"MessageInput"},"json_mode":{"trace_as_metadata":true,"list":false,"required":false,"placeholder":"","show":true,"name":"json_mode","value":false,"display_name":"JSON Mode","advanced":true,"dynamic":false,"info":"If True, it will output JSON regardless of passing a schema.","title_case":false,"type":"bool","_input_type":"BoolInput"},"max_tokens":{"trace_as_metadata":true,"range_spec":{"step_type":"float","min":0,"max":128000,"step":0.1},"list":false,"required":false,"placeholder":"","show":true,"name":"max_tokens","value":"","display_name":"Max Tokens","advanced":true,"dynamic":false,"info":"The maximum number of tokens to generate. Set to 0 for unlimited tokens.","title_case":false,"type":"int","_input_type":"IntInput"},"model_kwargs":{"trace_as_input":true,"list":false,"required":false,"placeholder":"","show":true,"name":"model_kwargs","value":{},"display_name":"Model Kwargs","advanced":true,"dynamic":false,"info":"","title_case":false,"type":"dict","_input_type":"DictInput"},"model_name":{"trace_as_metadata":true,"options":["gpt-4o-mini","gpt-4o","gpt-4-turbo","gpt-4-turbo-preview","gpt-4","gpt-3.5-turbo","gpt-3.5-turbo-0125"],"combobox":false,"required":false,"placeholder":"","show":true,"name":"model_name","value":"gpt-4o","display_name":"Model Name","advanced":false,"dynamic":false,"info":"","title_case":false,"type":"str","_input_type":"DropdownInput"},"openai_api_base":{"trace_as_metadata":true,"load_from_db":false,"list":false,"required":false,"placeholder":"","show":true,"name":"openai_api_base","value":"","display_name":"OpenAI API Base","advanced":true,"dynamic":false,"info":"The base URL of the OpenAI API. Defaults to https://api.openai.com/v1. You can change this to use other APIs like JinaChat, LocalAI and Prem.","title_case":false,"type":"str","_input_type":"StrInput"},"output_schema":{"trace_as_input":true,"list":true,"required":false,"placeholder":"","show":true,"name":"output_schema","value":{},"display_name":"Schema","advanced":true,"dynamic":false,"info":"The schema for the Output of the model. You must pass the word JSON in the prompt. If left blank, JSON mode will be disabled.","title_case":false,"type":"dict","_input_type":"DictInput"},"seed":{"trace_as_metadata":true,"list":false,"required":false,"placeholder":"","show":true,"name":"seed","value":1,"display_name":"Seed","advanced":true,"dynamic":false,"info":"The seed controls the reproducibility of the job.","title_case":false,"type":"int","_input_type":"IntInput"},"stream":{"trace_as_metadata":true,"list":false,"required":false,"placeholder":"","show":true,"name":"stream","value":false,"display_name":"Stream","advanced":true,"dynamic":false,"info":"Stream the response from the model. Streaming works only in Chat.","title_case":false,"type":"bool","_input_type":"BoolInput"},"system_message":{"trace_as_input":true,"trace_as_metadata":true,"load_from_db":false,"list":false,"required":false,"placeholder":"","show":true,"name":"system_message","value":"","display_name":"System Message","advanced":true,"input_types":["Message"],"dynamic":false,"info":"System message to pass to the model.","title_case":false,"type":"str","_input_type":"MessageTextInput"},"temperature":{"trace_as_metadata":true,"list":false,"required":false,"placeholder":"","show":true,"name":"temperature","value":0.1,"display_name":"Temperature","advanced":false,"dynamic":false,"info":"","title_case":false,"type":"float","_input_type":"FloatInput"}},"description":"Generates text using OpenAI LLMs.","icon":"OpenAI","base_classes":["LanguageModel","Message"],"display_name":"OpenAI","documentation":"","custom_fields":{},"output_types":[],"pinned":false,"conditional_paths":[],"frozen":false,"outputs":[{"types":["Message"],"selected":"Message","name":"text_output","display_name":"Text","method":"text_response","value":"__UNDEFINED__","cache":true},{"types":["LanguageModel"],"selected":"LanguageModel","name":"model_output","display_name":"Language Model","method":"build_model","value":"__UNDEFINED__","cache":true}],"field_order":["input_value","system_message","stream","max_tokens","model_kwargs","json_mode","output_schema","model_name","openai_api_base","api_key","temperature","seed"],"beta":false,"edited":false,"lf_version":"1.0.17"},"id":"OpenAIModel-joKQi","description":"Generates text using OpenAI LLMs.","display_name":"OpenAI"},"selected":true,"width":384,"height":601,"positionAbsolute":{"x":-1191.748050434836,"y":79.93314207599741},"dragging":false},{"id":"Prompt-CiXqc","type":"genericNode","position":{"x":-1652.4360176582995,"y":115.66517557500373},"data":{"type":"Prompt","node":{"template":{"_type":"Component","code":{"type":"code","required":true,"placeholder":"","list":false,"show":true,"multiline":true,"value":"from langflow.base.prompts.api_utils import process_prompt_template\nfrom langflow.custom import Component\nfrom langflow.inputs.inputs import DefaultPromptField\nfrom langflow.io import Output, PromptInput\nfrom langflow.schema.message import Message\nfrom langflow.template.utils import update_template_values\n\n\nclass PromptComponent(Component):\n    display_name: str = \"Prompt\"\n    description: str = \"Create a prompt template with dynamic variables.\"\n    icon = \"prompts\"\n    trace_type = \"prompt\"\n    name = \"Prompt\"\n\n    inputs = [\n        PromptInput(name=\"template\", display_name=\"Template\"),\n    ]\n\n    outputs = [\n        Output(display_name=\"Prompt Message\", name=\"prompt\", method=\"build_prompt\"),\n    ]\n\n    async def build_prompt(\n        self,\n    ) -> Message:\n        prompt = await Message.from_template_and_variables(**self._attributes)\n        self.status = prompt.text\n        return prompt\n\n    def _update_template(self, frontend_node: dict):\n        prompt_template = frontend_node[\"template\"][\"template\"][\"value\"]\n        custom_fields = frontend_node[\"custom_fields\"]\n        frontend_node_template = frontend_node[\"template\"]\n        _ = process_prompt_template(\n            template=prompt_template,\n            name=\"template\",\n            custom_fields=custom_fields,\n            frontend_node_template=frontend_node_template,\n        )\n        return frontend_node\n\n    def post_code_processing(self, new_frontend_node: dict, current_frontend_node: dict):\n        \"\"\"\n        This function is called after the code validation is done.\n        \"\"\"\n        frontend_node = super().post_code_processing(new_frontend_node, current_frontend_node)\n        template = frontend_node[\"template\"][\"template\"][\"value\"]\n        # Kept it duplicated for backwards compatibility\n        _ = process_prompt_template(\n            template=template,\n            name=\"template\",\n            custom_fields=frontend_node[\"custom_fields\"],\n            frontend_node_template=frontend_node[\"template\"],\n        )\n        # Now that template is updated, we need to grab any values that were set in the current_frontend_node\n        # and update the frontend_node with those values\n        update_template_values(new_template=frontend_node, previous_template=current_frontend_node[\"template\"])\n        return frontend_node\n\n    def _get_fallback_input(self, **kwargs):\n        return DefaultPromptField(**kwargs)\n","fileTypes":[],"file_path":"","password":false,"name":"code","advanced":true,"dynamic":true,"info":"","load_from_db":false,"title_case":false,"display_name":"code"},"template":{"trace_as_input":true,"list":false,"required":false,"placeholder":"","show":true,"name":"template","value":"**Context** :\n{context}\n**User Query** :\n{user_prompt}\n\n---\n\n**Task** : Create a GA4 SQL query to the `bigquery-public-data.ga4_obfuscated_sample_ecommerce.events_20201209` table. Use the schema information provided below to ensure the query is valid. \n1. **Field Validation** :\n  - Only reference fields that exist in the schema provided below.\n \n  - Use the correct dot notation when accessing fields within complex data structures like `RECORD` (also known as `STRUCT`). For example, access `ecommerce.purchase_revenue_in_usd` directly if it’s a non-repeated field.\n \n  - Use `UNNEST` only for repeated fields (arrays). Do not use `UNNEST` for fields that are simple `STRUCT` (single-record) types. For example, `event_params` and `user_properties` require `UNNEST`, while `ecommerce` does not.\n \n2. **Query Structure** : \n  - Validate that the query structure is correct and does not produce errors related to data types, field names, or incorrect usage of `UNNEST`.\n \n  - Access fields within `STRUCT` (or `RECORD`) directly using dot notation without `UNNEST` if they are not repeated. For instance, fields within `ecommerce` can be accessed directly.\n\n  - Ensure that all referenced fields are appropriately handled, especially when dealing with nested or repeated fields.\n \n3. **Error Handling** :\n  - If a query cannot be formed due to a missing or invalid field in the schema, return an error message instead of a query.\n\n\n---\n\nSchema Information for `events_20201209` Table** :\n\n```json\ncolumn_name,data_type,is_nullable\nevent_date,STRING,YES\nevent_timestamp,INT64,YES\nevent_name,STRING,YES\nevent_params,\"ARRAY<STRUCT<key STRING, value STRUCT<string_value STRING, int_value INT64, float_value FLOAT64, double_value FLOAT64>>>\",NO\nevent_previous_timestamp,INT64,YES\nevent_value_in_usd,FLOAT64,YES\nevent_bundle_sequence_id,INT64,YES\nevent_server_timestamp_offset,INT64,YES\nuser_id,STRING,YES\nuser_pseudo_id,STRING,YES\nprivacy_info,\"STRUCT<analytics_storage INT64, ads_storage INT64, uses_transient_token STRING>\",YES\nuser_properties,\"ARRAY<STRUCT<key INT64, value STRUCT<string_value INT64, int_value INT64, float_value INT64, double_value INT64, set_timestamp_micros INT64>>>\",NO\nuser_first_touch_timestamp,INT64,YES\nuser_ltv,\"STRUCT<revenue FLOAT64, currency STRING>\",YES\ndevice,\"STRUCT<category STRING, mobile_brand_name STRING, mobile_model_name STRING, mobile_marketing_name STRING, mobile_os_hardware_model INT64, operating_system STRING, operating_system_version STRING, vendor_id INT64, advertising_id INT64, language STRING, is_limited_ad_tracking STRING, time_zone_offset_seconds INT64, web_info STRUCT<browser STRING, browser_version STRING>>\",YES\ngeo,\"STRUCT<continent STRING, sub_continent STRING, country STRING, region STRING, city STRING, metro STRING>\",YES\napp_info,\"STRUCT<id STRING, version STRING, install_store STRING, firebase_app_id STRING, install_source STRING>\",YES\ntraffic_source,\"STRUCT<medium STRING, name STRING, source STRING>\",YES\nstream_id,INT64,YES\nplatform,STRING,YES\nevent_dimensions,STRUCT<hostname STRING>,YES\necommerce,\"STRUCT<total_item_quantity INT64, purchase_revenue_in_usd FLOAT64, purchase_revenue FLOAT64, refund_value_in_usd FLOAT64, refund_value FLOAT64, shipping_value_in_usd FLOAT64, shipping_value FLOAT64, tax_value_in_usd FLOAT64, tax_value FLOAT64, unique_items INT64, transaction_id STRING>\",YES\nitems,\"ARRAY<STRUCT<item_id STRING, item_name STRING, item_brand STRING, item_variant STRING, item_category STRING, item_category2 STRING, item_category3 STRING, item_category4 STRING, item_category5 STRING, price_in_usd FLOAT64, price FLOAT64, quantity INT64, item_revenue_in_usd FLOAT64, item_revenue FLOAT64, item_refund_in_usd FLOAT64, item_refund FLOAT64, coupon STRING, affiliation STRING, location_id STRING, item_list_id STRING, item_list_name STRING, item_list_index STRING, promotion_id STRING, promotion_name STRING, creative_name STRING, creative_slot STRING>>\",NO\n```\n\n\n---\n\n**Output Requirement** : Your output must be **only**  the SQL query like the example provided:\n\n```sql\nSELECT * FROM `bigquery-public-data.ga4_obfuscated_sample_ecommerce.events_20201209` LIMIT 0\n```\n\nIf you cannot generate a valid query due to missing or invalid fields, return an error message instead. Ensure that you are running a unique query that was not ran before in the context.\n\n\n---\n\n**AI** :","display_name":"Template","advanced":false,"dynamic":false,"info":"","title_case":false,"type":"prompt","_input_type":"PromptInput","load_from_db":false},"context":{"field_type":"str","required":false,"placeholder":"","list":false,"show":true,"multiline":true,"value":"","fileTypes":[],"file_path":"","password":false,"name":"context","display_name":"context","advanced":false,"input_types":["Message","Text"],"dynamic":false,"info":"","load_from_db":false,"title_case":false,"type":"str"},"user_prompt":{"field_type":"str","required":false,"placeholder":"","list":false,"show":true,"multiline":true,"value":"","fileTypes":[],"file_path":"","password":false,"name":"user_prompt","display_name":"user_prompt","advanced":false,"input_types":["Message","Text"],"dynamic":false,"info":"","load_from_db":false,"title_case":false,"type":"str"}},"description":"Create a prompt template with dynamic variables.","icon":"prompts","is_input":null,"is_output":null,"is_composition":null,"base_classes":["Message"],"name":"","display_name":"Prompt","documentation":"","custom_fields":{"template":["context","user_prompt"]},"output_types":[],"full_path":null,"pinned":false,"conditional_paths":[],"frozen":false,"outputs":[{"types":["Message"],"selected":"Message","name":"prompt","hidden":null,"display_name":"Prompt Message","method":"build_prompt","value":"__UNDEFINED__","cache":true}],"field_order":["template"],"beta":false,"error":null,"edited":false},"id":"Prompt-CiXqc","description":"Create a prompt template with dynamic variables.","display_name":"Prompt"},"selected":true,"width":384,"height":498,"positionAbsolute":{"x":-1652.4360176582995,"y":115.66517557500373},"dragging":false},{"id":"CustomComponent-PAxID","type":"genericNode","position":{"x":-722.2114864428826,"y":795.4262181316096},"data":{"type":"CustomComponent","node":{"template":{"_type":"Component","code":{"type":"code","required":true,"placeholder":"","list":false,"show":true,"multiline":true,"value":"# from langflow.field_typing import Data\nfrom langflow.custom import Component\nfrom langflow.io import MessageTextInput, Output\nfrom langflow.schema import Data\nimport re  # Import regex module\n\nclass CustomComponent(Component):\n    display_name = \"Format Body\"\n    description = \"Turn OpenAI SQL output into a body to send with a fetch.\"\n    documentation: str = \"http://docs.langflow.org/components/custom\"\n    icon = \"custom_components\"\n    name = \"CustomComponent\"\n\n    inputs = [\n        MessageTextInput(name=\"input_value\", display_name=\"Input Value\", value=\"```sql\\nbigquery-public-data.ga4_obfuscated_sample_ecommerce.events_*\\n```\"),\n    ]\n\n    outputs = [\n        Output(display_name=\"Output\", name=\"output\", method=\"build_output\"),\n    ]\n\n    def build_output(self) -> Data:\n        # Remove the surrounding markdown and any trailing newline\n        input_value_cleaned = re.sub(r\"^```sql\\n|```$\", \"\", self.input_value.strip()).strip()\n\n        # Format the cleaned SQL string into the desired object\n        formatted_data = {\n            \"query\": input_value_cleaned,\n            \"maxResults\": 20,\n            \"timeoutMs\": 10000,\n            \"useLegacySql\": False\n        }\n        \n        # Return the formatted data as the output\n        data = Data(value=formatted_data)\n        self.status = data\n        \n        # Print the formatted data to the chat (for debugging purposes)\n        print(f\"Output Data: {formatted_data}\")\n        \n        return formatted_data\n","fileTypes":[],"file_path":"","password":false,"name":"code","advanced":true,"dynamic":true,"info":"","load_from_db":false,"title_case":false,"display_name":"code"},"input_value":{"trace_as_input":true,"trace_as_metadata":true,"load_from_db":false,"list":false,"required":false,"placeholder":"","show":true,"name":"input_value","value":"","display_name":"Input Value","advanced":false,"input_types":["Message"],"dynamic":false,"info":"","title_case":false,"type":"str","_input_type":"MessageTextInput"}},"description":"Turn OpenAI SQL output into a body to send with a fetch.","icon":"custom_components","base_classes":["Data"],"display_name":"Custom Component","documentation":"http://docs.langflow.org/components/custom","custom_fields":{},"output_types":[],"pinned":false,"conditional_paths":[],"frozen":false,"outputs":[{"types":["Data"],"selected":"Data","name":"output","display_name":"Output","method":"build_output","value":"__UNDEFINED__","cache":true}],"field_order":["input_value"],"beta":false,"edited":true,"lf_version":"1.0.17"},"id":"CustomComponent-PAxID"},"selected":true,"width":384,"height":326,"dragging":false,"positionAbsolute":{"x":-722.2114864428826,"y":795.4262181316096}},{"id":"FetchProcessorComponent-toRll","type":"genericNode","position":{"x":373.18673165075575,"y":648.0200366696015},"data":{"type":"FetchProcessorComponent","node":{"template":{"_type":"Component","fetch_response":{"trace_as_metadata":true,"list":false,"required":false,"placeholder":"","show":true,"name":"fetch_response","value":"","display_name":"Fetch Response","advanced":false,"input_types":["Data"],"dynamic":false,"info":"","title_case":false,"type":"other","_input_type":"HandleInput"},"code":{"type":"code","required":true,"placeholder":"","list":false,"show":true,"multiline":true,"value":"from langflow.custom import Component\nfrom langflow.io import MessageTextInput, Output\nfrom langflow.schema import Data\nfrom langflow.schema.message import Message\nimport json\n\n\nclass FetchProcessorComponent(Component):\n    display_name = \"Fetch Processor\"\n    description = \"Process fetch output, print it, and stringify it for chat.\"\n    documentation: str = \"http://docs.langflow.org/components/custom\"\n    icon = \"custom_components\"\n    name = \"FetchProcessorComponent\"\n\n    inputs = [\n        HandleInput(\n            name=\"fetch_response\",\n            display_name=\"Fetch Response\",\n            input_types=[\"Data\"],\n        )\n    ]\n\n    outputs = [\n        Output(display_name=\"Processed Output\", name=\"processed_output\", method=\"process_output\"),\n    ]\n    \n    def process_output(self) -> Message:\n        try:\n            # Directly use the fetch_response as the input data\n            response_data = self.fetch_response\n\n            # Extract the response content\n            content = str(response_data[0].data[\"result\"])\n            \n            # Truncate the content to a specific length (e.g., 5000 characters)\n            max_length = 5000\n            truncated_content = content[:max_length]\n            \n            # Convert the truncated content to a string that can be printed\n            output_str = \"Processed Output: \" + str(truncated_content)+\"...\"\n            \n            \n            # Return the processed string as a Message\n            self.status = output_str\n            return Message(text=output_str)\n\n        except Exception as e:\n            # Handle any errors and print them\n            error_message = f\"Error processing fetch response: {str(e)}\"\n            print(error_message)\n            return Message(text=error_message)\n","fileTypes":[],"file_path":"","password":false,"name":"code","advanced":true,"dynamic":true,"info":"","load_from_db":false,"title_case":false,"display_name":"code"}},"description":"Process fetch output, print it, and stringify it for chat.","icon":"custom_components","base_classes":["Message"],"display_name":"Custom Component","documentation":"http://docs.langflow.org/components/custom","custom_fields":{},"output_types":[],"pinned":false,"conditional_paths":[],"frozen":false,"outputs":[{"types":["Message"],"selected":"Message","name":"processed_output","display_name":"Processed Output","method":"process_output","value":"__UNDEFINED__","cache":true}],"field_order":["fetch_response"],"beta":false,"edited":true,"lf_version":"1.0.17"},"id":"FetchProcessorComponent-toRll"},"selected":true,"width":384,"height":288,"dragging":false,"positionAbsolute":{"x":373.18673165075575,"y":648.0200366696015}},{"id":"OpenAIModel-fJtPg","type":"genericNode","position":{"x":1420.9686803289817,"y":501.3038614984411},"data":{"type":"OpenAIModel","node":{"template":{"_type":"Component","api_key":{"load_from_db":false,"required":false,"placeholder":"","show":true,"value":"","name":"api_key","display_name":"OpenAI API Key","advanced":false,"input_types":["Message"],"dynamic":false,"info":"The OpenAI API Key to use for the OpenAI model.","title_case":false,"password":true,"type":"str","_input_type":"SecretStrInput"},"code":{"type":"code","required":true,"placeholder":"","list":false,"show":true,"multiline":true,"value":"import operator\nfrom functools import reduce\n\nfrom langflow.field_typing.range_spec import RangeSpec\nfrom langchain_openai import ChatOpenAI\nfrom pydantic.v1 import SecretStr\n\nfrom langflow.base.models.model import LCModelComponent\nfrom langflow.base.models.openai_constants import OPENAI_MODEL_NAMES\nfrom langflow.field_typing import LanguageModel\nfrom langflow.inputs import (\n    BoolInput,\n    DictInput,\n    DropdownInput,\n    FloatInput,\n    IntInput,\n    SecretStrInput,\n    StrInput,\n)\n\n\nclass OpenAIModelComponent(LCModelComponent):\n    display_name = \"OpenAI\"\n    description = \"Generates text using OpenAI LLMs.\"\n    icon = \"OpenAI\"\n    name = \"OpenAIModel\"\n\n    inputs = LCModelComponent._base_inputs + [\n        IntInput(\n            name=\"max_tokens\",\n            display_name=\"Max Tokens\",\n            advanced=True,\n            info=\"The maximum number of tokens to generate. Set to 0 for unlimited tokens.\",\n            range_spec=RangeSpec(min=0, max=128000),\n        ),\n        DictInput(name=\"model_kwargs\", display_name=\"Model Kwargs\", advanced=True),\n        BoolInput(\n            name=\"json_mode\",\n            display_name=\"JSON Mode\",\n            advanced=True,\n            info=\"If True, it will output JSON regardless of passing a schema.\",\n        ),\n        DictInput(\n            name=\"output_schema\",\n            is_list=True,\n            display_name=\"Schema\",\n            advanced=True,\n            info=\"The schema for the Output of the model. You must pass the word JSON in the prompt. If left blank, JSON mode will be disabled.\",\n        ),\n        DropdownInput(\n            name=\"model_name\",\n            display_name=\"Model Name\",\n            advanced=False,\n            options=OPENAI_MODEL_NAMES,\n            value=OPENAI_MODEL_NAMES[0],\n        ),\n        StrInput(\n            name=\"openai_api_base\",\n            display_name=\"OpenAI API Base\",\n            advanced=True,\n            info=\"The base URL of the OpenAI API. Defaults to https://api.openai.com/v1. You can change this to use other APIs like JinaChat, LocalAI and Prem.\",\n        ),\n        SecretStrInput(\n            name=\"api_key\",\n            display_name=\"OpenAI API Key\",\n            info=\"The OpenAI API Key to use for the OpenAI model.\",\n            advanced=False,\n            value=\"OPENAI_API_KEY\",\n        ),\n        FloatInput(name=\"temperature\", display_name=\"Temperature\", value=0.1),\n        IntInput(\n            name=\"seed\",\n            display_name=\"Seed\",\n            info=\"The seed controls the reproducibility of the job.\",\n            advanced=True,\n            value=1,\n        ),\n    ]\n\n    def build_model(self) -> LanguageModel:  # type: ignore[type-var]\n        # self.output_schema is a list of dictionaries\n        # let's convert it to a dictionary\n        output_schema_dict: dict[str, str] = reduce(operator.ior, self.output_schema or {}, {})\n        openai_api_key = self.api_key\n        temperature = self.temperature\n        model_name: str = self.model_name\n        max_tokens = self.max_tokens\n        model_kwargs = self.model_kwargs or {}\n        openai_api_base = self.openai_api_base or \"https://api.openai.com/v1\"\n        json_mode = bool(output_schema_dict) or self.json_mode\n        seed = self.seed\n\n        if openai_api_key:\n            api_key = SecretStr(openai_api_key)\n        else:\n            api_key = None\n        output = ChatOpenAI(\n            max_tokens=max_tokens or None,\n            model_kwargs=model_kwargs,\n            model=model_name,\n            base_url=openai_api_base,\n            api_key=api_key,\n            temperature=temperature or 0.1,\n            seed=seed,\n        )\n        if json_mode:\n            if output_schema_dict:\n                output = output.with_structured_output(schema=output_schema_dict, method=\"json_mode\")  # type: ignore\n            else:\n                output = output.bind(response_format={\"type\": \"json_object\"})  # type: ignore\n\n        return output  # type: ignore\n\n    def _get_exception_message(self, e: Exception):\n        \"\"\"\n        Get a message from an OpenAI exception.\n\n        Args:\n            exception (Exception): The exception to get the message from.\n\n        Returns:\n            str: The message from the exception.\n        \"\"\"\n\n        try:\n            from openai import BadRequestError\n        except ImportError:\n            return\n        if isinstance(e, BadRequestError):\n            message = e.body.get(\"message\")  # type: ignore\n            if message:\n                return message\n        return\n","fileTypes":[],"file_path":"","password":false,"name":"code","advanced":true,"dynamic":true,"info":"","load_from_db":false,"title_case":false,"display_name":"code"},"input_value":{"trace_as_input":true,"trace_as_metadata":true,"load_from_db":false,"list":false,"required":false,"placeholder":"","show":true,"value":"","name":"input_value","display_name":"Input","advanced":false,"input_types":["Message"],"dynamic":false,"info":"","title_case":false,"type":"str","_input_type":"MessageInput"},"json_mode":{"trace_as_metadata":true,"list":false,"required":false,"placeholder":"","show":true,"value":false,"name":"json_mode","display_name":"JSON Mode","advanced":true,"dynamic":false,"info":"If True, it will output JSON regardless of passing a schema.","title_case":false,"type":"bool","_input_type":"BoolInput"},"max_tokens":{"trace_as_metadata":true,"range_spec":{"step_type":"float","min":0,"max":128000,"step":0.1},"list":false,"required":false,"placeholder":"","show":true,"value":"","name":"max_tokens","display_name":"Max Tokens","advanced":true,"dynamic":false,"info":"The maximum number of tokens to generate. Set to 0 for unlimited tokens.","title_case":false,"type":"int","_input_type":"IntInput"},"model_kwargs":{"trace_as_input":true,"list":false,"required":false,"placeholder":"","show":true,"value":{},"name":"model_kwargs","display_name":"Model Kwargs","advanced":true,"dynamic":false,"info":"","title_case":false,"type":"dict","_input_type":"DictInput"},"model_name":{"trace_as_metadata":true,"options":["gpt-4o-mini","gpt-4o","gpt-4-turbo","gpt-4-turbo-preview","gpt-4","gpt-3.5-turbo","gpt-3.5-turbo-0125"],"combobox":false,"required":false,"placeholder":"","show":true,"value":"gpt-4o-mini","name":"model_name","display_name":"Model Name","advanced":false,"dynamic":false,"info":"","title_case":false,"type":"str","_input_type":"DropdownInput"},"openai_api_base":{"trace_as_metadata":true,"load_from_db":false,"list":false,"required":false,"placeholder":"","show":true,"value":"","name":"openai_api_base","display_name":"OpenAI API Base","advanced":true,"dynamic":false,"info":"The base URL of the OpenAI API. Defaults to https://api.openai.com/v1. You can change this to use other APIs like JinaChat, LocalAI and Prem.","title_case":false,"type":"str","_input_type":"StrInput"},"output_schema":{"trace_as_input":true,"list":true,"required":false,"placeholder":"","show":true,"value":{},"name":"output_schema","display_name":"Schema","advanced":true,"dynamic":false,"info":"The schema for the Output of the model. You must pass the word JSON in the prompt. If left blank, JSON mode will be disabled.","title_case":false,"type":"dict","_input_type":"DictInput"},"seed":{"trace_as_metadata":true,"list":false,"required":false,"placeholder":"","show":true,"value":1,"name":"seed","display_name":"Seed","advanced":true,"dynamic":false,"info":"The seed controls the reproducibility of the job.","title_case":false,"type":"int","_input_type":"IntInput"},"stream":{"trace_as_metadata":true,"list":false,"required":false,"placeholder":"","show":true,"value":false,"name":"stream","display_name":"Stream","advanced":true,"dynamic":false,"info":"Stream the response from the model. Streaming works only in Chat.","title_case":false,"type":"bool","_input_type":"BoolInput"},"system_message":{"trace_as_input":true,"trace_as_metadata":true,"load_from_db":false,"list":false,"required":false,"placeholder":"","show":true,"value":"","name":"system_message","display_name":"System Message","advanced":true,"input_types":["Message"],"dynamic":false,"info":"System message to pass to the model.","title_case":false,"type":"str","_input_type":"MessageTextInput"},"temperature":{"trace_as_metadata":true,"list":false,"required":false,"placeholder":"","show":true,"value":0.1,"name":"temperature","display_name":"Temperature","advanced":false,"dynamic":false,"info":"","title_case":false,"type":"float","_input_type":"FloatInput"}},"description":"Generates text using OpenAI LLMs.","icon":"OpenAI","base_classes":["LanguageModel","Message"],"display_name":"OpenAI","documentation":"","custom_fields":{},"output_types":[],"pinned":false,"conditional_paths":[],"frozen":false,"outputs":[{"types":["Message"],"selected":"Message","name":"text_output","display_name":"Text","method":"text_response","value":"__UNDEFINED__","cache":true},{"types":["LanguageModel"],"selected":"LanguageModel","name":"model_output","display_name":"Language Model","method":"build_model","value":"__UNDEFINED__","cache":true}],"field_order":["input_value","system_message","stream","max_tokens","model_kwargs","json_mode","output_schema","model_name","openai_api_base","api_key","temperature","seed"],"beta":false,"edited":false,"lf_version":"1.0.17"},"id":"OpenAIModel-fJtPg"},"selected":true,"width":384,"height":601,"positionAbsolute":{"x":1420.9686803289817,"y":501.3038614984411},"dragging":false},{"id":"Prompt-r2X9G","type":"genericNode","position":{"x":914.8083947668401,"y":477.7711490769072},"data":{"type":"Prompt","node":{"template":{"_type":"Component","code":{"type":"code","required":true,"placeholder":"","list":false,"show":true,"multiline":true,"value":"from langflow.base.prompts.api_utils import process_prompt_template\nfrom langflow.custom import Component\nfrom langflow.inputs.inputs import DefaultPromptField\nfrom langflow.io import Output, PromptInput\nfrom langflow.schema.message import Message\nfrom langflow.template.utils import update_template_values\n\n\nclass PromptComponent(Component):\n    display_name: str = \"Prompt\"\n    description: str = \"Create a prompt template with dynamic variables.\"\n    icon = \"prompts\"\n    trace_type = \"prompt\"\n    name = \"Prompt\"\n\n    inputs = [\n        PromptInput(name=\"template\", display_name=\"Template\"),\n    ]\n\n    outputs = [\n        Output(display_name=\"Prompt Message\", name=\"prompt\", method=\"build_prompt\"),\n    ]\n\n    async def build_prompt(\n        self,\n    ) -> Message:\n        prompt = await Message.from_template_and_variables(**self._attributes)\n        self.status = prompt.text\n        return prompt\n\n    def _update_template(self, frontend_node: dict):\n        prompt_template = frontend_node[\"template\"][\"template\"][\"value\"]\n        custom_fields = frontend_node[\"custom_fields\"]\n        frontend_node_template = frontend_node[\"template\"]\n        _ = process_prompt_template(\n            template=prompt_template,\n            name=\"template\",\n            custom_fields=custom_fields,\n            frontend_node_template=frontend_node_template,\n        )\n        return frontend_node\n\n    def post_code_processing(self, new_frontend_node: dict, current_frontend_node: dict):\n        \"\"\"\n        This function is called after the code validation is done.\n        \"\"\"\n        frontend_node = super().post_code_processing(new_frontend_node, current_frontend_node)\n        template = frontend_node[\"template\"][\"template\"][\"value\"]\n        # Kept it duplicated for backwards compatibility\n        _ = process_prompt_template(\n            template=template,\n            name=\"template\",\n            custom_fields=frontend_node[\"custom_fields\"],\n            frontend_node_template=frontend_node[\"template\"],\n        )\n        # Now that template is updated, we need to grab any values that were set in the current_frontend_node\n        # and update the frontend_node with those values\n        update_template_values(new_template=frontend_node, previous_template=current_frontend_node[\"template\"])\n        return frontend_node\n\n    def _get_fallback_input(self, **kwargs):\n        return DefaultPromptField(**kwargs)\n","fileTypes":[],"file_path":"","password":false,"name":"code","advanced":true,"dynamic":true,"info":"","load_from_db":false,"title_case":false,"display_name":"code"},"template":{"trace_as_input":true,"list":false,"required":false,"placeholder":"","show":true,"name":"template","value":"Context: {context}\n\nUser: I was given these instructions:\n\n```\n{instructions}\n```\n\n\nI gave this query to run:\n\n```\n{query}\n```\n\nI got this output:\n\n```\n{output}\n```\n\n\nDo the following:\n1. Output the query executed\n2. Output a portion of the results (human readable)\n3. Interpret the results in the context of the instructions\n4. Output the insights that you have learned\n5. Create an action for what you think you should do next using reflection.\n\nIf there was an error, output what the error was\n\nAI:","display_name":"Template","advanced":false,"dynamic":false,"info":"","title_case":false,"type":"prompt","_input_type":"PromptInput","load_from_db":false},"context":{"field_type":"str","required":false,"placeholder":"","list":false,"show":true,"multiline":true,"value":"","fileTypes":[],"file_path":"","password":false,"name":"context","display_name":"context","advanced":false,"input_types":["Message","Text"],"dynamic":false,"info":"","load_from_db":false,"title_case":false,"type":"str"},"instructions":{"field_type":"str","required":false,"placeholder":"","list":false,"show":true,"multiline":true,"value":"","fileTypes":[],"file_path":"","password":false,"name":"instructions","display_name":"instructions","advanced":false,"input_types":["Message","Text"],"dynamic":false,"info":"","load_from_db":false,"title_case":false,"type":"str"},"query":{"field_type":"str","required":false,"placeholder":"","list":false,"show":true,"multiline":true,"value":"","fileTypes":[],"file_path":"","password":false,"name":"query","display_name":"query","advanced":false,"input_types":["Message","Text"],"dynamic":false,"info":"","load_from_db":false,"title_case":false,"type":"str"},"output":{"field_type":"str","required":false,"placeholder":"","list":false,"show":true,"multiline":true,"value":"","fileTypes":[],"file_path":"","password":false,"name":"output","display_name":"output","advanced":false,"input_types":["Message","Text"],"dynamic":false,"info":"","load_from_db":false,"title_case":false,"type":"str"}},"description":"Create a prompt template with dynamic variables.","icon":"prompts","is_input":null,"is_output":null,"is_composition":null,"base_classes":["Message"],"name":"","display_name":"Prompt","documentation":"","custom_fields":{"template":["context","instructions","query","output"]},"output_types":[],"full_path":null,"pinned":false,"conditional_paths":[],"frozen":false,"outputs":[{"types":["Message"],"selected":"Message","name":"prompt","hidden":null,"display_name":"Prompt Message","method":"build_prompt","value":"__UNDEFINED__","cache":true}],"field_order":["template"],"beta":false,"error":null,"edited":false},"id":"Prompt-r2X9G","description":"Create a prompt template with dynamic variables.","display_name":"Prompt"},"selected":true,"width":384,"height":670,"dragging":false,"positionAbsolute":{"x":914.8083947668401,"y":477.7711490769072}},{"id":"Prompt-1lyud","type":"genericNode","position":{"x":-2177.128851560763,"y":150.38359629552468},"data":{"type":"Prompt","node":{"template":{"_type":"Component","code":{"type":"code","required":true,"placeholder":"","list":false,"show":true,"multiline":true,"value":"from langflow.base.prompts.api_utils import process_prompt_template\nfrom langflow.custom import Component\nfrom langflow.inputs.inputs import DefaultPromptField\nfrom langflow.io import Output, PromptInput\nfrom langflow.schema.message import Message\nfrom langflow.template.utils import update_template_values\n\n\nclass PromptComponent(Component):\n    display_name: str = \"Prompt\"\n    description: str = \"Create a prompt template with dynamic variables.\"\n    icon = \"prompts\"\n    trace_type = \"prompt\"\n    name = \"Prompt\"\n\n    inputs = [\n        PromptInput(name=\"template\", display_name=\"Template\"),\n    ]\n\n    outputs = [\n        Output(display_name=\"Prompt Message\", name=\"prompt\", method=\"build_prompt\"),\n    ]\n\n    async def build_prompt(\n        self,\n    ) -> Message:\n        prompt = await Message.from_template_and_variables(**self._attributes)\n        self.status = prompt.text\n        return prompt\n\n    def _update_template(self, frontend_node: dict):\n        prompt_template = frontend_node[\"template\"][\"template\"][\"value\"]\n        custom_fields = frontend_node[\"custom_fields\"]\n        frontend_node_template = frontend_node[\"template\"]\n        _ = process_prompt_template(\n            template=prompt_template,\n            name=\"template\",\n            custom_fields=custom_fields,\n            frontend_node_template=frontend_node_template,\n        )\n        return frontend_node\n\n    def post_code_processing(self, new_frontend_node: dict, current_frontend_node: dict):\n        \"\"\"\n        This function is called after the code validation is done.\n        \"\"\"\n        frontend_node = super().post_code_processing(new_frontend_node, current_frontend_node)\n        template = frontend_node[\"template\"][\"template\"][\"value\"]\n        # Kept it duplicated for backwards compatibility\n        _ = process_prompt_template(\n            template=template,\n            name=\"template\",\n            custom_fields=frontend_node[\"custom_fields\"],\n            frontend_node_template=frontend_node[\"template\"],\n        )\n        # Now that template is updated, we need to grab any values that were set in the current_frontend_node\n        # and update the frontend_node with those values\n        update_template_values(new_template=frontend_node, previous_template=current_frontend_node[\"template\"])\n        return frontend_node\n\n    def _get_fallback_input(self, **kwargs):\n        return DefaultPromptField(**kwargs)\n","fileTypes":[],"file_path":"","password":false,"name":"code","advanced":true,"dynamic":true,"info":"","load_from_db":false,"title_case":false,"display_name":"code"},"template":{"trace_as_input":true,"list":false,"required":false,"placeholder":"","show":true,"name":"template","value":"{user_prompt}","display_name":"Template","advanced":false,"dynamic":false,"info":"","title_case":false,"type":"prompt","_input_type":"PromptInput"},"user_prompt":{"field_type":"str","required":false,"placeholder":"","list":false,"show":true,"multiline":true,"value":"","fileTypes":[],"file_path":"","password":false,"name":"user_prompt","display_name":"user_prompt","advanced":false,"input_types":["Message","Text"],"dynamic":false,"info":"","load_from_db":false,"title_case":false,"type":"str"}},"description":"Create a prompt template with dynamic variables.","icon":"prompts","is_input":null,"is_output":null,"is_composition":null,"base_classes":["Message"],"name":"","display_name":"Prompt","documentation":"","custom_fields":{"template":["user_prompt"]},"output_types":[],"full_path":null,"pinned":false,"conditional_paths":[],"frozen":false,"outputs":[{"types":["Message"],"selected":"Message","name":"prompt","hidden":null,"display_name":"Prompt Message","method":"build_prompt","value":"__UNDEFINED__","cache":true}],"field_order":["template"],"beta":false,"error":null,"edited":false,"lf_version":"1.0.17"},"id":"Prompt-1lyud"},"selected":true,"width":384,"height":412,"positionAbsolute":{"x":-2177.128851560763,"y":150.38359629552468},"dragging":false},{"id":"Prompt-CcCEo","type":"genericNode","position":{"x":-2178.68201579404,"y":-325.42016585687594},"data":{"type":"Prompt","node":{"template":{"_type":"Component","code":{"type":"code","required":true,"placeholder":"","list":false,"show":true,"multiline":true,"value":"from langflow.base.prompts.api_utils import process_prompt_template\nfrom langflow.custom import Component\nfrom langflow.inputs.inputs import DefaultPromptField\nfrom langflow.io import Output, PromptInput\nfrom langflow.schema.message import Message\nfrom langflow.template.utils import update_template_values\n\n\nclass PromptComponent(Component):\n    display_name: str = \"Prompt\"\n    description: str = \"Create a prompt template with dynamic variables.\"\n    icon = \"prompts\"\n    trace_type = \"prompt\"\n    name = \"Prompt\"\n\n    inputs = [\n        PromptInput(name=\"template\", display_name=\"Template\"),\n    ]\n\n    outputs = [\n        Output(display_name=\"Prompt Message\", name=\"prompt\", method=\"build_prompt\"),\n    ]\n\n    async def build_prompt(\n        self,\n    ) -> Message:\n        prompt = await Message.from_template_and_variables(**self._attributes)\n        self.status = prompt.text\n        return prompt\n\n    def _update_template(self, frontend_node: dict):\n        prompt_template = frontend_node[\"template\"][\"template\"][\"value\"]\n        custom_fields = frontend_node[\"custom_fields\"]\n        frontend_node_template = frontend_node[\"template\"]\n        _ = process_prompt_template(\n            template=prompt_template,\n            name=\"template\",\n            custom_fields=custom_fields,\n            frontend_node_template=frontend_node_template,\n        )\n        return frontend_node\n\n    def post_code_processing(self, new_frontend_node: dict, current_frontend_node: dict):\n        \"\"\"\n        This function is called after the code validation is done.\n        \"\"\"\n        frontend_node = super().post_code_processing(new_frontend_node, current_frontend_node)\n        template = frontend_node[\"template\"][\"template\"][\"value\"]\n        # Kept it duplicated for backwards compatibility\n        _ = process_prompt_template(\n            template=template,\n            name=\"template\",\n            custom_fields=frontend_node[\"custom_fields\"],\n            frontend_node_template=frontend_node[\"template\"],\n        )\n        # Now that template is updated, we need to grab any values that were set in the current_frontend_node\n        # and update the frontend_node with those values\n        update_template_values(new_template=frontend_node, previous_template=current_frontend_node[\"template\"])\n        return frontend_node\n\n    def _get_fallback_input(self, **kwargs):\n        return DefaultPromptField(**kwargs)\n","fileTypes":[],"file_path":"","password":false,"name":"code","advanced":true,"dynamic":true,"info":"","load_from_db":false,"title_case":false,"display_name":"code"},"template":{"trace_as_input":true,"list":false,"required":false,"placeholder":"","show":true,"name":"template","value":"{context}","display_name":"Template","advanced":false,"dynamic":false,"info":"","title_case":false,"type":"prompt","_input_type":"PromptInput"},"context":{"field_type":"str","required":false,"placeholder":"","list":false,"show":true,"multiline":true,"value":"","fileTypes":[],"file_path":"","password":false,"name":"context","display_name":"context","advanced":false,"input_types":["Message","Text"],"dynamic":false,"info":"","load_from_db":false,"title_case":false,"type":"str"}},"description":"Create a prompt template with dynamic variables.","icon":"prompts","is_input":null,"is_output":null,"is_composition":null,"base_classes":["Message"],"name":"","display_name":"Prompt","documentation":"","custom_fields":{"template":["context"]},"output_types":[],"full_path":null,"pinned":false,"conditional_paths":[],"frozen":false,"outputs":[{"types":["Message"],"selected":"Message","name":"prompt","hidden":null,"display_name":"Prompt Message","method":"build_prompt","value":"__UNDEFINED__","cache":true}],"field_order":["template"],"beta":false,"error":null,"edited":false,"lf_version":"1.0.17"},"id":"Prompt-CcCEo"},"selected":true,"width":384,"height":412,"positionAbsolute":{"x":-2178.68201579404,"y":-325.42016585687594},"dragging":false},{"id":"CreateHeaders-zkVPz","type":"genericNode","position":{"x":-657.0341125223471,"y":-128.46552887731997},"data":{"type":"CreateHeaders","node":{"template":{"_type":"Component","code":{"type":"code","required":true,"placeholder":"","list":false,"show":true,"multiline":true,"value":"from langflow.custom import Component\nfrom langflow.inputs import MessageTextInput\nfrom langflow.template import Output\nfrom pydantic.v1 import SecretStr\nfrom langflow.schema import Data\n\n\n\nclass CreateHeadersComponent(Component):\n    display_name = \"Create Headers\"\n    description = \"Creates a headers dictionary with a dynamic Authorization header.\"\n    icon = \"custom_components\"\n    name = \"CreateHeaders\"\n\n    inputs = [\n        MessageTextInput(\n            name=\"input_value\",\n            display_name=\"Authorization Token\",\n            info=\"Enter the token to be used in the Authorization header.\",\n        ),\n    ]\n\n    outputs = [\n        Output(display_name=\"Output\", name=\"output\", method=\"build_output\"),\n    ]\n\n    def build_output(self) -> Data:\n        authorization_header = f\"Bearer {self.input_value}\"\n        headers = {\n            \"Authorization\": authorization_header,\n            \"Content-Type\": \"application/json\",\n        }\n\n        data = Data(value=headers)\n        self.status = data\n        return headers\n        \n        ","fileTypes":[],"file_path":"","password":false,"name":"code","advanced":true,"dynamic":true,"info":"","load_from_db":false,"title_case":false,"display_name":"code"},"input_value":{"trace_as_input":true,"trace_as_metadata":true,"load_from_db":false,"list":false,"required":false,"placeholder":"","show":true,"name":"input_value","value":"","display_name":"Authorization Token","advanced":false,"input_types":["Message"],"dynamic":false,"info":"Enter the token to be used in the Authorization header.","title_case":false,"type":"str","_input_type":"MessageTextInput"}},"description":"Creates a headers dictionary with a dynamic Authorization header.","icon":"custom_components","base_classes":["Data"],"display_name":"Create List","documentation":"","custom_fields":{},"output_types":[],"pinned":false,"conditional_paths":[],"frozen":false,"outputs":[{"types":["Data"],"selected":"Data","name":"output","display_name":"Output","method":"build_output","value":"__UNDEFINED__","cache":true}],"field_order":["input_value"],"beta":false,"edited":true,"lf_version":"1.0.17"},"id":"CreateHeaders-zkVPz"},"selected":true,"width":384,"height":326,"positionAbsolute":{"x":-657.0341125223471,"y":-128.46552887731997},"dragging":false}],"edges":[{"source":"OpenAIModel-joKQi","sourceHandle":"{œdataTypeœ:œOpenAIModelœ,œidœ:œOpenAIModel-joKQiœ,œnameœ:œtext_outputœ,œoutput_typesœ:[œMessageœ]}","target":"CustomComponent-PAxID","targetHandle":"{œfieldNameœ:œinput_valueœ,œidœ:œCustomComponent-PAxIDœ,œinputTypesœ:[œMessageœ],œtypeœ:œstrœ}","data":{"targetHandle":{"fieldName":"input_value","id":"CustomComponent-PAxID","inputTypes":["Message"],"type":"str"},"sourceHandle":{"dataType":"OpenAIModel","id":"OpenAIModel-joKQi","name":"text_output","output_types":["Message"]}},"id":"reactflow__edge-OpenAIModel-joKQi{œdataTypeœ:œOpenAIModelœ,œidœ:œOpenAIModel-joKQiœ,œnameœ:œtext_outputœ,œoutput_typesœ:[œMessageœ]}-CustomComponent-PAxID{œfieldNameœ:œinput_valueœ,œidœ:œCustomComponent-PAxIDœ,œinputTypesœ:[œMessageœ],œtypeœ:œstrœ}","className":"","selected":true},{"source":"APIRequest-PrH39","sourceHandle":"{œdataTypeœ:œAPIRequestœ,œidœ:œAPIRequest-PrH39œ,œnameœ:œdataœ,œoutput_typesœ:[œDataœ]}","target":"FetchProcessorComponent-toRll","targetHandle":"{œfieldNameœ:œfetch_responseœ,œidœ:œFetchProcessorComponent-toRllœ,œinputTypesœ:[œDataœ],œtypeœ:œotherœ}","data":{"targetHandle":{"fieldName":"fetch_response","id":"FetchProcessorComponent-toRll","inputTypes":["Data"],"type":"other"},"sourceHandle":{"dataType":"APIRequest","id":"APIRequest-PrH39","name":"data","output_types":["Data"]}},"id":"reactflow__edge-APIRequest-PrH39{œdataTypeœ:œAPIRequestœ,œidœ:œAPIRequest-PrH39œ,œnameœ:œdataœ,œoutput_typesœ:[œDataœ]}-FetchProcessorComponent-toRll{œfieldNameœ:œfetch_responseœ,œidœ:œFetchProcessorComponent-toRllœ,œinputTypesœ:[œDataœ],œtypeœ:œotherœ}","className":"","selected":true},{"source":"FetchProcessorComponent-toRll","sourceHandle":"{œdataTypeœ:œFetchProcessorComponentœ,œidœ:œFetchProcessorComponent-toRllœ,œnameœ:œprocessed_outputœ,œoutput_typesœ:[œMessageœ]}","target":"Prompt-r2X9G","targetHandle":"{œfieldNameœ:œoutputœ,œidœ:œPrompt-r2X9Gœ,œinputTypesœ:[œMessageœ,œTextœ],œtypeœ:œstrœ}","data":{"targetHandle":{"fieldName":"output","id":"Prompt-r2X9G","inputTypes":["Message","Text"],"type":"str"},"sourceHandle":{"dataType":"FetchProcessorComponent","id":"FetchProcessorComponent-toRll","name":"processed_output","output_types":["Message"]}},"id":"reactflow__edge-FetchProcessorComponent-toRll{œdataTypeœ:œFetchProcessorComponentœ,œidœ:œFetchProcessorComponent-toRllœ,œnameœ:œprocessed_outputœ,œoutput_typesœ:[œMessageœ]}-Prompt-r2X9G{œfieldNameœ:œoutputœ,œidœ:œPrompt-r2X9Gœ,œinputTypesœ:[œMessageœ,œTextœ],œtypeœ:œstrœ}","className":"","selected":true},{"source":"Prompt-r2X9G","sourceHandle":"{œdataTypeœ:œPromptœ,œidœ:œPrompt-r2X9Gœ,œnameœ:œpromptœ,œoutput_typesœ:[œMessageœ]}","target":"OpenAIModel-fJtPg","targetHandle":"{œfieldNameœ:œinput_valueœ,œidœ:œOpenAIModel-fJtPgœ,œinputTypesœ:[œMessageœ],œtypeœ:œstrœ}","data":{"targetHandle":{"fieldName":"input_value","id":"OpenAIModel-fJtPg","inputTypes":["Message"],"type":"str"},"sourceHandle":{"dataType":"Prompt","id":"Prompt-r2X9G","name":"prompt","output_types":["Message"]}},"id":"reactflow__edge-Prompt-r2X9G{œdataTypeœ:œPromptœ,œidœ:œPrompt-r2X9Gœ,œnameœ:œpromptœ,œoutput_typesœ:[œMessageœ]}-OpenAIModel-fJtPg{œfieldNameœ:œinput_valueœ,œidœ:œOpenAIModel-fJtPgœ,œinputTypesœ:[œMessageœ],œtypeœ:œstrœ}","className":"","selected":true},{"source":"CustomComponent-PAxID","sourceHandle":"{œdataTypeœ:œCustomComponentœ,œidœ:œCustomComponent-PAxIDœ,œnameœ:œoutputœ,œoutput_typesœ:[œDataœ]}","target":"APIRequest-PrH39","targetHandle":"{œfieldNameœ:œbodyœ,œidœ:œAPIRequest-PrH39œ,œinputTypesœ:[œDataœ],œtypeœ:œNestedDictœ}","data":{"targetHandle":{"fieldName":"body","id":"APIRequest-PrH39","inputTypes":["Data"],"type":"NestedDict"},"sourceHandle":{"dataType":"CustomComponent","id":"CustomComponent-PAxID","name":"output","output_types":["Data"]}},"id":"reactflow__edge-CustomComponent-PAxID{œdataTypeœ:œCustomComponentœ,œidœ:œCustomComponent-PAxIDœ,œnameœ:œoutputœ,œoutput_typesœ:[œDataœ]}-APIRequest-PrH39{œfieldNameœ:œbodyœ,œidœ:œAPIRequest-PrH39œ,œinputTypesœ:[œDataœ],œtypeœ:œNestedDictœ}","className":"","selected":true},{"source":"OpenAIModel-joKQi","sourceHandle":"{œdataTypeœ:œOpenAIModelœ,œidœ:œOpenAIModel-joKQiœ,œnameœ:œtext_outputœ,œoutput_typesœ:[œMessageœ]}","target":"Prompt-r2X9G","targetHandle":"{œfieldNameœ:œqueryœ,œidœ:œPrompt-r2X9Gœ,œinputTypesœ:[œMessageœ,œTextœ],œtypeœ:œstrœ}","data":{"targetHandle":{"fieldName":"query","id":"Prompt-r2X9G","inputTypes":["Message","Text"],"type":"str"},"sourceHandle":{"dataType":"OpenAIModel","id":"OpenAIModel-joKQi","name":"text_output","output_types":["Message"]}},"id":"reactflow__edge-OpenAIModel-joKQi{œdataTypeœ:œOpenAIModelœ,œidœ:œOpenAIModel-joKQiœ,œnameœ:œtext_outputœ,œoutput_typesœ:[œMessageœ]}-Prompt-r2X9G{œfieldNameœ:œqueryœ,œidœ:œPrompt-r2X9Gœ,œinputTypesœ:[œMessageœ,œTextœ],œtypeœ:œstrœ}","className":"","selected":true},{"source":"Prompt-CiXqc","sourceHandle":"{œdataTypeœ:œPromptœ,œidœ:œPrompt-CiXqcœ,œnameœ:œpromptœ,œoutput_typesœ:[œMessageœ]}","target":"OpenAIModel-joKQi","targetHandle":"{œfieldNameœ:œinput_valueœ,œidœ:œOpenAIModel-joKQiœ,œinputTypesœ:[œMessageœ],œtypeœ:œstrœ}","data":{"targetHandle":{"fieldName":"input_value","id":"OpenAIModel-joKQi","inputTypes":["Message"],"type":"str"},"sourceHandle":{"dataType":"Prompt","id":"Prompt-CiXqc","name":"prompt","output_types":["Message"]}},"id":"reactflow__edge-Prompt-CiXqc{œdataTypeœ:œPromptœ,œidœ:œPrompt-CiXqcœ,œnameœ:œpromptœ,œoutput_typesœ:[œMessageœ]}-OpenAIModel-joKQi{œfieldNameœ:œinput_valueœ,œidœ:œOpenAIModel-joKQiœ,œinputTypesœ:[œMessageœ],œtypeœ:œstrœ}","selected":true,"className":""},{"source":"Prompt-1lyud","sourceHandle":"{œdataTypeœ:œPromptœ,œidœ:œPrompt-1lyudœ,œnameœ:œpromptœ,œoutput_typesœ:[œMessageœ]}","target":"Prompt-CiXqc","targetHandle":"{œfieldNameœ:œuser_promptœ,œidœ:œPrompt-CiXqcœ,œinputTypesœ:[œMessageœ,œTextœ],œtypeœ:œstrœ}","data":{"targetHandle":{"fieldName":"user_prompt","id":"Prompt-CiXqc","inputTypes":["Message","Text"],"type":"str"},"sourceHandle":{"dataType":"Prompt","id":"Prompt-1lyud","name":"prompt","output_types":["Message"]}},"id":"reactflow__edge-Prompt-1lyud{œdataTypeœ:œPromptœ,œidœ:œPrompt-1lyudœ,œnameœ:œpromptœ,œoutput_typesœ:[œMessageœ]}-Prompt-CiXqc{œfieldNameœ:œuser_promptœ,œidœ:œPrompt-CiXqcœ,œinputTypesœ:[œMessageœ,œTextœ],œtypeœ:œstrœ}","selected":true},{"source":"Prompt-1lyud","sourceHandle":"{œdataTypeœ:œPromptœ,œidœ:œPrompt-1lyudœ,œnameœ:œpromptœ,œoutput_typesœ:[œMessageœ]}","target":"Prompt-r2X9G","targetHandle":"{œfieldNameœ:œinstructionsœ,œidœ:œPrompt-r2X9Gœ,œinputTypesœ:[œMessageœ,œTextœ],œtypeœ:œstrœ}","data":{"targetHandle":{"fieldName":"instructions","id":"Prompt-r2X9G","inputTypes":["Message","Text"],"type":"str"},"sourceHandle":{"dataType":"Prompt","id":"Prompt-1lyud","name":"prompt","output_types":["Message"]}},"id":"reactflow__edge-Prompt-1lyud{œdataTypeœ:œPromptœ,œidœ:œPrompt-1lyudœ,œnameœ:œpromptœ,œoutput_typesœ:[œMessageœ]}-Prompt-r2X9G{œfieldNameœ:œinstructionsœ,œidœ:œPrompt-r2X9Gœ,œinputTypesœ:[œMessageœ,œTextœ],œtypeœ:œstrœ}","selected":true},{"source":"Prompt-CcCEo","sourceHandle":"{œdataTypeœ:œPromptœ,œidœ:œPrompt-CcCEoœ,œnameœ:œpromptœ,œoutput_typesœ:[œMessageœ]}","target":"Prompt-r2X9G","targetHandle":"{œfieldNameœ:œcontextœ,œidœ:œPrompt-r2X9Gœ,œinputTypesœ:[œMessageœ,œTextœ],œtypeœ:œstrœ}","data":{"targetHandle":{"fieldName":"context","id":"Prompt-r2X9G","inputTypes":["Message","Text"],"type":"str"},"sourceHandle":{"dataType":"Prompt","id":"Prompt-CcCEo","name":"prompt","output_types":["Message"]}},"id":"reactflow__edge-Prompt-CcCEo{œdataTypeœ:œPromptœ,œidœ:œPrompt-CcCEoœ,œnameœ:œpromptœ,œoutput_typesœ:[œMessageœ]}-Prompt-r2X9G{œfieldNameœ:œcontextœ,œidœ:œPrompt-r2X9Gœ,œinputTypesœ:[œMessageœ,œTextœ],œtypeœ:œstrœ}","selected":true},{"source":"Prompt-CcCEo","sourceHandle":"{œdataTypeœ:œPromptœ,œidœ:œPrompt-CcCEoœ,œnameœ:œpromptœ,œoutput_typesœ:[œMessageœ]}","target":"Prompt-CiXqc","targetHandle":"{œfieldNameœ:œcontextœ,œidœ:œPrompt-CiXqcœ,œinputTypesœ:[œMessageœ,œTextœ],œtypeœ:œstrœ}","data":{"targetHandle":{"fieldName":"context","id":"Prompt-CiXqc","inputTypes":["Message","Text"],"type":"str"},"sourceHandle":{"dataType":"Prompt","id":"Prompt-CcCEo","name":"prompt","output_types":["Message"]}},"id":"reactflow__edge-Prompt-CcCEo{œdataTypeœ:œPromptœ,œidœ:œPrompt-CcCEoœ,œnameœ:œpromptœ,œoutput_typesœ:[œMessageœ]}-Prompt-CiXqc{œfieldNameœ:œcontextœ,œidœ:œPrompt-CiXqcœ,œinputTypesœ:[œMessageœ,œTextœ],œtypeœ:œstrœ}","selected":true},{"source":"CreateHeaders-zkVPz","sourceHandle":"{œdataTypeœ:œCreateHeadersœ,œidœ:œCreateHeaders-zkVPzœ,œnameœ:œoutputœ,œoutput_typesœ:[œDataœ]}","target":"APIRequest-PrH39","targetHandle":"{œfieldNameœ:œheadersœ,œidœ:œAPIRequest-PrH39œ,œinputTypesœ:[œDataœ],œtypeœ:œNestedDictœ}","data":{"targetHandle":{"fieldName":"headers","id":"APIRequest-PrH39","inputTypes":["Data"],"type":"NestedDict"},"sourceHandle":{"dataType":"CreateHeaders","id":"CreateHeaders-zkVPz","name":"output","output_types":["Data"]}},"id":"reactflow__edge-CreateHeaders-zkVPz{œdataTypeœ:œCreateHeadersœ,œidœ:œCreateHeaders-zkVPzœ,œnameœ:œoutputœ,œoutput_typesœ:[œDataœ]}-APIRequest-PrH39{œfieldNameœ:œheadersœ,œidœ:œAPIRequest-PrH39œ,œinputTypesœ:[œDataœ],œtypeœ:œNestedDictœ}","selected":true}],"viewport":{"zoom":1,"x":0,"y":0}},"is_component":false,"name":"Dazzling Volhard","description":"","id":"A8EHc"},"outputs":[{"types":["LanguageModel"],"selected":"LanguageModel","name":"OpenAIModel-OHg1W_model_output","display_name":"Language Model","method":"build_model","value":"__UNDEFINED__","cache":true,"proxy":{"id":"OpenAIModel-joKQi","name":"model_output","nodeDisplayName":"OpenAI"}},{"types":["Message"],"selected":"Message","name":"OpenAIModel-BxXKu_text_output","display_name":"Text","method":"text_response","value":"__UNDEFINED__","cache":true,"proxy":{"id":"OpenAIModel-fJtPg","name":"text_output","nodeDisplayName":"OpenAI"}},{"types":["LanguageModel"],"selected":"LanguageModel","name":"OpenAIModel-BxXKu_model_output","display_name":"Language Model","method":"build_model","value":"__UNDEFINED__","cache":true,"proxy":{"id":"OpenAIModel-fJtPg","name":"model_output","nodeDisplayName":"OpenAI"}}]}},"selected":false,"width":384,"height":944,"positionAbsolute":{"x":-1023.8468916717701,"y":1331.818463507503},"dragging":false},{"id":"GroupNode-OjoFe","type":"genericNode","position":{"x":-452.7851817216633,"y":1327.9770738829372},"data":{"id":"GroupNode-OjoFe","type":"GroupNode","node":{"display_name":"GA4 Agent","documentation":"","description":"","template":{"query_params_APIRequest-XsMPY":{"trace_as_input":true,"trace_as_metadata":true,"list":false,"required":false,"placeholder":"","show":true,"value":"","name":"query_params","display_name":"Query Parameters","advanced":false,"input_types":["Data"],"dynamic":false,"info":"The query parameters to append to the URL.","title_case":false,"type":"other","_input_type":"DataInput","proxy":{"id":"APIRequest-kjVCi","field":"query_params"}},"code_APIRequest-XsMPY":{"type":"code","required":true,"placeholder":"","list":false,"show":true,"multiline":true,"value":"import asyncio\nimport json\nfrom typing import Any, List, Optional\nfrom urllib.parse import parse_qsl, urlencode, urlparse, urlunparse\n\nimport httpx\nfrom loguru import logger\n\nfrom langflow.base.curl.parse import parse_context\nfrom langflow.custom import Component\nfrom langflow.io import DataInput, DropdownInput, IntInput, MessageTextInput, NestedDictInput, Output\nfrom langflow.schema import Data\nfrom langflow.schema.dotdict import dotdict\n\n\nclass APIRequestComponent(Component):\n    display_name = \"API Request\"\n    description = (\n        \"This component allows you to make HTTP requests to one or more URLs. \"\n        \"You can provide headers and body as either dictionaries or Data objects. \"\n        \"Additionally, you can append query parameters to the URLs.\\n\\n\"\n        \"**Note:** Check advanced options for more settings.\"\n    )\n    icon = \"Globe\"\n    name = \"APIRequest\"\n\n    inputs = [\n        MessageTextInput(\n            name=\"urls\",\n            display_name=\"URLs\",\n            is_list=True,\n            info=\"Enter one or more URLs, separated by commas.\",\n        ),\n        MessageTextInput(\n            name=\"curl\",\n            display_name=\"Curl\",\n            info=\"Paste a curl command to populate the fields. This will fill in the dictionary fields for headers and body.\",\n            advanced=False,\n            refresh_button=True,\n        ),\n        DropdownInput(\n            name=\"method\",\n            display_name=\"Method\",\n            options=[\"GET\", \"POST\", \"PATCH\", \"PUT\"],\n            value=\"GET\",\n            info=\"The HTTP method to use (GET, POST, PATCH, PUT).\",\n        ),\n        NestedDictInput(\n            name=\"headers\",\n            display_name=\"Headers\",\n            info=\"The headers to send with the request as a dictionary. This is populated when using the CURL field.\",\n            input_types=[\"Data\"],\n        ),\n        NestedDictInput(\n            name=\"body\",\n            display_name=\"Body\",\n            info=\"The body to send with the request as a dictionary (for POST, PATCH, PUT). This is populated when using the CURL field.\",\n            input_types=[\"Data\"],\n        ),\n        DataInput(\n            name=\"query_params\",\n            display_name=\"Query Parameters\",\n            info=\"The query parameters to append to the URL.\",\n        ),\n        IntInput(\n            name=\"timeout\",\n            display_name=\"Timeout\",\n            value=5,\n            info=\"The timeout to use for the request.\",\n        ),\n    ]\n\n    outputs = [\n        Output(display_name=\"Data\", name=\"data\", method=\"make_requests\"),\n    ]\n\n    def parse_curl(self, curl: str, build_config: dotdict) -> dotdict:\n        try:\n            parsed = parse_context(curl)\n            build_config[\"urls\"][\"value\"] = [parsed.url]\n            build_config[\"method\"][\"value\"] = parsed.method.upper()\n            build_config[\"headers\"][\"value\"] = dict(parsed.headers)\n\n            if parsed.data:\n                try:\n                    json_data = json.loads(parsed.data)\n                    build_config[\"body\"][\"value\"] = json_data\n                except json.JSONDecodeError as e:\n                    logger.error(f\"Error decoding JSON data: {e}\")\n            else:\n                build_config[\"body\"][\"value\"] = {}\n        except Exception as exc:\n            logger.error(f\"Error parsing curl: {exc}\")\n            raise ValueError(f\"Error parsing curl: {exc}\")\n        return build_config\n\n    def update_build_config(self, build_config: dotdict, field_value: Any, field_name: str | None = None):\n        if field_name == \"curl\" and field_value:\n            build_config = self.parse_curl(field_value, build_config)\n        return build_config\n\n    async def make_request(\n        self,\n        client: httpx.AsyncClient,\n        method: str,\n        url: str,\n        headers: Optional[dict] = None,\n        body: Optional[dict] = None,\n        timeout: int = 5,\n    ) -> Data:\n        method = method.upper()\n        if method not in [\"GET\", \"POST\", \"PATCH\", \"PUT\", \"DELETE\"]:\n            raise ValueError(f\"Unsupported method: {method}\")\n\n        if isinstance(body, str) and body:\n            try:\n                body = json.loads(body)\n            except Exception as e:\n                logger.error(f\"Error decoding JSON data: {e}\")\n                body = None\n                raise ValueError(f\"Error decoding JSON data: {e}\")\n\n        data = body if body else None\n\n        try:\n            response = await client.request(method, url, headers=headers, json=data, timeout=timeout)\n            try:\n                result = response.json()\n            except Exception:\n                result = response.text\n            return Data(\n                data={\n                    \"source\": url,\n                    \"headers\": headers,\n                    \"status_code\": response.status_code,\n                    \"result\": result,\n                },\n            )\n        except httpx.TimeoutException:\n            return Data(\n                data={\n                    \"source\": url,\n                    \"headers\": headers,\n                    \"status_code\": 408,\n                    \"error\": \"Request timed out\",\n                },\n            )\n        except Exception as exc:\n            return Data(\n                data={\n                    \"source\": url,\n                    \"headers\": headers,\n                    \"status_code\": 500,\n                    \"error\": str(exc),\n                },\n            )\n\n    def add_query_params(self, url: str, params: dict) -> str:\n        url_parts = list(urlparse(url))\n        query = dict(parse_qsl(url_parts[4]))\n        query.update(params)\n        url_parts[4] = urlencode(query)\n        return urlunparse(url_parts)\n\n    async def make_requests(self) -> List[Data]:\n        method = self.method\n        urls = [url.strip() for url in self.urls if url.strip()]\n        curl = self.curl\n        headers = self.headers or {}\n        body = self.body or {}\n        timeout = self.timeout\n        query_params = self.query_params.data if self.query_params else {}\n\n        if curl:\n            self._build_config = self.parse_curl(curl, dotdict())\n\n        if isinstance(headers, Data):\n            headers = headers.data\n\n        if isinstance(body, Data):\n            body = body.data\n\n        bodies = [body] * len(urls)\n\n        urls = [self.add_query_params(url, query_params) for url in urls]\n\n        async with httpx.AsyncClient() as client:\n            results = await asyncio.gather(\n                *[self.make_request(client, method, u, headers, rec, timeout) for u, rec in zip(urls, bodies)]\n            )\n        self.status = results\n        return results\n","fileTypes":[],"file_path":"","password":false,"name":"code","advanced":true,"dynamic":true,"info":"","load_from_db":false,"title_case":false,"display_name":"code","proxy":{"id":"APIRequest-kjVCi","field":"code"}},"curl_APIRequest-XsMPY":{"trace_as_input":true,"trace_as_metadata":true,"load_from_db":false,"list":false,"required":false,"placeholder":"","show":true,"value":"","name":"curl","display_name":"Curl","advanced":false,"input_types":["Message"],"dynamic":false,"info":"Paste a curl command to populate the fields. This will fill in the dictionary fields for headers and body.","refresh_button":true,"title_case":false,"type":"str","_input_type":"MessageTextInput","proxy":{"id":"APIRequest-kjVCi","field":"curl"}},"method_APIRequest-XsMPY":{"trace_as_metadata":true,"options":["GET","POST","PATCH","PUT"],"combobox":false,"required":false,"placeholder":"","show":true,"value":"POST","name":"method","display_name":"Method","advanced":true,"dynamic":false,"info":"The HTTP method to use (GET, POST, PATCH, PUT).","title_case":false,"type":"str","_input_type":"DropdownInput","proxy":{"id":"APIRequest-kjVCi","field":"method"}},"timeout_APIRequest-XsMPY":{"trace_as_metadata":true,"list":false,"required":false,"placeholder":"","show":true,"value":"5","name":"timeout","display_name":"Timeout","advanced":true,"dynamic":false,"info":"The timeout to use for the request.","title_case":false,"type":"int","_input_type":"IntInput","proxy":{"id":"APIRequest-kjVCi","field":"timeout"}},"urls_APIRequest-XsMPY":{"trace_as_input":true,"trace_as_metadata":true,"load_from_db":false,"list":true,"required":false,"placeholder":"","show":true,"value":["https://bigquery.googleapis.com/bigquery/v2/projects/spherical-proxy-424916-c9/queries"],"name":"urls","display_name":"URLs","advanced":false,"input_types":["Message"],"dynamic":false,"info":"Enter one or more URLs, separated by commas.","title_case":false,"type":"str","_input_type":"MessageTextInput","proxy":{"id":"APIRequest-kjVCi","field":"urls"}},"api_key_OpenAIModel-OHg1W":{"load_from_db":false,"required":false,"placeholder":"","show":true,"name":"api_key","value":null,"display_name":"OpenAI API Key","advanced":false,"input_types":["Message"],"dynamic":false,"info":"The OpenAI API Key to use for the OpenAI model.","title_case":false,"password":true,"type":"str","_input_type":"SecretStrInput","proxy":{"id":"OpenAIModel-4LBlQ","field":"api_key"}},"code_OpenAIModel-OHg1W":{"type":"code","required":true,"placeholder":"","list":false,"show":true,"multiline":true,"value":"import operator\nfrom functools import reduce\n\nfrom langflow.field_typing.range_spec import RangeSpec\nfrom langchain_openai import ChatOpenAI\nfrom pydantic.v1 import SecretStr\n\nfrom langflow.base.models.model import LCModelComponent\nfrom langflow.base.models.openai_constants import OPENAI_MODEL_NAMES\nfrom langflow.field_typing import LanguageModel\nfrom langflow.inputs import (\n    BoolInput,\n    DictInput,\n    DropdownInput,\n    FloatInput,\n    IntInput,\n    SecretStrInput,\n    StrInput,\n)\n\n\nclass OpenAIModelComponent(LCModelComponent):\n    display_name = \"OpenAI\"\n    description = \"Generates text using OpenAI LLMs.\"\n    icon = \"OpenAI\"\n    name = \"OpenAIModel\"\n\n    inputs = LCModelComponent._base_inputs + [\n        IntInput(\n            name=\"max_tokens\",\n            display_name=\"Max Tokens\",\n            advanced=True,\n            info=\"The maximum number of tokens to generate. Set to 0 for unlimited tokens.\",\n            range_spec=RangeSpec(min=0, max=128000),\n        ),\n        DictInput(name=\"model_kwargs\", display_name=\"Model Kwargs\", advanced=True),\n        BoolInput(\n            name=\"json_mode\",\n            display_name=\"JSON Mode\",\n            advanced=True,\n            info=\"If True, it will output JSON regardless of passing a schema.\",\n        ),\n        DictInput(\n            name=\"output_schema\",\n            is_list=True,\n            display_name=\"Schema\",\n            advanced=True,\n            info=\"The schema for the Output of the model. You must pass the word JSON in the prompt. If left blank, JSON mode will be disabled.\",\n        ),\n        DropdownInput(\n            name=\"model_name\",\n            display_name=\"Model Name\",\n            advanced=False,\n            options=OPENAI_MODEL_NAMES,\n            value=OPENAI_MODEL_NAMES[0],\n        ),\n        StrInput(\n            name=\"openai_api_base\",\n            display_name=\"OpenAI API Base\",\n            advanced=True,\n            info=\"The base URL of the OpenAI API. Defaults to https://api.openai.com/v1. You can change this to use other APIs like JinaChat, LocalAI and Prem.\",\n        ),\n        SecretStrInput(\n            name=\"api_key\",\n            display_name=\"OpenAI API Key\",\n            info=\"The OpenAI API Key to use for the OpenAI model.\",\n            advanced=False,\n            value=\"OPENAI_API_KEY\",\n        ),\n        FloatInput(name=\"temperature\", display_name=\"Temperature\", value=0.1),\n        IntInput(\n            name=\"seed\",\n            display_name=\"Seed\",\n            info=\"The seed controls the reproducibility of the job.\",\n            advanced=True,\n            value=1,\n        ),\n    ]\n\n    def build_model(self) -> LanguageModel:  # type: ignore[type-var]\n        # self.output_schema is a list of dictionaries\n        # let's convert it to a dictionary\n        output_schema_dict: dict[str, str] = reduce(operator.ior, self.output_schema or {}, {})\n        openai_api_key = self.api_key\n        temperature = self.temperature\n        model_name: str = self.model_name\n        max_tokens = self.max_tokens\n        model_kwargs = self.model_kwargs or {}\n        openai_api_base = self.openai_api_base or \"https://api.openai.com/v1\"\n        json_mode = bool(output_schema_dict) or self.json_mode\n        seed = self.seed\n\n        if openai_api_key:\n            api_key = SecretStr(openai_api_key)\n        else:\n            api_key = None\n        output = ChatOpenAI(\n            max_tokens=max_tokens or None,\n            model_kwargs=model_kwargs,\n            model=model_name,\n            base_url=openai_api_base,\n            api_key=api_key,\n            temperature=temperature if temperature is not None else 0.1,\n            seed=seed,\n        )\n        if json_mode:\n            if output_schema_dict:\n                output = output.with_structured_output(schema=output_schema_dict, method=\"json_mode\")  # type: ignore\n            else:\n                output = output.bind(response_format={\"type\": \"json_object\"})  # type: ignore\n\n        return output  # type: ignore\n\n    def _get_exception_message(self, e: Exception):\n        \"\"\"\n        Get a message from an OpenAI exception.\n\n        Args:\n            exception (Exception): The exception to get the message from.\n\n        Returns:\n            str: The message from the exception.\n        \"\"\"\n\n        try:\n            from openai import BadRequestError\n        except ImportError:\n            return\n        if isinstance(e, BadRequestError):\n            message = e.body.get(\"message\")  # type: ignore\n            if message:\n                return message\n        return\n","fileTypes":[],"file_path":"","password":false,"name":"code","advanced":true,"dynamic":true,"info":"","load_from_db":false,"title_case":false,"display_name":"code","proxy":{"id":"OpenAIModel-4LBlQ","field":"code"}},"json_mode_OpenAIModel-OHg1W":{"trace_as_metadata":true,"list":false,"required":false,"placeholder":"","show":true,"name":"json_mode","value":false,"display_name":"JSON Mode","advanced":true,"dynamic":false,"info":"If True, it will output JSON regardless of passing a schema.","title_case":false,"type":"bool","_input_type":"BoolInput","proxy":{"id":"OpenAIModel-4LBlQ","field":"json_mode"}},"max_tokens_OpenAIModel-OHg1W":{"trace_as_metadata":true,"range_spec":{"step_type":"float","min":0,"max":128000,"step":0.1},"list":false,"required":false,"placeholder":"","show":true,"name":"max_tokens","value":"","display_name":"Max Tokens","advanced":true,"dynamic":false,"info":"The maximum number of tokens to generate. Set to 0 for unlimited tokens.","title_case":false,"type":"int","_input_type":"IntInput","proxy":{"id":"OpenAIModel-4LBlQ","field":"max_tokens"}},"model_kwargs_OpenAIModel-OHg1W":{"trace_as_input":true,"list":false,"required":false,"placeholder":"","show":true,"name":"model_kwargs","value":{},"display_name":"Model Kwargs","advanced":true,"dynamic":false,"info":"","title_case":false,"type":"dict","_input_type":"DictInput","proxy":{"id":"OpenAIModel-4LBlQ","field":"model_kwargs"}},"model_name_OpenAIModel-OHg1W":{"trace_as_metadata":true,"options":["gpt-4o-mini","gpt-4o","gpt-4-turbo","gpt-4-turbo-preview","gpt-4","gpt-3.5-turbo","gpt-3.5-turbo-0125"],"combobox":false,"required":false,"placeholder":"","show":true,"name":"model_name","value":"gpt-4o","display_name":"Model Name","advanced":true,"dynamic":false,"info":"","title_case":false,"type":"str","_input_type":"DropdownInput","proxy":{"id":"OpenAIModel-4LBlQ","field":"model_name"}},"openai_api_base_OpenAIModel-OHg1W":{"trace_as_metadata":true,"load_from_db":false,"list":false,"required":false,"placeholder":"","show":true,"name":"openai_api_base","value":"","display_name":"OpenAI API Base","advanced":true,"dynamic":false,"info":"The base URL of the OpenAI API. Defaults to https://api.openai.com/v1. You can change this to use other APIs like JinaChat, LocalAI and Prem.","title_case":false,"type":"str","_input_type":"StrInput","proxy":{"id":"OpenAIModel-4LBlQ","field":"openai_api_base"}},"output_schema_OpenAIModel-OHg1W":{"trace_as_input":true,"list":true,"required":false,"placeholder":"","show":true,"name":"output_schema","value":{},"display_name":"Schema","advanced":true,"dynamic":false,"info":"The schema for the Output of the model. You must pass the word JSON in the prompt. If left blank, JSON mode will be disabled.","title_case":false,"type":"dict","_input_type":"DictInput","proxy":{"id":"OpenAIModel-4LBlQ","field":"output_schema"}},"seed_OpenAIModel-OHg1W":{"trace_as_metadata":true,"list":false,"required":false,"placeholder":"","show":true,"name":"seed","value":1,"display_name":"Seed","advanced":true,"dynamic":false,"info":"The seed controls the reproducibility of the job.","title_case":false,"type":"int","_input_type":"IntInput","proxy":{"id":"OpenAIModel-4LBlQ","field":"seed"}},"stream_OpenAIModel-OHg1W":{"trace_as_metadata":true,"list":false,"required":false,"placeholder":"","show":true,"name":"stream","value":false,"display_name":"Stream","advanced":true,"dynamic":false,"info":"Stream the response from the model. Streaming works only in Chat.","title_case":false,"type":"bool","_input_type":"BoolInput","proxy":{"id":"OpenAIModel-4LBlQ","field":"stream"}},"system_message_OpenAIModel-OHg1W":{"trace_as_input":true,"trace_as_metadata":true,"load_from_db":false,"list":false,"required":false,"placeholder":"","show":true,"name":"system_message","value":"","display_name":"System Message","advanced":true,"input_types":["Message"],"dynamic":false,"info":"System message to pass to the model.","title_case":false,"type":"str","_input_type":"MessageTextInput","proxy":{"id":"OpenAIModel-4LBlQ","field":"system_message"}},"temperature_OpenAIModel-OHg1W":{"trace_as_metadata":true,"list":false,"required":false,"placeholder":"","show":true,"name":"temperature","value":0.1,"display_name":"Temperature","advanced":true,"dynamic":false,"info":"","title_case":false,"type":"float","_input_type":"FloatInput","proxy":{"id":"OpenAIModel-4LBlQ","field":"temperature"}},"code_Prompt-AOko9":{"type":"code","required":true,"placeholder":"","list":false,"show":true,"multiline":true,"value":"from langflow.base.prompts.api_utils import process_prompt_template\nfrom langflow.custom import Component\nfrom langflow.inputs.inputs import DefaultPromptField\nfrom langflow.io import Output, PromptInput\nfrom langflow.schema.message import Message\nfrom langflow.template.utils import update_template_values\n\n\nclass PromptComponent(Component):\n    display_name: str = \"Prompt\"\n    description: str = \"Create a prompt template with dynamic variables.\"\n    icon = \"prompts\"\n    trace_type = \"prompt\"\n    name = \"Prompt\"\n\n    inputs = [\n        PromptInput(name=\"template\", display_name=\"Template\"),\n    ]\n\n    outputs = [\n        Output(display_name=\"Prompt Message\", name=\"prompt\", method=\"build_prompt\"),\n    ]\n\n    async def build_prompt(\n        self,\n    ) -> Message:\n        prompt = await Message.from_template_and_variables(**self._attributes)\n        self.status = prompt.text\n        return prompt\n\n    def _update_template(self, frontend_node: dict):\n        prompt_template = frontend_node[\"template\"][\"template\"][\"value\"]\n        custom_fields = frontend_node[\"custom_fields\"]\n        frontend_node_template = frontend_node[\"template\"]\n        _ = process_prompt_template(\n            template=prompt_template,\n            name=\"template\",\n            custom_fields=custom_fields,\n            frontend_node_template=frontend_node_template,\n        )\n        return frontend_node\n\n    def post_code_processing(self, new_frontend_node: dict, current_frontend_node: dict):\n        \"\"\"\n        This function is called after the code validation is done.\n        \"\"\"\n        frontend_node = super().post_code_processing(new_frontend_node, current_frontend_node)\n        template = frontend_node[\"template\"][\"template\"][\"value\"]\n        # Kept it duplicated for backwards compatibility\n        _ = process_prompt_template(\n            template=template,\n            name=\"template\",\n            custom_fields=frontend_node[\"custom_fields\"],\n            frontend_node_template=frontend_node[\"template\"],\n        )\n        # Now that template is updated, we need to grab any values that were set in the current_frontend_node\n        # and update the frontend_node with those values\n        update_template_values(new_template=frontend_node, previous_template=current_frontend_node[\"template\"])\n        return frontend_node\n\n    def _get_fallback_input(self, **kwargs):\n        return DefaultPromptField(**kwargs)\n","fileTypes":[],"file_path":"","password":false,"name":"code","advanced":true,"dynamic":true,"info":"","load_from_db":false,"title_case":false,"display_name":"code","proxy":{"id":"Prompt-R3rtu","field":"code"}},"template_Prompt-AOko9":{"trace_as_input":true,"list":false,"required":false,"placeholder":"","show":true,"name":"template","value":"**Context** :\n{context}\n**User Query** :\n{user_prompt}\n\n---\n\n**Task** : Create a GA4 SQL query to the `bigquery-public-data.ga4_obfuscated_sample_ecommerce.events_20201209` table. Use the schema information provided below to ensure the query is valid. \n1. **Field Validation** :\n  - Only reference fields that exist in the schema provided below.\n \n  - Use the correct dot notation when accessing fields within complex data structures like `RECORD` (also known as `STRUCT`). For example, access `ecommerce.purchase_revenue_in_usd` directly if it’s a non-repeated field.\n \n  - Use `UNNEST` only for repeated fields (arrays). Do not use `UNNEST` for fields that are simple `STRUCT` (single-record) types. For example, `event_params` and `user_properties` require `UNNEST`, while `ecommerce` does not.\n \n2. **Query Structure** : \n  - Validate that the query structure is correct and does not produce errors related to data types, field names, or incorrect usage of `UNNEST`.\n \n  - Access fields within `STRUCT` (or `RECORD`) directly using dot notation without `UNNEST` if they are not repeated. For instance, fields within `ecommerce` can be accessed directly.\n\n  - Ensure that all referenced fields are appropriately handled, especially when dealing with nested or repeated fields.\n \n3. **Error Handling** :\n  - If a query cannot be formed due to a missing or invalid field in the schema, return an error message instead of a query.\n\n\n---\n\nSchema Information for `events_20201209` Table** :\n\n```json\ncolumn_name,data_type,is_nullable\nevent_date,STRING,YES\nevent_timestamp,INT64,YES\nevent_name,STRING,YES\nevent_params,\"ARRAY<STRUCT<key STRING, value STRUCT<string_value STRING, int_value INT64, float_value FLOAT64, double_value FLOAT64>>>\",NO\nevent_previous_timestamp,INT64,YES\nevent_value_in_usd,FLOAT64,YES\nevent_bundle_sequence_id,INT64,YES\nevent_server_timestamp_offset,INT64,YES\nuser_id,STRING,YES\nuser_pseudo_id,STRING,YES\nprivacy_info,\"STRUCT<analytics_storage INT64, ads_storage INT64, uses_transient_token STRING>\",YES\nuser_properties,\"ARRAY<STRUCT<key INT64, value STRUCT<string_value INT64, int_value INT64, float_value INT64, double_value INT64, set_timestamp_micros INT64>>>\",NO\nuser_first_touch_timestamp,INT64,YES\nuser_ltv,\"STRUCT<revenue FLOAT64, currency STRING>\",YES\ndevice,\"STRUCT<category STRING, mobile_brand_name STRING, mobile_model_name STRING, mobile_marketing_name STRING, mobile_os_hardware_model INT64, operating_system STRING, operating_system_version STRING, vendor_id INT64, advertising_id INT64, language STRING, is_limited_ad_tracking STRING, time_zone_offset_seconds INT64, web_info STRUCT<browser STRING, browser_version STRING>>\",YES\ngeo,\"STRUCT<continent STRING, sub_continent STRING, country STRING, region STRING, city STRING, metro STRING>\",YES\napp_info,\"STRUCT<id STRING, version STRING, install_store STRING, firebase_app_id STRING, install_source STRING>\",YES\ntraffic_source,\"STRUCT<medium STRING, name STRING, source STRING>\",YES\nstream_id,INT64,YES\nplatform,STRING,YES\nevent_dimensions,STRUCT<hostname STRING>,YES\necommerce,\"STRUCT<total_item_quantity INT64, purchase_revenue_in_usd FLOAT64, purchase_revenue FLOAT64, refund_value_in_usd FLOAT64, refund_value FLOAT64, shipping_value_in_usd FLOAT64, shipping_value FLOAT64, tax_value_in_usd FLOAT64, tax_value FLOAT64, unique_items INT64, transaction_id STRING>\",YES\nitems,\"ARRAY<STRUCT<item_id STRING, item_name STRING, item_brand STRING, item_variant STRING, item_category STRING, item_category2 STRING, item_category3 STRING, item_category4 STRING, item_category5 STRING, price_in_usd FLOAT64, price FLOAT64, quantity INT64, item_revenue_in_usd FLOAT64, item_revenue FLOAT64, item_refund_in_usd FLOAT64, item_refund FLOAT64, coupon STRING, affiliation STRING, location_id STRING, item_list_id STRING, item_list_name STRING, item_list_index STRING, promotion_id STRING, promotion_name STRING, creative_name STRING, creative_slot STRING>>\",NO\n```\n\n\n---\n\n**Output Requirement** : Your output must be **only**  the SQL query like the example provided:\n\n```sql\nSELECT * FROM `bigquery-public-data.ga4_obfuscated_sample_ecommerce.events_20201209` LIMIT 0\n```\n\nIf you cannot generate a valid query due to missing or invalid fields, return an error message instead. Ensure that you are running a unique query that was not ran before in the context.\n\n\n---\n\n**AI** :","display_name":"Template","advanced":true,"dynamic":false,"info":"","title_case":false,"type":"prompt","_input_type":"PromptInput","load_from_db":false,"proxy":{"id":"Prompt-R3rtu","field":"template"}},"code_CustomComponent-NwxNM":{"type":"code","required":true,"placeholder":"","list":false,"show":true,"multiline":true,"value":"# from langflow.field_typing import Data\nfrom langflow.custom import Component\nfrom langflow.io import MessageTextInput, Output\nfrom langflow.schema import Data\nimport re  # Import regex module\n\nclass CustomComponent(Component):\n    display_name = \"Format Body\"\n    description = \"Turn OpenAI SQL output into a body to send with a fetch.\"\n    documentation: str = \"http://docs.langflow.org/components/custom\"\n    icon = \"custom_components\"\n    name = \"CustomComponent\"\n\n    inputs = [\n        MessageTextInput(name=\"input_value\", display_name=\"Input Value\", value=\"```sql\\nbigquery-public-data.ga4_obfuscated_sample_ecommerce.events_*\\n```\"),\n    ]\n\n    outputs = [\n        Output(display_name=\"Output\", name=\"output\", method=\"build_output\"),\n    ]\n\n    def build_output(self) -> Data:\n        # Remove the surrounding markdown and any trailing newline\n        input_value_cleaned = re.sub(r\"^```sql\\n|```$\", \"\", self.input_value.strip()).strip()\n\n        # Format the cleaned SQL string into the desired object\n        formatted_data = {\n            \"query\": input_value_cleaned,\n            \"maxResults\": 20,\n            \"timeoutMs\": 10000,\n            \"useLegacySql\": False\n        }\n        \n        # Return the formatted data as the output\n        data = Data(value=formatted_data)\n        self.status = data\n        \n        # Print the formatted data to the chat (for debugging purposes)\n        print(f\"Output Data: {formatted_data}\")\n        \n        return formatted_data\n","fileTypes":[],"file_path":"","password":false,"name":"code","advanced":true,"dynamic":true,"info":"","load_from_db":false,"title_case":false,"display_name":"code","proxy":{"id":"CustomComponent-omGVe","field":"code"}},"code_FetchProcessorComponent-jzDpE":{"type":"code","required":true,"placeholder":"","list":false,"show":true,"multiline":true,"value":"from langflow.custom import Component\nfrom langflow.io import MessageTextInput, Output\nfrom langflow.schema import Data\nfrom langflow.schema.message import Message\nimport json\n\n\nclass FetchProcessorComponent(Component):\n    display_name = \"Fetch Processor\"\n    description = \"Process fetch output, print it, and stringify it for chat.\"\n    documentation: str = \"http://docs.langflow.org/components/custom\"\n    icon = \"custom_components\"\n    name = \"FetchProcessorComponent\"\n\n    inputs = [\n        HandleInput(\n            name=\"fetch_response\",\n            display_name=\"Fetch Response\",\n            input_types=[\"Data\"],\n        )\n    ]\n\n    outputs = [\n        Output(display_name=\"Processed Output\", name=\"processed_output\", method=\"process_output\"),\n    ]\n    \n    def process_output(self) -> Message:\n        try:\n            # Directly use the fetch_response as the input data\n            response_data = self.fetch_response\n\n            # Extract the response content\n            content = str(response_data[0].data[\"result\"])\n            \n            # Truncate the content to a specific length (e.g., 5000 characters)\n            max_length = 5000\n            truncated_content = content[:max_length]\n            \n            # Convert the truncated content to a string that can be printed\n            output_str = \"Processed Output: \" + str(truncated_content)+\"...\"\n            \n            \n            # Return the processed string as a Message\n            self.status = output_str\n            return Message(text=output_str)\n\n        except Exception as e:\n            # Handle any errors and print them\n            error_message = f\"Error processing fetch response: {str(e)}\"\n            print(error_message)\n            return Message(text=error_message)\n","fileTypes":[],"file_path":"","password":false,"name":"code","advanced":true,"dynamic":true,"info":"","load_from_db":false,"title_case":false,"display_name":"code","proxy":{"id":"FetchProcessorComponent-HwwIa","field":"code"}},"api_key_OpenAIModel-BxXKu":{"load_from_db":false,"required":false,"placeholder":"","show":true,"value":null,"name":"api_key","display_name":"OpenAI API Key","advanced":false,"input_types":["Message"],"dynamic":false,"info":"The OpenAI API Key to use for the OpenAI model.","title_case":false,"password":true,"type":"str","_input_type":"SecretStrInput","proxy":{"id":"OpenAIModel-7WJ6a","field":"api_key"}},"code_OpenAIModel-BxXKu":{"type":"code","required":true,"placeholder":"","list":false,"show":true,"multiline":true,"value":"import operator\nfrom functools import reduce\n\nfrom langflow.field_typing.range_spec import RangeSpec\nfrom langchain_openai import ChatOpenAI\nfrom pydantic.v1 import SecretStr\n\nfrom langflow.base.models.model import LCModelComponent\nfrom langflow.base.models.openai_constants import OPENAI_MODEL_NAMES\nfrom langflow.field_typing import LanguageModel\nfrom langflow.inputs import (\n    BoolInput,\n    DictInput,\n    DropdownInput,\n    FloatInput,\n    IntInput,\n    SecretStrInput,\n    StrInput,\n)\n\n\nclass OpenAIModelComponent(LCModelComponent):\n    display_name = \"OpenAI\"\n    description = \"Generates text using OpenAI LLMs.\"\n    icon = \"OpenAI\"\n    name = \"OpenAIModel\"\n\n    inputs = LCModelComponent._base_inputs + [\n        IntInput(\n            name=\"max_tokens\",\n            display_name=\"Max Tokens\",\n            advanced=True,\n            info=\"The maximum number of tokens to generate. Set to 0 for unlimited tokens.\",\n            range_spec=RangeSpec(min=0, max=128000),\n        ),\n        DictInput(name=\"model_kwargs\", display_name=\"Model Kwargs\", advanced=True),\n        BoolInput(\n            name=\"json_mode\",\n            display_name=\"JSON Mode\",\n            advanced=True,\n            info=\"If True, it will output JSON regardless of passing a schema.\",\n        ),\n        DictInput(\n            name=\"output_schema\",\n            is_list=True,\n            display_name=\"Schema\",\n            advanced=True,\n            info=\"The schema for the Output of the model. You must pass the word JSON in the prompt. If left blank, JSON mode will be disabled.\",\n        ),\n        DropdownInput(\n            name=\"model_name\",\n            display_name=\"Model Name\",\n            advanced=False,\n            options=OPENAI_MODEL_NAMES,\n            value=OPENAI_MODEL_NAMES[0],\n        ),\n        StrInput(\n            name=\"openai_api_base\",\n            display_name=\"OpenAI API Base\",\n            advanced=True,\n            info=\"The base URL of the OpenAI API. Defaults to https://api.openai.com/v1. You can change this to use other APIs like JinaChat, LocalAI and Prem.\",\n        ),\n        SecretStrInput(\n            name=\"api_key\",\n            display_name=\"OpenAI API Key\",\n            info=\"The OpenAI API Key to use for the OpenAI model.\",\n            advanced=False,\n            value=\"OPENAI_API_KEY\",\n        ),\n        FloatInput(name=\"temperature\", display_name=\"Temperature\", value=0.1),\n        IntInput(\n            name=\"seed\",\n            display_name=\"Seed\",\n            info=\"The seed controls the reproducibility of the job.\",\n            advanced=True,\n            value=1,\n        ),\n    ]\n\n    def build_model(self) -> LanguageModel:  # type: ignore[type-var]\n        # self.output_schema is a list of dictionaries\n        # let's convert it to a dictionary\n        output_schema_dict: dict[str, str] = reduce(operator.ior, self.output_schema or {}, {})\n        openai_api_key = self.api_key\n        temperature = self.temperature\n        model_name: str = self.model_name\n        max_tokens = self.max_tokens\n        model_kwargs = self.model_kwargs or {}\n        openai_api_base = self.openai_api_base or \"https://api.openai.com/v1\"\n        json_mode = bool(output_schema_dict) or self.json_mode\n        seed = self.seed\n\n        if openai_api_key:\n            api_key = SecretStr(openai_api_key)\n        else:\n            api_key = None\n        output = ChatOpenAI(\n            max_tokens=max_tokens or None,\n            model_kwargs=model_kwargs,\n            model=model_name,\n            base_url=openai_api_base,\n            api_key=api_key,\n            temperature=temperature or 0.1,\n            seed=seed,\n        )\n        if json_mode:\n            if output_schema_dict:\n                output = output.with_structured_output(schema=output_schema_dict, method=\"json_mode\")  # type: ignore\n            else:\n                output = output.bind(response_format={\"type\": \"json_object\"})  # type: ignore\n\n        return output  # type: ignore\n\n    def _get_exception_message(self, e: Exception):\n        \"\"\"\n        Get a message from an OpenAI exception.\n\n        Args:\n            exception (Exception): The exception to get the message from.\n\n        Returns:\n            str: The message from the exception.\n        \"\"\"\n\n        try:\n            from openai import BadRequestError\n        except ImportError:\n            return\n        if isinstance(e, BadRequestError):\n            message = e.body.get(\"message\")  # type: ignore\n            if message:\n                return message\n        return\n","fileTypes":[],"file_path":"","password":false,"name":"code","advanced":true,"dynamic":true,"info":"","load_from_db":false,"title_case":false,"display_name":"code","proxy":{"id":"OpenAIModel-7WJ6a","field":"code"}},"json_mode_OpenAIModel-BxXKu":{"trace_as_metadata":true,"list":false,"required":false,"placeholder":"","show":true,"value":false,"name":"json_mode","display_name":"JSON Mode","advanced":true,"dynamic":false,"info":"If True, it will output JSON regardless of passing a schema.","title_case":false,"type":"bool","_input_type":"BoolInput","proxy":{"id":"OpenAIModel-7WJ6a","field":"json_mode"}},"max_tokens_OpenAIModel-BxXKu":{"trace_as_metadata":true,"range_spec":{"step_type":"float","min":0,"max":128000,"step":0.1},"list":false,"required":false,"placeholder":"","show":true,"value":"","name":"max_tokens","display_name":"Max Tokens","advanced":true,"dynamic":false,"info":"The maximum number of tokens to generate. Set to 0 for unlimited tokens.","title_case":false,"type":"int","_input_type":"IntInput","proxy":{"id":"OpenAIModel-7WJ6a","field":"max_tokens"}},"model_kwargs_OpenAIModel-BxXKu":{"trace_as_input":true,"list":false,"required":false,"placeholder":"","show":true,"value":{},"name":"model_kwargs","display_name":"Model Kwargs","advanced":true,"dynamic":false,"info":"","title_case":false,"type":"dict","_input_type":"DictInput","proxy":{"id":"OpenAIModel-7WJ6a","field":"model_kwargs"}},"model_name_OpenAIModel-BxXKu":{"trace_as_metadata":true,"options":["gpt-4o-mini","gpt-4o","gpt-4-turbo","gpt-4-turbo-preview","gpt-4","gpt-3.5-turbo","gpt-3.5-turbo-0125"],"combobox":false,"required":false,"placeholder":"","show":true,"value":"gpt-4o-mini","name":"model_name","display_name":"Model Name","advanced":true,"dynamic":false,"info":"","title_case":false,"type":"str","_input_type":"DropdownInput","proxy":{"id":"OpenAIModel-7WJ6a","field":"model_name"}},"openai_api_base_OpenAIModel-BxXKu":{"trace_as_metadata":true,"load_from_db":false,"list":false,"required":false,"placeholder":"","show":true,"value":"","name":"openai_api_base","display_name":"OpenAI API Base","advanced":true,"dynamic":false,"info":"The base URL of the OpenAI API. Defaults to https://api.openai.com/v1. You can change this to use other APIs like JinaChat, LocalAI and Prem.","title_case":false,"type":"str","_input_type":"StrInput","proxy":{"id":"OpenAIModel-7WJ6a","field":"openai_api_base"}},"output_schema_OpenAIModel-BxXKu":{"trace_as_input":true,"list":true,"required":false,"placeholder":"","show":true,"value":{},"name":"output_schema","display_name":"Schema","advanced":true,"dynamic":false,"info":"The schema for the Output of the model. You must pass the word JSON in the prompt. If left blank, JSON mode will be disabled.","title_case":false,"type":"dict","_input_type":"DictInput","proxy":{"id":"OpenAIModel-7WJ6a","field":"output_schema"}},"seed_OpenAIModel-BxXKu":{"trace_as_metadata":true,"list":false,"required":false,"placeholder":"","show":true,"value":1,"name":"seed","display_name":"Seed","advanced":true,"dynamic":false,"info":"The seed controls the reproducibility of the job.","title_case":false,"type":"int","_input_type":"IntInput","proxy":{"id":"OpenAIModel-7WJ6a","field":"seed"}},"stream_OpenAIModel-BxXKu":{"trace_as_metadata":true,"list":false,"required":false,"placeholder":"","show":true,"value":false,"name":"stream","display_name":"Stream","advanced":true,"dynamic":false,"info":"Stream the response from the model. Streaming works only in Chat.","title_case":false,"type":"bool","_input_type":"BoolInput","proxy":{"id":"OpenAIModel-7WJ6a","field":"stream"}},"system_message_OpenAIModel-BxXKu":{"trace_as_input":true,"trace_as_metadata":true,"load_from_db":false,"list":false,"required":false,"placeholder":"","show":true,"value":"","name":"system_message","display_name":"System Message","advanced":true,"input_types":["Message"],"dynamic":false,"info":"System message to pass to the model.","title_case":false,"type":"str","_input_type":"MessageTextInput","proxy":{"id":"OpenAIModel-7WJ6a","field":"system_message"}},"temperature_OpenAIModel-BxXKu":{"trace_as_metadata":true,"list":false,"required":false,"placeholder":"","show":true,"value":0.1,"name":"temperature","display_name":"Temperature","advanced":true,"dynamic":false,"info":"","title_case":false,"type":"float","_input_type":"FloatInput","proxy":{"id":"OpenAIModel-7WJ6a","field":"temperature"}},"code_Prompt-NCppB":{"type":"code","required":true,"placeholder":"","list":false,"show":true,"multiline":true,"value":"from langflow.base.prompts.api_utils import process_prompt_template\nfrom langflow.custom import Component\nfrom langflow.inputs.inputs import DefaultPromptField\nfrom langflow.io import Output, PromptInput\nfrom langflow.schema.message import Message\nfrom langflow.template.utils import update_template_values\n\n\nclass PromptComponent(Component):\n    display_name: str = \"Prompt\"\n    description: str = \"Create a prompt template with dynamic variables.\"\n    icon = \"prompts\"\n    trace_type = \"prompt\"\n    name = \"Prompt\"\n\n    inputs = [\n        PromptInput(name=\"template\", display_name=\"Template\"),\n    ]\n\n    outputs = [\n        Output(display_name=\"Prompt Message\", name=\"prompt\", method=\"build_prompt\"),\n    ]\n\n    async def build_prompt(\n        self,\n    ) -> Message:\n        prompt = await Message.from_template_and_variables(**self._attributes)\n        self.status = prompt.text\n        return prompt\n\n    def _update_template(self, frontend_node: dict):\n        prompt_template = frontend_node[\"template\"][\"template\"][\"value\"]\n        custom_fields = frontend_node[\"custom_fields\"]\n        frontend_node_template = frontend_node[\"template\"]\n        _ = process_prompt_template(\n            template=prompt_template,\n            name=\"template\",\n            custom_fields=custom_fields,\n            frontend_node_template=frontend_node_template,\n        )\n        return frontend_node\n\n    def post_code_processing(self, new_frontend_node: dict, current_frontend_node: dict):\n        \"\"\"\n        This function is called after the code validation is done.\n        \"\"\"\n        frontend_node = super().post_code_processing(new_frontend_node, current_frontend_node)\n        template = frontend_node[\"template\"][\"template\"][\"value\"]\n        # Kept it duplicated for backwards compatibility\n        _ = process_prompt_template(\n            template=template,\n            name=\"template\",\n            custom_fields=frontend_node[\"custom_fields\"],\n            frontend_node_template=frontend_node[\"template\"],\n        )\n        # Now that template is updated, we need to grab any values that were set in the current_frontend_node\n        # and update the frontend_node with those values\n        update_template_values(new_template=frontend_node, previous_template=current_frontend_node[\"template\"])\n        return frontend_node\n\n    def _get_fallback_input(self, **kwargs):\n        return DefaultPromptField(**kwargs)\n","fileTypes":[],"file_path":"","password":false,"name":"code","advanced":true,"dynamic":true,"info":"","load_from_db":false,"title_case":false,"display_name":"code","proxy":{"id":"Prompt-6uwLf","field":"code"}},"template_Prompt-NCppB":{"trace_as_input":true,"list":false,"required":false,"placeholder":"","show":true,"name":"template","value":"Context: {context}\n\nUser: I was given these instructions:\n\n```\n{instructions}\n```\n\n\nI gave this query to run:\n\n```\n{query}\n```\n\nI got this output:\n\n```\n{output}\n```\n\n\nDo the following:\n1. Output the query executed\n2. Output a portion of the results (human readable)\n3. Interpret the results in the context of the instructions\n4. Output the insights that you have learned\n5. Create an action for what you think you should do next using reflection.\n\nIf there was an error, output what the error was\n\nAI:","display_name":"Template","advanced":true,"dynamic":false,"info":"","title_case":false,"type":"prompt","_input_type":"PromptInput","load_from_db":false,"proxy":{"id":"Prompt-6uwLf","field":"template"}},"code_Prompt-mw3zb":{"type":"code","required":true,"placeholder":"","list":false,"show":true,"multiline":true,"value":"from langflow.base.prompts.api_utils import process_prompt_template\nfrom langflow.custom import Component\nfrom langflow.inputs.inputs import DefaultPromptField\nfrom langflow.io import Output, PromptInput\nfrom langflow.schema.message import Message\nfrom langflow.template.utils import update_template_values\n\n\nclass PromptComponent(Component):\n    display_name: str = \"Prompt\"\n    description: str = \"Create a prompt template with dynamic variables.\"\n    icon = \"prompts\"\n    trace_type = \"prompt\"\n    name = \"Prompt\"\n\n    inputs = [\n        PromptInput(name=\"template\", display_name=\"Template\"),\n    ]\n\n    outputs = [\n        Output(display_name=\"Prompt Message\", name=\"prompt\", method=\"build_prompt\"),\n    ]\n\n    async def build_prompt(\n        self,\n    ) -> Message:\n        prompt = await Message.from_template_and_variables(**self._attributes)\n        self.status = prompt.text\n        return prompt\n\n    def _update_template(self, frontend_node: dict):\n        prompt_template = frontend_node[\"template\"][\"template\"][\"value\"]\n        custom_fields = frontend_node[\"custom_fields\"]\n        frontend_node_template = frontend_node[\"template\"]\n        _ = process_prompt_template(\n            template=prompt_template,\n            name=\"template\",\n            custom_fields=custom_fields,\n            frontend_node_template=frontend_node_template,\n        )\n        return frontend_node\n\n    def post_code_processing(self, new_frontend_node: dict, current_frontend_node: dict):\n        \"\"\"\n        This function is called after the code validation is done.\n        \"\"\"\n        frontend_node = super().post_code_processing(new_frontend_node, current_frontend_node)\n        template = frontend_node[\"template\"][\"template\"][\"value\"]\n        # Kept it duplicated for backwards compatibility\n        _ = process_prompt_template(\n            template=template,\n            name=\"template\",\n            custom_fields=frontend_node[\"custom_fields\"],\n            frontend_node_template=frontend_node[\"template\"],\n        )\n        # Now that template is updated, we need to grab any values that were set in the current_frontend_node\n        # and update the frontend_node with those values\n        update_template_values(new_template=frontend_node, previous_template=current_frontend_node[\"template\"])\n        return frontend_node\n\n    def _get_fallback_input(self, **kwargs):\n        return DefaultPromptField(**kwargs)\n","fileTypes":[],"file_path":"","password":false,"name":"code","advanced":true,"dynamic":true,"info":"","load_from_db":false,"title_case":false,"display_name":"code","proxy":{"id":"Prompt-1FJ5C","field":"code"}},"template_Prompt-mw3zb":{"trace_as_input":true,"list":false,"required":false,"placeholder":"","show":true,"name":"template","value":"{user_prompt}","display_name":"Template","advanced":true,"dynamic":false,"info":"","title_case":false,"type":"prompt","_input_type":"PromptInput","proxy":{"id":"Prompt-1FJ5C","field":"template"}},"user_prompt_Prompt-mw3zb":{"field_type":"str","required":false,"placeholder":"","list":false,"show":true,"multiline":true,"value":"","fileTypes":[],"file_path":"","password":false,"name":"user_prompt","display_name":"user_prompt","advanced":false,"input_types":["Message","Text"],"dynamic":false,"info":"","load_from_db":false,"title_case":false,"type":"str","proxy":{"id":"Prompt-1FJ5C","field":"user_prompt"}},"code_Prompt-HrBgE":{"type":"code","required":true,"placeholder":"","list":false,"show":true,"multiline":true,"value":"from langflow.base.prompts.api_utils import process_prompt_template\nfrom langflow.custom import Component\nfrom langflow.inputs.inputs import DefaultPromptField\nfrom langflow.io import Output, PromptInput\nfrom langflow.schema.message import Message\nfrom langflow.template.utils import update_template_values\n\n\nclass PromptComponent(Component):\n    display_name: str = \"Prompt\"\n    description: str = \"Create a prompt template with dynamic variables.\"\n    icon = \"prompts\"\n    trace_type = \"prompt\"\n    name = \"Prompt\"\n\n    inputs = [\n        PromptInput(name=\"template\", display_name=\"Template\"),\n    ]\n\n    outputs = [\n        Output(display_name=\"Prompt Message\", name=\"prompt\", method=\"build_prompt\"),\n    ]\n\n    async def build_prompt(\n        self,\n    ) -> Message:\n        prompt = await Message.from_template_and_variables(**self._attributes)\n        self.status = prompt.text\n        return prompt\n\n    def _update_template(self, frontend_node: dict):\n        prompt_template = frontend_node[\"template\"][\"template\"][\"value\"]\n        custom_fields = frontend_node[\"custom_fields\"]\n        frontend_node_template = frontend_node[\"template\"]\n        _ = process_prompt_template(\n            template=prompt_template,\n            name=\"template\",\n            custom_fields=custom_fields,\n            frontend_node_template=frontend_node_template,\n        )\n        return frontend_node\n\n    def post_code_processing(self, new_frontend_node: dict, current_frontend_node: dict):\n        \"\"\"\n        This function is called after the code validation is done.\n        \"\"\"\n        frontend_node = super().post_code_processing(new_frontend_node, current_frontend_node)\n        template = frontend_node[\"template\"][\"template\"][\"value\"]\n        # Kept it duplicated for backwards compatibility\n        _ = process_prompt_template(\n            template=template,\n            name=\"template\",\n            custom_fields=frontend_node[\"custom_fields\"],\n            frontend_node_template=frontend_node[\"template\"],\n        )\n        # Now that template is updated, we need to grab any values that were set in the current_frontend_node\n        # and update the frontend_node with those values\n        update_template_values(new_template=frontend_node, previous_template=current_frontend_node[\"template\"])\n        return frontend_node\n\n    def _get_fallback_input(self, **kwargs):\n        return DefaultPromptField(**kwargs)\n","fileTypes":[],"file_path":"","password":false,"name":"code","advanced":true,"dynamic":true,"info":"","load_from_db":false,"title_case":false,"display_name":"code","proxy":{"id":"Prompt-XM6Zt","field":"code"}},"template_Prompt-HrBgE":{"trace_as_input":true,"list":false,"required":false,"placeholder":"","show":true,"name":"template","value":"{context}","display_name":"Template","advanced":true,"dynamic":false,"info":"","title_case":false,"type":"prompt","_input_type":"PromptInput","proxy":{"id":"Prompt-XM6Zt","field":"template"}},"context_Prompt-HrBgE":{"field_type":"str","required":false,"placeholder":"","list":false,"show":true,"multiline":true,"value":"","fileTypes":[],"file_path":"","password":false,"name":"context","display_name":"context","advanced":false,"input_types":["Message","Text"],"dynamic":false,"info":"","load_from_db":false,"title_case":false,"type":"str","proxy":{"id":"Prompt-XM6Zt","field":"context"}},"code_CreateHeaders-Jw5wk":{"type":"code","required":true,"placeholder":"","list":false,"show":true,"multiline":true,"value":"from langflow.custom import Component\nfrom langflow.inputs import MessageTextInput\nfrom langflow.template import Output\nfrom pydantic.v1 import SecretStr\nfrom langflow.schema import Data\n\n\n\nclass CreateHeadersComponent(Component):\n    display_name = \"Create Headers\"\n    description = \"Creates a headers dictionary with a dynamic Authorization header.\"\n    icon = \"custom_components\"\n    name = \"CreateHeaders\"\n\n    inputs = [\n        MessageTextInput(\n            name=\"input_value\",\n            display_name=\"Authorization Token\",\n            info=\"Enter the token to be used in the Authorization header.\",\n        ),\n    ]\n\n    outputs = [\n        Output(display_name=\"Output\", name=\"output\", method=\"build_output\"),\n    ]\n\n    def build_output(self) -> Data:\n        authorization_header = f\"Bearer {self.input_value}\"\n        headers = {\n            \"Authorization\": authorization_header,\n            \"Content-Type\": \"application/json\",\n        }\n\n        data = Data(value=headers)\n        self.status = data\n        return headers\n        \n        ","fileTypes":[],"file_path":"","password":false,"name":"code","advanced":true,"dynamic":true,"info":"","load_from_db":false,"title_case":false,"display_name":"code","proxy":{"id":"CreateHeaders-MKjxJ","field":"code"}},"input_value_CreateHeaders-Jw5wk":{"trace_as_input":true,"trace_as_metadata":true,"load_from_db":false,"list":false,"required":false,"placeholder":"","show":true,"name":"input_value","value":"","display_name":"Authorization Token","advanced":false,"input_types":["Message"],"dynamic":false,"info":"Enter the token to be used in the Authorization header.","title_case":false,"type":"str","_input_type":"MessageTextInput","proxy":{"id":"CreateHeaders-MKjxJ","field":"input_value"}}},"flow":{"data":{"nodes":[{"id":"APIRequest-kjVCi","type":"genericNode","position":{"x":-160.1020978191558,"y":-152.81819864712116},"data":{"type":"APIRequest","node":{"template":{"_type":"Component","query_params":{"trace_as_input":true,"trace_as_metadata":true,"list":false,"required":false,"placeholder":"","show":true,"value":"","name":"query_params","display_name":"Query Parameters","advanced":false,"input_types":["Data"],"dynamic":false,"info":"The query parameters to append to the URL.","title_case":false,"type":"other","_input_type":"DataInput"},"body":{"trace_as_input":true,"trace_as_metadata":true,"list":false,"required":false,"placeholder":"","show":true,"value":{},"name":"body","display_name":"Body","advanced":false,"input_types":["Data"],"dynamic":false,"info":"The body to send with the request as a dictionary (for POST, PATCH, PUT). This is populated when using the CURL field.","title_case":false,"type":"NestedDict","_input_type":"NestedDictInput"},"code":{"type":"code","required":true,"placeholder":"","list":false,"show":true,"multiline":true,"value":"import asyncio\nimport json\nfrom typing import Any, List, Optional\nfrom urllib.parse import parse_qsl, urlencode, urlparse, urlunparse\n\nimport httpx\nfrom loguru import logger\n\nfrom langflow.base.curl.parse import parse_context\nfrom langflow.custom import Component\nfrom langflow.io import DataInput, DropdownInput, IntInput, MessageTextInput, NestedDictInput, Output\nfrom langflow.schema import Data\nfrom langflow.schema.dotdict import dotdict\n\n\nclass APIRequestComponent(Component):\n    display_name = \"API Request\"\n    description = (\n        \"This component allows you to make HTTP requests to one or more URLs. \"\n        \"You can provide headers and body as either dictionaries or Data objects. \"\n        \"Additionally, you can append query parameters to the URLs.\\n\\n\"\n        \"**Note:** Check advanced options for more settings.\"\n    )\n    icon = \"Globe\"\n    name = \"APIRequest\"\n\n    inputs = [\n        MessageTextInput(\n            name=\"urls\",\n            display_name=\"URLs\",\n            is_list=True,\n            info=\"Enter one or more URLs, separated by commas.\",\n        ),\n        MessageTextInput(\n            name=\"curl\",\n            display_name=\"Curl\",\n            info=\"Paste a curl command to populate the fields. This will fill in the dictionary fields for headers and body.\",\n            advanced=False,\n            refresh_button=True,\n        ),\n        DropdownInput(\n            name=\"method\",\n            display_name=\"Method\",\n            options=[\"GET\", \"POST\", \"PATCH\", \"PUT\"],\n            value=\"GET\",\n            info=\"The HTTP method to use (GET, POST, PATCH, PUT).\",\n        ),\n        NestedDictInput(\n            name=\"headers\",\n            display_name=\"Headers\",\n            info=\"The headers to send with the request as a dictionary. This is populated when using the CURL field.\",\n            input_types=[\"Data\"],\n        ),\n        NestedDictInput(\n            name=\"body\",\n            display_name=\"Body\",\n            info=\"The body to send with the request as a dictionary (for POST, PATCH, PUT). This is populated when using the CURL field.\",\n            input_types=[\"Data\"],\n        ),\n        DataInput(\n            name=\"query_params\",\n            display_name=\"Query Parameters\",\n            info=\"The query parameters to append to the URL.\",\n        ),\n        IntInput(\n            name=\"timeout\",\n            display_name=\"Timeout\",\n            value=5,\n            info=\"The timeout to use for the request.\",\n        ),\n    ]\n\n    outputs = [\n        Output(display_name=\"Data\", name=\"data\", method=\"make_requests\"),\n    ]\n\n    def parse_curl(self, curl: str, build_config: dotdict) -> dotdict:\n        try:\n            parsed = parse_context(curl)\n            build_config[\"urls\"][\"value\"] = [parsed.url]\n            build_config[\"method\"][\"value\"] = parsed.method.upper()\n            build_config[\"headers\"][\"value\"] = dict(parsed.headers)\n\n            if parsed.data:\n                try:\n                    json_data = json.loads(parsed.data)\n                    build_config[\"body\"][\"value\"] = json_data\n                except json.JSONDecodeError as e:\n                    logger.error(f\"Error decoding JSON data: {e}\")\n            else:\n                build_config[\"body\"][\"value\"] = {}\n        except Exception as exc:\n            logger.error(f\"Error parsing curl: {exc}\")\n            raise ValueError(f\"Error parsing curl: {exc}\")\n        return build_config\n\n    def update_build_config(self, build_config: dotdict, field_value: Any, field_name: str | None = None):\n        if field_name == \"curl\" and field_value:\n            build_config = self.parse_curl(field_value, build_config)\n        return build_config\n\n    async def make_request(\n        self,\n        client: httpx.AsyncClient,\n        method: str,\n        url: str,\n        headers: Optional[dict] = None,\n        body: Optional[dict] = None,\n        timeout: int = 5,\n    ) -> Data:\n        method = method.upper()\n        if method not in [\"GET\", \"POST\", \"PATCH\", \"PUT\", \"DELETE\"]:\n            raise ValueError(f\"Unsupported method: {method}\")\n\n        if isinstance(body, str) and body:\n            try:\n                body = json.loads(body)\n            except Exception as e:\n                logger.error(f\"Error decoding JSON data: {e}\")\n                body = None\n                raise ValueError(f\"Error decoding JSON data: {e}\")\n\n        data = body if body else None\n\n        try:\n            response = await client.request(method, url, headers=headers, json=data, timeout=timeout)\n            try:\n                result = response.json()\n            except Exception:\n                result = response.text\n            return Data(\n                data={\n                    \"source\": url,\n                    \"headers\": headers,\n                    \"status_code\": response.status_code,\n                    \"result\": result,\n                },\n            )\n        except httpx.TimeoutException:\n            return Data(\n                data={\n                    \"source\": url,\n                    \"headers\": headers,\n                    \"status_code\": 408,\n                    \"error\": \"Request timed out\",\n                },\n            )\n        except Exception as exc:\n            return Data(\n                data={\n                    \"source\": url,\n                    \"headers\": headers,\n                    \"status_code\": 500,\n                    \"error\": str(exc),\n                },\n            )\n\n    def add_query_params(self, url: str, params: dict) -> str:\n        url_parts = list(urlparse(url))\n        query = dict(parse_qsl(url_parts[4]))\n        query.update(params)\n        url_parts[4] = urlencode(query)\n        return urlunparse(url_parts)\n\n    async def make_requests(self) -> List[Data]:\n        method = self.method\n        urls = [url.strip() for url in self.urls if url.strip()]\n        curl = self.curl\n        headers = self.headers or {}\n        body = self.body or {}\n        timeout = self.timeout\n        query_params = self.query_params.data if self.query_params else {}\n\n        if curl:\n            self._build_config = self.parse_curl(curl, dotdict())\n\n        if isinstance(headers, Data):\n            headers = headers.data\n\n        if isinstance(body, Data):\n            body = body.data\n\n        bodies = [body] * len(urls)\n\n        urls = [self.add_query_params(url, query_params) for url in urls]\n\n        async with httpx.AsyncClient() as client:\n            results = await asyncio.gather(\n                *[self.make_request(client, method, u, headers, rec, timeout) for u, rec in zip(urls, bodies)]\n            )\n        self.status = results\n        return results\n","fileTypes":[],"file_path":"","password":false,"name":"code","advanced":true,"dynamic":true,"info":"","load_from_db":false,"title_case":false,"display_name":"code"},"curl":{"trace_as_input":true,"trace_as_metadata":true,"load_from_db":false,"list":false,"required":false,"placeholder":"","show":true,"value":"","name":"curl","display_name":"Curl","advanced":false,"input_types":["Message"],"dynamic":false,"info":"Paste a curl command to populate the fields. This will fill in the dictionary fields for headers and body.","refresh_button":true,"title_case":false,"type":"str","_input_type":"MessageTextInput"},"headers":{"trace_as_input":true,"trace_as_metadata":true,"list":false,"required":false,"placeholder":"","show":true,"value":{},"name":"headers","display_name":"Headers","advanced":false,"input_types":["Data"],"dynamic":false,"info":"The headers to send with the request as a dictionary. This is populated when using the CURL field.","title_case":false,"type":"NestedDict","_input_type":"NestedDictInput"},"method":{"trace_as_metadata":true,"options":["GET","POST","PATCH","PUT"],"combobox":false,"required":false,"placeholder":"","show":true,"value":"POST","name":"method","display_name":"Method","advanced":false,"dynamic":false,"info":"The HTTP method to use (GET, POST, PATCH, PUT).","title_case":false,"type":"str","_input_type":"DropdownInput"},"timeout":{"trace_as_metadata":true,"list":false,"required":false,"placeholder":"","show":true,"value":"5","name":"timeout","display_name":"Timeout","advanced":false,"dynamic":false,"info":"The timeout to use for the request.","title_case":false,"type":"int","_input_type":"IntInput"},"urls":{"trace_as_input":true,"trace_as_metadata":true,"load_from_db":false,"list":true,"required":false,"placeholder":"","show":true,"value":["https://bigquery.googleapis.com/bigquery/v2/projects/spherical-proxy-424916-c9/queries"],"name":"urls","display_name":"URLs","advanced":false,"input_types":["Message"],"dynamic":false,"info":"Enter one or more URLs, separated by commas.","title_case":false,"type":"str","_input_type":"MessageTextInput"}},"description":"This component allows you to make HTTP requests to one or more URLs. You can provide headers and body as either dictionaries or Data objects. Additionally, you can append query parameters to the URLs.\n\n**Note:** Check advanced options for more settings.","icon":"Globe","base_classes":["Data"],"display_name":"API Request","documentation":"","custom_fields":{},"output_types":[],"pinned":false,"conditional_paths":[],"frozen":false,"outputs":[{"types":["Data"],"selected":"Data","name":"data","display_name":"Data","method":"make_requests","value":"__UNDEFINED__","cache":true,"hidden":false}],"field_order":["urls","curl","method","headers","body","query_params","timeout"],"beta":false,"edited":false,"lf_version":"1.0.17"},"id":"APIRequest-kjVCi"},"selected":true,"width":384,"height":984,"positionAbsolute":{"x":-160.1020978191558,"y":-152.81819864712116},"dragging":false},{"id":"OpenAIModel-4LBlQ","type":"genericNode","position":{"x":-1191.748050434836,"y":79.93314207599741},"data":{"type":"OpenAIModel","node":{"template":{"_type":"Component","api_key":{"load_from_db":false,"required":false,"placeholder":"","show":true,"name":"api_key","value":"","display_name":"OpenAI API Key","advanced":false,"input_types":["Message"],"dynamic":false,"info":"The OpenAI API Key to use for the OpenAI model.","title_case":false,"password":true,"type":"str","_input_type":"SecretStrInput"},"code":{"type":"code","required":true,"placeholder":"","list":false,"show":true,"multiline":true,"value":"import operator\nfrom functools import reduce\n\nfrom langflow.field_typing.range_spec import RangeSpec\nfrom langchain_openai import ChatOpenAI\nfrom pydantic.v1 import SecretStr\n\nfrom langflow.base.models.model import LCModelComponent\nfrom langflow.base.models.openai_constants import OPENAI_MODEL_NAMES\nfrom langflow.field_typing import LanguageModel\nfrom langflow.inputs import (\n    BoolInput,\n    DictInput,\n    DropdownInput,\n    FloatInput,\n    IntInput,\n    SecretStrInput,\n    StrInput,\n)\n\n\nclass OpenAIModelComponent(LCModelComponent):\n    display_name = \"OpenAI\"\n    description = \"Generates text using OpenAI LLMs.\"\n    icon = \"OpenAI\"\n    name = \"OpenAIModel\"\n\n    inputs = LCModelComponent._base_inputs + [\n        IntInput(\n            name=\"max_tokens\",\n            display_name=\"Max Tokens\",\n            advanced=True,\n            info=\"The maximum number of tokens to generate. Set to 0 for unlimited tokens.\",\n            range_spec=RangeSpec(min=0, max=128000),\n        ),\n        DictInput(name=\"model_kwargs\", display_name=\"Model Kwargs\", advanced=True),\n        BoolInput(\n            name=\"json_mode\",\n            display_name=\"JSON Mode\",\n            advanced=True,\n            info=\"If True, it will output JSON regardless of passing a schema.\",\n        ),\n        DictInput(\n            name=\"output_schema\",\n            is_list=True,\n            display_name=\"Schema\",\n            advanced=True,\n            info=\"The schema for the Output of the model. You must pass the word JSON in the prompt. If left blank, JSON mode will be disabled.\",\n        ),\n        DropdownInput(\n            name=\"model_name\",\n            display_name=\"Model Name\",\n            advanced=False,\n            options=OPENAI_MODEL_NAMES,\n            value=OPENAI_MODEL_NAMES[0],\n        ),\n        StrInput(\n            name=\"openai_api_base\",\n            display_name=\"OpenAI API Base\",\n            advanced=True,\n            info=\"The base URL of the OpenAI API. Defaults to https://api.openai.com/v1. You can change this to use other APIs like JinaChat, LocalAI and Prem.\",\n        ),\n        SecretStrInput(\n            name=\"api_key\",\n            display_name=\"OpenAI API Key\",\n            info=\"The OpenAI API Key to use for the OpenAI model.\",\n            advanced=False,\n            value=\"OPENAI_API_KEY\",\n        ),\n        FloatInput(name=\"temperature\", display_name=\"Temperature\", value=0.1),\n        IntInput(\n            name=\"seed\",\n            display_name=\"Seed\",\n            info=\"The seed controls the reproducibility of the job.\",\n            advanced=True,\n            value=1,\n        ),\n    ]\n\n    def build_model(self) -> LanguageModel:  # type: ignore[type-var]\n        # self.output_schema is a list of dictionaries\n        # let's convert it to a dictionary\n        output_schema_dict: dict[str, str] = reduce(operator.ior, self.output_schema or {}, {})\n        openai_api_key = self.api_key\n        temperature = self.temperature\n        model_name: str = self.model_name\n        max_tokens = self.max_tokens\n        model_kwargs = self.model_kwargs or {}\n        openai_api_base = self.openai_api_base or \"https://api.openai.com/v1\"\n        json_mode = bool(output_schema_dict) or self.json_mode\n        seed = self.seed\n\n        if openai_api_key:\n            api_key = SecretStr(openai_api_key)\n        else:\n            api_key = None\n        output = ChatOpenAI(\n            max_tokens=max_tokens or None,\n            model_kwargs=model_kwargs,\n            model=model_name,\n            base_url=openai_api_base,\n            api_key=api_key,\n            temperature=temperature if temperature is not None else 0.1,\n            seed=seed,\n        )\n        if json_mode:\n            if output_schema_dict:\n                output = output.with_structured_output(schema=output_schema_dict, method=\"json_mode\")  # type: ignore\n            else:\n                output = output.bind(response_format={\"type\": \"json_object\"})  # type: ignore\n\n        return output  # type: ignore\n\n    def _get_exception_message(self, e: Exception):\n        \"\"\"\n        Get a message from an OpenAI exception.\n\n        Args:\n            exception (Exception): The exception to get the message from.\n\n        Returns:\n            str: The message from the exception.\n        \"\"\"\n\n        try:\n            from openai import BadRequestError\n        except ImportError:\n            return\n        if isinstance(e, BadRequestError):\n            message = e.body.get(\"message\")  # type: ignore\n            if message:\n                return message\n        return\n","fileTypes":[],"file_path":"","password":false,"name":"code","advanced":true,"dynamic":true,"info":"","load_from_db":false,"title_case":false,"display_name":"code"},"input_value":{"trace_as_input":true,"trace_as_metadata":true,"load_from_db":false,"list":false,"required":false,"placeholder":"","show":true,"name":"input_value","value":"","display_name":"Input","advanced":false,"input_types":["Message"],"dynamic":false,"info":"","title_case":false,"type":"str","_input_type":"MessageInput"},"json_mode":{"trace_as_metadata":true,"list":false,"required":false,"placeholder":"","show":true,"name":"json_mode","value":false,"display_name":"JSON Mode","advanced":true,"dynamic":false,"info":"If True, it will output JSON regardless of passing a schema.","title_case":false,"type":"bool","_input_type":"BoolInput"},"max_tokens":{"trace_as_metadata":true,"range_spec":{"step_type":"float","min":0,"max":128000,"step":0.1},"list":false,"required":false,"placeholder":"","show":true,"name":"max_tokens","value":"","display_name":"Max Tokens","advanced":true,"dynamic":false,"info":"The maximum number of tokens to generate. Set to 0 for unlimited tokens.","title_case":false,"type":"int","_input_type":"IntInput"},"model_kwargs":{"trace_as_input":true,"list":false,"required":false,"placeholder":"","show":true,"name":"model_kwargs","value":{},"display_name":"Model Kwargs","advanced":true,"dynamic":false,"info":"","title_case":false,"type":"dict","_input_type":"DictInput"},"model_name":{"trace_as_metadata":true,"options":["gpt-4o-mini","gpt-4o","gpt-4-turbo","gpt-4-turbo-preview","gpt-4","gpt-3.5-turbo","gpt-3.5-turbo-0125"],"combobox":false,"required":false,"placeholder":"","show":true,"name":"model_name","value":"gpt-4o","display_name":"Model Name","advanced":false,"dynamic":false,"info":"","title_case":false,"type":"str","_input_type":"DropdownInput"},"openai_api_base":{"trace_as_metadata":true,"load_from_db":false,"list":false,"required":false,"placeholder":"","show":true,"name":"openai_api_base","value":"","display_name":"OpenAI API Base","advanced":true,"dynamic":false,"info":"The base URL of the OpenAI API. Defaults to https://api.openai.com/v1. You can change this to use other APIs like JinaChat, LocalAI and Prem.","title_case":false,"type":"str","_input_type":"StrInput"},"output_schema":{"trace_as_input":true,"list":true,"required":false,"placeholder":"","show":true,"name":"output_schema","value":{},"display_name":"Schema","advanced":true,"dynamic":false,"info":"The schema for the Output of the model. You must pass the word JSON in the prompt. If left blank, JSON mode will be disabled.","title_case":false,"type":"dict","_input_type":"DictInput"},"seed":{"trace_as_metadata":true,"list":false,"required":false,"placeholder":"","show":true,"name":"seed","value":1,"display_name":"Seed","advanced":true,"dynamic":false,"info":"The seed controls the reproducibility of the job.","title_case":false,"type":"int","_input_type":"IntInput"},"stream":{"trace_as_metadata":true,"list":false,"required":false,"placeholder":"","show":true,"name":"stream","value":false,"display_name":"Stream","advanced":true,"dynamic":false,"info":"Stream the response from the model. Streaming works only in Chat.","title_case":false,"type":"bool","_input_type":"BoolInput"},"system_message":{"trace_as_input":true,"trace_as_metadata":true,"load_from_db":false,"list":false,"required":false,"placeholder":"","show":true,"name":"system_message","value":"","display_name":"System Message","advanced":true,"input_types":["Message"],"dynamic":false,"info":"System message to pass to the model.","title_case":false,"type":"str","_input_type":"MessageTextInput"},"temperature":{"trace_as_metadata":true,"list":false,"required":false,"placeholder":"","show":true,"name":"temperature","value":0.1,"display_name":"Temperature","advanced":false,"dynamic":false,"info":"","title_case":false,"type":"float","_input_type":"FloatInput"}},"description":"Generates text using OpenAI LLMs.","icon":"OpenAI","base_classes":["LanguageModel","Message"],"display_name":"OpenAI","documentation":"","custom_fields":{},"output_types":[],"pinned":false,"conditional_paths":[],"frozen":false,"outputs":[{"types":["Message"],"selected":"Message","name":"text_output","display_name":"Text","method":"text_response","value":"__UNDEFINED__","cache":true},{"types":["LanguageModel"],"selected":"LanguageModel","name":"model_output","display_name":"Language Model","method":"build_model","value":"__UNDEFINED__","cache":true}],"field_order":["input_value","system_message","stream","max_tokens","model_kwargs","json_mode","output_schema","model_name","openai_api_base","api_key","temperature","seed"],"beta":false,"edited":false,"lf_version":"1.0.17"},"id":"OpenAIModel-4LBlQ","description":"Generates text using OpenAI LLMs.","display_name":"OpenAI"},"selected":true,"width":384,"height":601,"positionAbsolute":{"x":-1191.748050434836,"y":79.93314207599741},"dragging":false},{"id":"Prompt-R3rtu","type":"genericNode","position":{"x":-1652.4360176582995,"y":115.66517557500373},"data":{"type":"Prompt","node":{"template":{"_type":"Component","code":{"type":"code","required":true,"placeholder":"","list":false,"show":true,"multiline":true,"value":"from langflow.base.prompts.api_utils import process_prompt_template\nfrom langflow.custom import Component\nfrom langflow.inputs.inputs import DefaultPromptField\nfrom langflow.io import Output, PromptInput\nfrom langflow.schema.message import Message\nfrom langflow.template.utils import update_template_values\n\n\nclass PromptComponent(Component):\n    display_name: str = \"Prompt\"\n    description: str = \"Create a prompt template with dynamic variables.\"\n    icon = \"prompts\"\n    trace_type = \"prompt\"\n    name = \"Prompt\"\n\n    inputs = [\n        PromptInput(name=\"template\", display_name=\"Template\"),\n    ]\n\n    outputs = [\n        Output(display_name=\"Prompt Message\", name=\"prompt\", method=\"build_prompt\"),\n    ]\n\n    async def build_prompt(\n        self,\n    ) -> Message:\n        prompt = await Message.from_template_and_variables(**self._attributes)\n        self.status = prompt.text\n        return prompt\n\n    def _update_template(self, frontend_node: dict):\n        prompt_template = frontend_node[\"template\"][\"template\"][\"value\"]\n        custom_fields = frontend_node[\"custom_fields\"]\n        frontend_node_template = frontend_node[\"template\"]\n        _ = process_prompt_template(\n            template=prompt_template,\n            name=\"template\",\n            custom_fields=custom_fields,\n            frontend_node_template=frontend_node_template,\n        )\n        return frontend_node\n\n    def post_code_processing(self, new_frontend_node: dict, current_frontend_node: dict):\n        \"\"\"\n        This function is called after the code validation is done.\n        \"\"\"\n        frontend_node = super().post_code_processing(new_frontend_node, current_frontend_node)\n        template = frontend_node[\"template\"][\"template\"][\"value\"]\n        # Kept it duplicated for backwards compatibility\n        _ = process_prompt_template(\n            template=template,\n            name=\"template\",\n            custom_fields=frontend_node[\"custom_fields\"],\n            frontend_node_template=frontend_node[\"template\"],\n        )\n        # Now that template is updated, we need to grab any values that were set in the current_frontend_node\n        # and update the frontend_node with those values\n        update_template_values(new_template=frontend_node, previous_template=current_frontend_node[\"template\"])\n        return frontend_node\n\n    def _get_fallback_input(self, **kwargs):\n        return DefaultPromptField(**kwargs)\n","fileTypes":[],"file_path":"","password":false,"name":"code","advanced":true,"dynamic":true,"info":"","load_from_db":false,"title_case":false,"display_name":"code"},"template":{"trace_as_input":true,"list":false,"required":false,"placeholder":"","show":true,"name":"template","value":"**Context** :\n{context}\n**User Query** :\n{user_prompt}\n\n---\n\n**Task** : Create a GA4 SQL query to the `bigquery-public-data.ga4_obfuscated_sample_ecommerce.events_20201209` table. Use the schema information provided below to ensure the query is valid. \n1. **Field Validation** :\n  - Only reference fields that exist in the schema provided below.\n \n  - Use the correct dot notation when accessing fields within complex data structures like `RECORD` (also known as `STRUCT`). For example, access `ecommerce.purchase_revenue_in_usd` directly if it’s a non-repeated field.\n \n  - Use `UNNEST` only for repeated fields (arrays). Do not use `UNNEST` for fields that are simple `STRUCT` (single-record) types. For example, `event_params` and `user_properties` require `UNNEST`, while `ecommerce` does not.\n \n2. **Query Structure** : \n  - Validate that the query structure is correct and does not produce errors related to data types, field names, or incorrect usage of `UNNEST`.\n \n  - Access fields within `STRUCT` (or `RECORD`) directly using dot notation without `UNNEST` if they are not repeated. For instance, fields within `ecommerce` can be accessed directly.\n\n  - Ensure that all referenced fields are appropriately handled, especially when dealing with nested or repeated fields.\n \n3. **Error Handling** :\n  - If a query cannot be formed due to a missing or invalid field in the schema, return an error message instead of a query.\n\n\n---\n\nSchema Information for `events_20201209` Table** :\n\n```json\ncolumn_name,data_type,is_nullable\nevent_date,STRING,YES\nevent_timestamp,INT64,YES\nevent_name,STRING,YES\nevent_params,\"ARRAY<STRUCT<key STRING, value STRUCT<string_value STRING, int_value INT64, float_value FLOAT64, double_value FLOAT64>>>\",NO\nevent_previous_timestamp,INT64,YES\nevent_value_in_usd,FLOAT64,YES\nevent_bundle_sequence_id,INT64,YES\nevent_server_timestamp_offset,INT64,YES\nuser_id,STRING,YES\nuser_pseudo_id,STRING,YES\nprivacy_info,\"STRUCT<analytics_storage INT64, ads_storage INT64, uses_transient_token STRING>\",YES\nuser_properties,\"ARRAY<STRUCT<key INT64, value STRUCT<string_value INT64, int_value INT64, float_value INT64, double_value INT64, set_timestamp_micros INT64>>>\",NO\nuser_first_touch_timestamp,INT64,YES\nuser_ltv,\"STRUCT<revenue FLOAT64, currency STRING>\",YES\ndevice,\"STRUCT<category STRING, mobile_brand_name STRING, mobile_model_name STRING, mobile_marketing_name STRING, mobile_os_hardware_model INT64, operating_system STRING, operating_system_version STRING, vendor_id INT64, advertising_id INT64, language STRING, is_limited_ad_tracking STRING, time_zone_offset_seconds INT64, web_info STRUCT<browser STRING, browser_version STRING>>\",YES\ngeo,\"STRUCT<continent STRING, sub_continent STRING, country STRING, region STRING, city STRING, metro STRING>\",YES\napp_info,\"STRUCT<id STRING, version STRING, install_store STRING, firebase_app_id STRING, install_source STRING>\",YES\ntraffic_source,\"STRUCT<medium STRING, name STRING, source STRING>\",YES\nstream_id,INT64,YES\nplatform,STRING,YES\nevent_dimensions,STRUCT<hostname STRING>,YES\necommerce,\"STRUCT<total_item_quantity INT64, purchase_revenue_in_usd FLOAT64, purchase_revenue FLOAT64, refund_value_in_usd FLOAT64, refund_value FLOAT64, shipping_value_in_usd FLOAT64, shipping_value FLOAT64, tax_value_in_usd FLOAT64, tax_value FLOAT64, unique_items INT64, transaction_id STRING>\",YES\nitems,\"ARRAY<STRUCT<item_id STRING, item_name STRING, item_brand STRING, item_variant STRING, item_category STRING, item_category2 STRING, item_category3 STRING, item_category4 STRING, item_category5 STRING, price_in_usd FLOAT64, price FLOAT64, quantity INT64, item_revenue_in_usd FLOAT64, item_revenue FLOAT64, item_refund_in_usd FLOAT64, item_refund FLOAT64, coupon STRING, affiliation STRING, location_id STRING, item_list_id STRING, item_list_name STRING, item_list_index STRING, promotion_id STRING, promotion_name STRING, creative_name STRING, creative_slot STRING>>\",NO\n```\n\n\n---\n\n**Output Requirement** : Your output must be **only**  the SQL query like the example provided:\n\n```sql\nSELECT * FROM `bigquery-public-data.ga4_obfuscated_sample_ecommerce.events_20201209` LIMIT 0\n```\n\nIf you cannot generate a valid query due to missing or invalid fields, return an error message instead. Ensure that you are running a unique query that was not ran before in the context.\n\n\n---\n\n**AI** :","display_name":"Template","advanced":false,"dynamic":false,"info":"","title_case":false,"type":"prompt","_input_type":"PromptInput","load_from_db":false},"context":{"field_type":"str","required":false,"placeholder":"","list":false,"show":true,"multiline":true,"value":"","fileTypes":[],"file_path":"","password":false,"name":"context","display_name":"context","advanced":false,"input_types":["Message","Text"],"dynamic":false,"info":"","load_from_db":false,"title_case":false,"type":"str"},"user_prompt":{"field_type":"str","required":false,"placeholder":"","list":false,"show":true,"multiline":true,"value":"","fileTypes":[],"file_path":"","password":false,"name":"user_prompt","display_name":"user_prompt","advanced":false,"input_types":["Message","Text"],"dynamic":false,"info":"","load_from_db":false,"title_case":false,"type":"str"}},"description":"Create a prompt template with dynamic variables.","icon":"prompts","is_input":null,"is_output":null,"is_composition":null,"base_classes":["Message"],"name":"","display_name":"Prompt","documentation":"","custom_fields":{"template":["context","user_prompt"]},"output_types":[],"full_path":null,"pinned":false,"conditional_paths":[],"frozen":false,"outputs":[{"types":["Message"],"selected":"Message","name":"prompt","hidden":null,"display_name":"Prompt Message","method":"build_prompt","value":"__UNDEFINED__","cache":true}],"field_order":["template"],"beta":false,"error":null,"edited":false},"id":"Prompt-R3rtu","description":"Create a prompt template with dynamic variables.","display_name":"Prompt"},"selected":true,"width":384,"height":498,"positionAbsolute":{"x":-1652.4360176582995,"y":115.66517557500373},"dragging":false},{"id":"CustomComponent-omGVe","type":"genericNode","position":{"x":-722.2114864428826,"y":795.4262181316096},"data":{"type":"CustomComponent","node":{"template":{"_type":"Component","code":{"type":"code","required":true,"placeholder":"","list":false,"show":true,"multiline":true,"value":"# from langflow.field_typing import Data\nfrom langflow.custom import Component\nfrom langflow.io import MessageTextInput, Output\nfrom langflow.schema import Data\nimport re  # Import regex module\n\nclass CustomComponent(Component):\n    display_name = \"Format Body\"\n    description = \"Turn OpenAI SQL output into a body to send with a fetch.\"\n    documentation: str = \"http://docs.langflow.org/components/custom\"\n    icon = \"custom_components\"\n    name = \"CustomComponent\"\n\n    inputs = [\n        MessageTextInput(name=\"input_value\", display_name=\"Input Value\", value=\"```sql\\nbigquery-public-data.ga4_obfuscated_sample_ecommerce.events_*\\n```\"),\n    ]\n\n    outputs = [\n        Output(display_name=\"Output\", name=\"output\", method=\"build_output\"),\n    ]\n\n    def build_output(self) -> Data:\n        # Remove the surrounding markdown and any trailing newline\n        input_value_cleaned = re.sub(r\"^```sql\\n|```$\", \"\", self.input_value.strip()).strip()\n\n        # Format the cleaned SQL string into the desired object\n        formatted_data = {\n            \"query\": input_value_cleaned,\n            \"maxResults\": 20,\n            \"timeoutMs\": 10000,\n            \"useLegacySql\": False\n        }\n        \n        # Return the formatted data as the output\n        data = Data(value=formatted_data)\n        self.status = data\n        \n        # Print the formatted data to the chat (for debugging purposes)\n        print(f\"Output Data: {formatted_data}\")\n        \n        return formatted_data\n","fileTypes":[],"file_path":"","password":false,"name":"code","advanced":true,"dynamic":true,"info":"","load_from_db":false,"title_case":false,"display_name":"code"},"input_value":{"trace_as_input":true,"trace_as_metadata":true,"load_from_db":false,"list":false,"required":false,"placeholder":"","show":true,"name":"input_value","value":"","display_name":"Input Value","advanced":false,"input_types":["Message"],"dynamic":false,"info":"","title_case":false,"type":"str","_input_type":"MessageTextInput"}},"description":"Turn OpenAI SQL output into a body to send with a fetch.","icon":"custom_components","base_classes":["Data"],"display_name":"Custom Component","documentation":"http://docs.langflow.org/components/custom","custom_fields":{},"output_types":[],"pinned":false,"conditional_paths":[],"frozen":false,"outputs":[{"types":["Data"],"selected":"Data","name":"output","display_name":"Output","method":"build_output","value":"__UNDEFINED__","cache":true}],"field_order":["input_value"],"beta":false,"edited":true,"lf_version":"1.0.17"},"id":"CustomComponent-omGVe"},"selected":true,"width":384,"height":326,"dragging":false,"positionAbsolute":{"x":-722.2114864428826,"y":795.4262181316096}},{"id":"FetchProcessorComponent-HwwIa","type":"genericNode","position":{"x":373.18673165075575,"y":648.0200366696015},"data":{"type":"FetchProcessorComponent","node":{"template":{"_type":"Component","fetch_response":{"trace_as_metadata":true,"list":false,"required":false,"placeholder":"","show":true,"name":"fetch_response","value":"","display_name":"Fetch Response","advanced":false,"input_types":["Data"],"dynamic":false,"info":"","title_case":false,"type":"other","_input_type":"HandleInput"},"code":{"type":"code","required":true,"placeholder":"","list":false,"show":true,"multiline":true,"value":"from langflow.custom import Component\nfrom langflow.io import MessageTextInput, Output\nfrom langflow.schema import Data\nfrom langflow.schema.message import Message\nimport json\n\n\nclass FetchProcessorComponent(Component):\n    display_name = \"Fetch Processor\"\n    description = \"Process fetch output, print it, and stringify it for chat.\"\n    documentation: str = \"http://docs.langflow.org/components/custom\"\n    icon = \"custom_components\"\n    name = \"FetchProcessorComponent\"\n\n    inputs = [\n        HandleInput(\n            name=\"fetch_response\",\n            display_name=\"Fetch Response\",\n            input_types=[\"Data\"],\n        )\n    ]\n\n    outputs = [\n        Output(display_name=\"Processed Output\", name=\"processed_output\", method=\"process_output\"),\n    ]\n    \n    def process_output(self) -> Message:\n        try:\n            # Directly use the fetch_response as the input data\n            response_data = self.fetch_response\n\n            # Extract the response content\n            content = str(response_data[0].data[\"result\"])\n            \n            # Truncate the content to a specific length (e.g., 5000 characters)\n            max_length = 5000\n            truncated_content = content[:max_length]\n            \n            # Convert the truncated content to a string that can be printed\n            output_str = \"Processed Output: \" + str(truncated_content)+\"...\"\n            \n            \n            # Return the processed string as a Message\n            self.status = output_str\n            return Message(text=output_str)\n\n        except Exception as e:\n            # Handle any errors and print them\n            error_message = f\"Error processing fetch response: {str(e)}\"\n            print(error_message)\n            return Message(text=error_message)\n","fileTypes":[],"file_path":"","password":false,"name":"code","advanced":true,"dynamic":true,"info":"","load_from_db":false,"title_case":false,"display_name":"code"}},"description":"Process fetch output, print it, and stringify it for chat.","icon":"custom_components","base_classes":["Message"],"display_name":"Custom Component","documentation":"http://docs.langflow.org/components/custom","custom_fields":{},"output_types":[],"pinned":false,"conditional_paths":[],"frozen":false,"outputs":[{"types":["Message"],"selected":"Message","name":"processed_output","display_name":"Processed Output","method":"process_output","value":"__UNDEFINED__","cache":true}],"field_order":["fetch_response"],"beta":false,"edited":true,"lf_version":"1.0.17"},"id":"FetchProcessorComponent-HwwIa"},"selected":true,"width":384,"height":288,"dragging":false,"positionAbsolute":{"x":373.18673165075575,"y":648.0200366696015}},{"id":"OpenAIModel-7WJ6a","type":"genericNode","position":{"x":1420.9686803289817,"y":501.3038614984411},"data":{"type":"OpenAIModel","node":{"template":{"_type":"Component","api_key":{"load_from_db":false,"required":false,"placeholder":"","show":true,"value":"","name":"api_key","display_name":"OpenAI API Key","advanced":false,"input_types":["Message"],"dynamic":false,"info":"The OpenAI API Key to use for the OpenAI model.","title_case":false,"password":true,"type":"str","_input_type":"SecretStrInput"},"code":{"type":"code","required":true,"placeholder":"","list":false,"show":true,"multiline":true,"value":"import operator\nfrom functools import reduce\n\nfrom langflow.field_typing.range_spec import RangeSpec\nfrom langchain_openai import ChatOpenAI\nfrom pydantic.v1 import SecretStr\n\nfrom langflow.base.models.model import LCModelComponent\nfrom langflow.base.models.openai_constants import OPENAI_MODEL_NAMES\nfrom langflow.field_typing import LanguageModel\nfrom langflow.inputs import (\n    BoolInput,\n    DictInput,\n    DropdownInput,\n    FloatInput,\n    IntInput,\n    SecretStrInput,\n    StrInput,\n)\n\n\nclass OpenAIModelComponent(LCModelComponent):\n    display_name = \"OpenAI\"\n    description = \"Generates text using OpenAI LLMs.\"\n    icon = \"OpenAI\"\n    name = \"OpenAIModel\"\n\n    inputs = LCModelComponent._base_inputs + [\n        IntInput(\n            name=\"max_tokens\",\n            display_name=\"Max Tokens\",\n            advanced=True,\n            info=\"The maximum number of tokens to generate. Set to 0 for unlimited tokens.\",\n            range_spec=RangeSpec(min=0, max=128000),\n        ),\n        DictInput(name=\"model_kwargs\", display_name=\"Model Kwargs\", advanced=True),\n        BoolInput(\n            name=\"json_mode\",\n            display_name=\"JSON Mode\",\n            advanced=True,\n            info=\"If True, it will output JSON regardless of passing a schema.\",\n        ),\n        DictInput(\n            name=\"output_schema\",\n            is_list=True,\n            display_name=\"Schema\",\n            advanced=True,\n            info=\"The schema for the Output of the model. You must pass the word JSON in the prompt. If left blank, JSON mode will be disabled.\",\n        ),\n        DropdownInput(\n            name=\"model_name\",\n            display_name=\"Model Name\",\n            advanced=False,\n            options=OPENAI_MODEL_NAMES,\n            value=OPENAI_MODEL_NAMES[0],\n        ),\n        StrInput(\n            name=\"openai_api_base\",\n            display_name=\"OpenAI API Base\",\n            advanced=True,\n            info=\"The base URL of the OpenAI API. Defaults to https://api.openai.com/v1. You can change this to use other APIs like JinaChat, LocalAI and Prem.\",\n        ),\n        SecretStrInput(\n            name=\"api_key\",\n            display_name=\"OpenAI API Key\",\n            info=\"The OpenAI API Key to use for the OpenAI model.\",\n            advanced=False,\n            value=\"OPENAI_API_KEY\",\n        ),\n        FloatInput(name=\"temperature\", display_name=\"Temperature\", value=0.1),\n        IntInput(\n            name=\"seed\",\n            display_name=\"Seed\",\n            info=\"The seed controls the reproducibility of the job.\",\n            advanced=True,\n            value=1,\n        ),\n    ]\n\n    def build_model(self) -> LanguageModel:  # type: ignore[type-var]\n        # self.output_schema is a list of dictionaries\n        # let's convert it to a dictionary\n        output_schema_dict: dict[str, str] = reduce(operator.ior, self.output_schema or {}, {})\n        openai_api_key = self.api_key\n        temperature = self.temperature\n        model_name: str = self.model_name\n        max_tokens = self.max_tokens\n        model_kwargs = self.model_kwargs or {}\n        openai_api_base = self.openai_api_base or \"https://api.openai.com/v1\"\n        json_mode = bool(output_schema_dict) or self.json_mode\n        seed = self.seed\n\n        if openai_api_key:\n            api_key = SecretStr(openai_api_key)\n        else:\n            api_key = None\n        output = ChatOpenAI(\n            max_tokens=max_tokens or None,\n            model_kwargs=model_kwargs,\n            model=model_name,\n            base_url=openai_api_base,\n            api_key=api_key,\n            temperature=temperature or 0.1,\n            seed=seed,\n        )\n        if json_mode:\n            if output_schema_dict:\n                output = output.with_structured_output(schema=output_schema_dict, method=\"json_mode\")  # type: ignore\n            else:\n                output = output.bind(response_format={\"type\": \"json_object\"})  # type: ignore\n\n        return output  # type: ignore\n\n    def _get_exception_message(self, e: Exception):\n        \"\"\"\n        Get a message from an OpenAI exception.\n\n        Args:\n            exception (Exception): The exception to get the message from.\n\n        Returns:\n            str: The message from the exception.\n        \"\"\"\n\n        try:\n            from openai import BadRequestError\n        except ImportError:\n            return\n        if isinstance(e, BadRequestError):\n            message = e.body.get(\"message\")  # type: ignore\n            if message:\n                return message\n        return\n","fileTypes":[],"file_path":"","password":false,"name":"code","advanced":true,"dynamic":true,"info":"","load_from_db":false,"title_case":false,"display_name":"code"},"input_value":{"trace_as_input":true,"trace_as_metadata":true,"load_from_db":false,"list":false,"required":false,"placeholder":"","show":true,"value":"","name":"input_value","display_name":"Input","advanced":false,"input_types":["Message"],"dynamic":false,"info":"","title_case":false,"type":"str","_input_type":"MessageInput"},"json_mode":{"trace_as_metadata":true,"list":false,"required":false,"placeholder":"","show":true,"value":false,"name":"json_mode","display_name":"JSON Mode","advanced":true,"dynamic":false,"info":"If True, it will output JSON regardless of passing a schema.","title_case":false,"type":"bool","_input_type":"BoolInput"},"max_tokens":{"trace_as_metadata":true,"range_spec":{"step_type":"float","min":0,"max":128000,"step":0.1},"list":false,"required":false,"placeholder":"","show":true,"value":"","name":"max_tokens","display_name":"Max Tokens","advanced":true,"dynamic":false,"info":"The maximum number of tokens to generate. Set to 0 for unlimited tokens.","title_case":false,"type":"int","_input_type":"IntInput"},"model_kwargs":{"trace_as_input":true,"list":false,"required":false,"placeholder":"","show":true,"value":{},"name":"model_kwargs","display_name":"Model Kwargs","advanced":true,"dynamic":false,"info":"","title_case":false,"type":"dict","_input_type":"DictInput"},"model_name":{"trace_as_metadata":true,"options":["gpt-4o-mini","gpt-4o","gpt-4-turbo","gpt-4-turbo-preview","gpt-4","gpt-3.5-turbo","gpt-3.5-turbo-0125"],"combobox":false,"required":false,"placeholder":"","show":true,"value":"gpt-4o-mini","name":"model_name","display_name":"Model Name","advanced":false,"dynamic":false,"info":"","title_case":false,"type":"str","_input_type":"DropdownInput"},"openai_api_base":{"trace_as_metadata":true,"load_from_db":false,"list":false,"required":false,"placeholder":"","show":true,"value":"","name":"openai_api_base","display_name":"OpenAI API Base","advanced":true,"dynamic":false,"info":"The base URL of the OpenAI API. Defaults to https://api.openai.com/v1. You can change this to use other APIs like JinaChat, LocalAI and Prem.","title_case":false,"type":"str","_input_type":"StrInput"},"output_schema":{"trace_as_input":true,"list":true,"required":false,"placeholder":"","show":true,"value":{},"name":"output_schema","display_name":"Schema","advanced":true,"dynamic":false,"info":"The schema for the Output of the model. You must pass the word JSON in the prompt. If left blank, JSON mode will be disabled.","title_case":false,"type":"dict","_input_type":"DictInput"},"seed":{"trace_as_metadata":true,"list":false,"required":false,"placeholder":"","show":true,"value":1,"name":"seed","display_name":"Seed","advanced":true,"dynamic":false,"info":"The seed controls the reproducibility of the job.","title_case":false,"type":"int","_input_type":"IntInput"},"stream":{"trace_as_metadata":true,"list":false,"required":false,"placeholder":"","show":true,"value":false,"name":"stream","display_name":"Stream","advanced":true,"dynamic":false,"info":"Stream the response from the model. Streaming works only in Chat.","title_case":false,"type":"bool","_input_type":"BoolInput"},"system_message":{"trace_as_input":true,"trace_as_metadata":true,"load_from_db":false,"list":false,"required":false,"placeholder":"","show":true,"value":"","name":"system_message","display_name":"System Message","advanced":true,"input_types":["Message"],"dynamic":false,"info":"System message to pass to the model.","title_case":false,"type":"str","_input_type":"MessageTextInput"},"temperature":{"trace_as_metadata":true,"list":false,"required":false,"placeholder":"","show":true,"value":0.1,"name":"temperature","display_name":"Temperature","advanced":false,"dynamic":false,"info":"","title_case":false,"type":"float","_input_type":"FloatInput"}},"description":"Generates text using OpenAI LLMs.","icon":"OpenAI","base_classes":["LanguageModel","Message"],"display_name":"OpenAI","documentation":"","custom_fields":{},"output_types":[],"pinned":false,"conditional_paths":[],"frozen":false,"outputs":[{"types":["Message"],"selected":"Message","name":"text_output","display_name":"Text","method":"text_response","value":"__UNDEFINED__","cache":true},{"types":["LanguageModel"],"selected":"LanguageModel","name":"model_output","display_name":"Language Model","method":"build_model","value":"__UNDEFINED__","cache":true}],"field_order":["input_value","system_message","stream","max_tokens","model_kwargs","json_mode","output_schema","model_name","openai_api_base","api_key","temperature","seed"],"beta":false,"edited":false,"lf_version":"1.0.17"},"id":"OpenAIModel-7WJ6a"},"selected":true,"width":384,"height":601,"positionAbsolute":{"x":1420.9686803289817,"y":501.3038614984411},"dragging":false},{"id":"Prompt-6uwLf","type":"genericNode","position":{"x":914.8083947668401,"y":477.7711490769072},"data":{"type":"Prompt","node":{"template":{"_type":"Component","code":{"type":"code","required":true,"placeholder":"","list":false,"show":true,"multiline":true,"value":"from langflow.base.prompts.api_utils import process_prompt_template\nfrom langflow.custom import Component\nfrom langflow.inputs.inputs import DefaultPromptField\nfrom langflow.io import Output, PromptInput\nfrom langflow.schema.message import Message\nfrom langflow.template.utils import update_template_values\n\n\nclass PromptComponent(Component):\n    display_name: str = \"Prompt\"\n    description: str = \"Create a prompt template with dynamic variables.\"\n    icon = \"prompts\"\n    trace_type = \"prompt\"\n    name = \"Prompt\"\n\n    inputs = [\n        PromptInput(name=\"template\", display_name=\"Template\"),\n    ]\n\n    outputs = [\n        Output(display_name=\"Prompt Message\", name=\"prompt\", method=\"build_prompt\"),\n    ]\n\n    async def build_prompt(\n        self,\n    ) -> Message:\n        prompt = await Message.from_template_and_variables(**self._attributes)\n        self.status = prompt.text\n        return prompt\n\n    def _update_template(self, frontend_node: dict):\n        prompt_template = frontend_node[\"template\"][\"template\"][\"value\"]\n        custom_fields = frontend_node[\"custom_fields\"]\n        frontend_node_template = frontend_node[\"template\"]\n        _ = process_prompt_template(\n            template=prompt_template,\n            name=\"template\",\n            custom_fields=custom_fields,\n            frontend_node_template=frontend_node_template,\n        )\n        return frontend_node\n\n    def post_code_processing(self, new_frontend_node: dict, current_frontend_node: dict):\n        \"\"\"\n        This function is called after the code validation is done.\n        \"\"\"\n        frontend_node = super().post_code_processing(new_frontend_node, current_frontend_node)\n        template = frontend_node[\"template\"][\"template\"][\"value\"]\n        # Kept it duplicated for backwards compatibility\n        _ = process_prompt_template(\n            template=template,\n            name=\"template\",\n            custom_fields=frontend_node[\"custom_fields\"],\n            frontend_node_template=frontend_node[\"template\"],\n        )\n        # Now that template is updated, we need to grab any values that were set in the current_frontend_node\n        # and update the frontend_node with those values\n        update_template_values(new_template=frontend_node, previous_template=current_frontend_node[\"template\"])\n        return frontend_node\n\n    def _get_fallback_input(self, **kwargs):\n        return DefaultPromptField(**kwargs)\n","fileTypes":[],"file_path":"","password":false,"name":"code","advanced":true,"dynamic":true,"info":"","load_from_db":false,"title_case":false,"display_name":"code"},"template":{"trace_as_input":true,"list":false,"required":false,"placeholder":"","show":true,"name":"template","value":"Context: {context}\n\nUser: I was given these instructions:\n\n```\n{instructions}\n```\n\n\nI gave this query to run:\n\n```\n{query}\n```\n\nI got this output:\n\n```\n{output}\n```\n\n\nDo the following:\n1. Output the query executed\n2. Output a portion of the results (human readable)\n3. Interpret the results in the context of the instructions\n4. Output the insights that you have learned\n5. Create an action for what you think you should do next using reflection.\n\nIf there was an error, output what the error was\n\nAI:","display_name":"Template","advanced":false,"dynamic":false,"info":"","title_case":false,"type":"prompt","_input_type":"PromptInput","load_from_db":false},"context":{"field_type":"str","required":false,"placeholder":"","list":false,"show":true,"multiline":true,"value":"","fileTypes":[],"file_path":"","password":false,"name":"context","display_name":"context","advanced":false,"input_types":["Message","Text"],"dynamic":false,"info":"","load_from_db":false,"title_case":false,"type":"str"},"instructions":{"field_type":"str","required":false,"placeholder":"","list":false,"show":true,"multiline":true,"value":"","fileTypes":[],"file_path":"","password":false,"name":"instructions","display_name":"instructions","advanced":false,"input_types":["Message","Text"],"dynamic":false,"info":"","load_from_db":false,"title_case":false,"type":"str"},"query":{"field_type":"str","required":false,"placeholder":"","list":false,"show":true,"multiline":true,"value":"","fileTypes":[],"file_path":"","password":false,"name":"query","display_name":"query","advanced":false,"input_types":["Message","Text"],"dynamic":false,"info":"","load_from_db":false,"title_case":false,"type":"str"},"output":{"field_type":"str","required":false,"placeholder":"","list":false,"show":true,"multiline":true,"value":"","fileTypes":[],"file_path":"","password":false,"name":"output","display_name":"output","advanced":false,"input_types":["Message","Text"],"dynamic":false,"info":"","load_from_db":false,"title_case":false,"type":"str"}},"description":"Create a prompt template with dynamic variables.","icon":"prompts","is_input":null,"is_output":null,"is_composition":null,"base_classes":["Message"],"name":"","display_name":"Prompt","documentation":"","custom_fields":{"template":["context","instructions","query","output"]},"output_types":[],"full_path":null,"pinned":false,"conditional_paths":[],"frozen":false,"outputs":[{"types":["Message"],"selected":"Message","name":"prompt","hidden":null,"display_name":"Prompt Message","method":"build_prompt","value":"__UNDEFINED__","cache":true}],"field_order":["template"],"beta":false,"error":null,"edited":false},"id":"Prompt-6uwLf","description":"Create a prompt template with dynamic variables.","display_name":"Prompt"},"selected":true,"width":384,"height":670,"dragging":false,"positionAbsolute":{"x":914.8083947668401,"y":477.7711490769072}},{"id":"Prompt-1FJ5C","type":"genericNode","position":{"x":-2177.128851560763,"y":150.38359629552468},"data":{"type":"Prompt","node":{"template":{"_type":"Component","code":{"type":"code","required":true,"placeholder":"","list":false,"show":true,"multiline":true,"value":"from langflow.base.prompts.api_utils import process_prompt_template\nfrom langflow.custom import Component\nfrom langflow.inputs.inputs import DefaultPromptField\nfrom langflow.io import Output, PromptInput\nfrom langflow.schema.message import Message\nfrom langflow.template.utils import update_template_values\n\n\nclass PromptComponent(Component):\n    display_name: str = \"Prompt\"\n    description: str = \"Create a prompt template with dynamic variables.\"\n    icon = \"prompts\"\n    trace_type = \"prompt\"\n    name = \"Prompt\"\n\n    inputs = [\n        PromptInput(name=\"template\", display_name=\"Template\"),\n    ]\n\n    outputs = [\n        Output(display_name=\"Prompt Message\", name=\"prompt\", method=\"build_prompt\"),\n    ]\n\n    async def build_prompt(\n        self,\n    ) -> Message:\n        prompt = await Message.from_template_and_variables(**self._attributes)\n        self.status = prompt.text\n        return prompt\n\n    def _update_template(self, frontend_node: dict):\n        prompt_template = frontend_node[\"template\"][\"template\"][\"value\"]\n        custom_fields = frontend_node[\"custom_fields\"]\n        frontend_node_template = frontend_node[\"template\"]\n        _ = process_prompt_template(\n            template=prompt_template,\n            name=\"template\",\n            custom_fields=custom_fields,\n            frontend_node_template=frontend_node_template,\n        )\n        return frontend_node\n\n    def post_code_processing(self, new_frontend_node: dict, current_frontend_node: dict):\n        \"\"\"\n        This function is called after the code validation is done.\n        \"\"\"\n        frontend_node = super().post_code_processing(new_frontend_node, current_frontend_node)\n        template = frontend_node[\"template\"][\"template\"][\"value\"]\n        # Kept it duplicated for backwards compatibility\n        _ = process_prompt_template(\n            template=template,\n            name=\"template\",\n            custom_fields=frontend_node[\"custom_fields\"],\n            frontend_node_template=frontend_node[\"template\"],\n        )\n        # Now that template is updated, we need to grab any values that were set in the current_frontend_node\n        # and update the frontend_node with those values\n        update_template_values(new_template=frontend_node, previous_template=current_frontend_node[\"template\"])\n        return frontend_node\n\n    def _get_fallback_input(self, **kwargs):\n        return DefaultPromptField(**kwargs)\n","fileTypes":[],"file_path":"","password":false,"name":"code","advanced":true,"dynamic":true,"info":"","load_from_db":false,"title_case":false,"display_name":"code"},"template":{"trace_as_input":true,"list":false,"required":false,"placeholder":"","show":true,"name":"template","value":"{user_prompt}","display_name":"Template","advanced":false,"dynamic":false,"info":"","title_case":false,"type":"prompt","_input_type":"PromptInput"},"user_prompt":{"field_type":"str","required":false,"placeholder":"","list":false,"show":true,"multiline":true,"value":"","fileTypes":[],"file_path":"","password":false,"name":"user_prompt","display_name":"user_prompt","advanced":false,"input_types":["Message","Text"],"dynamic":false,"info":"","load_from_db":false,"title_case":false,"type":"str"}},"description":"Create a prompt template with dynamic variables.","icon":"prompts","is_input":null,"is_output":null,"is_composition":null,"base_classes":["Message"],"name":"","display_name":"Prompt","documentation":"","custom_fields":{"template":["user_prompt"]},"output_types":[],"full_path":null,"pinned":false,"conditional_paths":[],"frozen":false,"outputs":[{"types":["Message"],"selected":"Message","name":"prompt","hidden":null,"display_name":"Prompt Message","method":"build_prompt","value":"__UNDEFINED__","cache":true}],"field_order":["template"],"beta":false,"error":null,"edited":false,"lf_version":"1.0.17"},"id":"Prompt-1FJ5C"},"selected":true,"width":384,"height":412,"positionAbsolute":{"x":-2177.128851560763,"y":150.38359629552468},"dragging":false},{"id":"Prompt-XM6Zt","type":"genericNode","position":{"x":-2178.68201579404,"y":-325.42016585687594},"data":{"type":"Prompt","node":{"template":{"_type":"Component","code":{"type":"code","required":true,"placeholder":"","list":false,"show":true,"multiline":true,"value":"from langflow.base.prompts.api_utils import process_prompt_template\nfrom langflow.custom import Component\nfrom langflow.inputs.inputs import DefaultPromptField\nfrom langflow.io import Output, PromptInput\nfrom langflow.schema.message import Message\nfrom langflow.template.utils import update_template_values\n\n\nclass PromptComponent(Component):\n    display_name: str = \"Prompt\"\n    description: str = \"Create a prompt template with dynamic variables.\"\n    icon = \"prompts\"\n    trace_type = \"prompt\"\n    name = \"Prompt\"\n\n    inputs = [\n        PromptInput(name=\"template\", display_name=\"Template\"),\n    ]\n\n    outputs = [\n        Output(display_name=\"Prompt Message\", name=\"prompt\", method=\"build_prompt\"),\n    ]\n\n    async def build_prompt(\n        self,\n    ) -> Message:\n        prompt = await Message.from_template_and_variables(**self._attributes)\n        self.status = prompt.text\n        return prompt\n\n    def _update_template(self, frontend_node: dict):\n        prompt_template = frontend_node[\"template\"][\"template\"][\"value\"]\n        custom_fields = frontend_node[\"custom_fields\"]\n        frontend_node_template = frontend_node[\"template\"]\n        _ = process_prompt_template(\n            template=prompt_template,\n            name=\"template\",\n            custom_fields=custom_fields,\n            frontend_node_template=frontend_node_template,\n        )\n        return frontend_node\n\n    def post_code_processing(self, new_frontend_node: dict, current_frontend_node: dict):\n        \"\"\"\n        This function is called after the code validation is done.\n        \"\"\"\n        frontend_node = super().post_code_processing(new_frontend_node, current_frontend_node)\n        template = frontend_node[\"template\"][\"template\"][\"value\"]\n        # Kept it duplicated for backwards compatibility\n        _ = process_prompt_template(\n            template=template,\n            name=\"template\",\n            custom_fields=frontend_node[\"custom_fields\"],\n            frontend_node_template=frontend_node[\"template\"],\n        )\n        # Now that template is updated, we need to grab any values that were set in the current_frontend_node\n        # and update the frontend_node with those values\n        update_template_values(new_template=frontend_node, previous_template=current_frontend_node[\"template\"])\n        return frontend_node\n\n    def _get_fallback_input(self, **kwargs):\n        return DefaultPromptField(**kwargs)\n","fileTypes":[],"file_path":"","password":false,"name":"code","advanced":true,"dynamic":true,"info":"","load_from_db":false,"title_case":false,"display_name":"code"},"template":{"trace_as_input":true,"list":false,"required":false,"placeholder":"","show":true,"name":"template","value":"{context}","display_name":"Template","advanced":false,"dynamic":false,"info":"","title_case":false,"type":"prompt","_input_type":"PromptInput"},"context":{"field_type":"str","required":false,"placeholder":"","list":false,"show":true,"multiline":true,"value":"","fileTypes":[],"file_path":"","password":false,"name":"context","display_name":"context","advanced":false,"input_types":["Message","Text"],"dynamic":false,"info":"","load_from_db":false,"title_case":false,"type":"str"}},"description":"Create a prompt template with dynamic variables.","icon":"prompts","is_input":null,"is_output":null,"is_composition":null,"base_classes":["Message"],"name":"","display_name":"Prompt","documentation":"","custom_fields":{"template":["context"]},"output_types":[],"full_path":null,"pinned":false,"conditional_paths":[],"frozen":false,"outputs":[{"types":["Message"],"selected":"Message","name":"prompt","hidden":null,"display_name":"Prompt Message","method":"build_prompt","value":"__UNDEFINED__","cache":true}],"field_order":["template"],"beta":false,"error":null,"edited":false,"lf_version":"1.0.17"},"id":"Prompt-XM6Zt"},"selected":true,"width":384,"height":412,"positionAbsolute":{"x":-2178.68201579404,"y":-325.42016585687594},"dragging":false},{"id":"CreateHeaders-MKjxJ","type":"genericNode","position":{"x":-657.0341125223471,"y":-128.46552887731997},"data":{"type":"CreateHeaders","node":{"template":{"_type":"Component","code":{"type":"code","required":true,"placeholder":"","list":false,"show":true,"multiline":true,"value":"from langflow.custom import Component\nfrom langflow.inputs import MessageTextInput\nfrom langflow.template import Output\nfrom pydantic.v1 import SecretStr\nfrom langflow.schema import Data\n\n\n\nclass CreateHeadersComponent(Component):\n    display_name = \"Create Headers\"\n    description = \"Creates a headers dictionary with a dynamic Authorization header.\"\n    icon = \"custom_components\"\n    name = \"CreateHeaders\"\n\n    inputs = [\n        MessageTextInput(\n            name=\"input_value\",\n            display_name=\"Authorization Token\",\n            info=\"Enter the token to be used in the Authorization header.\",\n        ),\n    ]\n\n    outputs = [\n        Output(display_name=\"Output\", name=\"output\", method=\"build_output\"),\n    ]\n\n    def build_output(self) -> Data:\n        authorization_header = f\"Bearer {self.input_value}\"\n        headers = {\n            \"Authorization\": authorization_header,\n            \"Content-Type\": \"application/json\",\n        }\n\n        data = Data(value=headers)\n        self.status = data\n        return headers\n        \n        ","fileTypes":[],"file_path":"","password":false,"name":"code","advanced":true,"dynamic":true,"info":"","load_from_db":false,"title_case":false,"display_name":"code"},"input_value":{"trace_as_input":true,"trace_as_metadata":true,"load_from_db":false,"list":false,"required":false,"placeholder":"","show":true,"name":"input_value","value":"","display_name":"Authorization Token","advanced":false,"input_types":["Message"],"dynamic":false,"info":"Enter the token to be used in the Authorization header.","title_case":false,"type":"str","_input_type":"MessageTextInput"}},"description":"Creates a headers dictionary with a dynamic Authorization header.","icon":"custom_components","base_classes":["Data"],"display_name":"Create List","documentation":"","custom_fields":{},"output_types":[],"pinned":false,"conditional_paths":[],"frozen":false,"outputs":[{"types":["Data"],"selected":"Data","name":"output","display_name":"Output","method":"build_output","value":"__UNDEFINED__","cache":true}],"field_order":["input_value"],"beta":false,"edited":true,"lf_version":"1.0.17"},"id":"CreateHeaders-MKjxJ"},"selected":true,"width":384,"height":326,"positionAbsolute":{"x":-657.0341125223471,"y":-128.46552887731997},"dragging":false}],"edges":[{"source":"OpenAIModel-4LBlQ","sourceHandle":"{œdataTypeœ:œOpenAIModelœ,œidœ:œOpenAIModel-4LBlQœ,œnameœ:œtext_outputœ,œoutput_typesœ:[œMessageœ]}","target":"CustomComponent-omGVe","targetHandle":"{œfieldNameœ:œinput_valueœ,œidœ:œCustomComponent-omGVeœ,œinputTypesœ:[œMessageœ],œtypeœ:œstrœ}","data":{"targetHandle":{"fieldName":"input_value","id":"CustomComponent-omGVe","inputTypes":["Message"],"type":"str"},"sourceHandle":{"dataType":"OpenAIModel","id":"OpenAIModel-4LBlQ","name":"text_output","output_types":["Message"]}},"id":"reactflow__edge-OpenAIModel-4LBlQ{œdataTypeœ:œOpenAIModelœ,œidœ:œOpenAIModel-4LBlQœ,œnameœ:œtext_outputœ,œoutput_typesœ:[œMessageœ]}-CustomComponent-omGVe{œfieldNameœ:œinput_valueœ,œidœ:œCustomComponent-omGVeœ,œinputTypesœ:[œMessageœ],œtypeœ:œstrœ}","className":"","selected":true},{"source":"APIRequest-kjVCi","sourceHandle":"{œdataTypeœ:œAPIRequestœ,œidœ:œAPIRequest-kjVCiœ,œnameœ:œdataœ,œoutput_typesœ:[œDataœ]}","target":"FetchProcessorComponent-HwwIa","targetHandle":"{œfieldNameœ:œfetch_responseœ,œidœ:œFetchProcessorComponent-HwwIaœ,œinputTypesœ:[œDataœ],œtypeœ:œotherœ}","data":{"targetHandle":{"fieldName":"fetch_response","id":"FetchProcessorComponent-HwwIa","inputTypes":["Data"],"type":"other"},"sourceHandle":{"dataType":"APIRequest","id":"APIRequest-kjVCi","name":"data","output_types":["Data"]}},"id":"reactflow__edge-APIRequest-kjVCi{œdataTypeœ:œAPIRequestœ,œidœ:œAPIRequest-kjVCiœ,œnameœ:œdataœ,œoutput_typesœ:[œDataœ]}-FetchProcessorComponent-HwwIa{œfieldNameœ:œfetch_responseœ,œidœ:œFetchProcessorComponent-HwwIaœ,œinputTypesœ:[œDataœ],œtypeœ:œotherœ}","className":"","selected":true},{"source":"FetchProcessorComponent-HwwIa","sourceHandle":"{œdataTypeœ:œFetchProcessorComponentœ,œidœ:œFetchProcessorComponent-HwwIaœ,œnameœ:œprocessed_outputœ,œoutput_typesœ:[œMessageœ]}","target":"Prompt-6uwLf","targetHandle":"{œfieldNameœ:œoutputœ,œidœ:œPrompt-6uwLfœ,œinputTypesœ:[œMessageœ,œTextœ],œtypeœ:œstrœ}","data":{"targetHandle":{"fieldName":"output","id":"Prompt-6uwLf","inputTypes":["Message","Text"],"type":"str"},"sourceHandle":{"dataType":"FetchProcessorComponent","id":"FetchProcessorComponent-HwwIa","name":"processed_output","output_types":["Message"]}},"id":"reactflow__edge-FetchProcessorComponent-HwwIa{œdataTypeœ:œFetchProcessorComponentœ,œidœ:œFetchProcessorComponent-HwwIaœ,œnameœ:œprocessed_outputœ,œoutput_typesœ:[œMessageœ]}-Prompt-6uwLf{œfieldNameœ:œoutputœ,œidœ:œPrompt-6uwLfœ,œinputTypesœ:[œMessageœ,œTextœ],œtypeœ:œstrœ}","className":"","selected":true},{"source":"Prompt-6uwLf","sourceHandle":"{œdataTypeœ:œPromptœ,œidœ:œPrompt-6uwLfœ,œnameœ:œpromptœ,œoutput_typesœ:[œMessageœ]}","target":"OpenAIModel-7WJ6a","targetHandle":"{œfieldNameœ:œinput_valueœ,œidœ:œOpenAIModel-7WJ6aœ,œinputTypesœ:[œMessageœ],œtypeœ:œstrœ}","data":{"targetHandle":{"fieldName":"input_value","id":"OpenAIModel-7WJ6a","inputTypes":["Message"],"type":"str"},"sourceHandle":{"dataType":"Prompt","id":"Prompt-6uwLf","name":"prompt","output_types":["Message"]}},"id":"reactflow__edge-Prompt-6uwLf{œdataTypeœ:œPromptœ,œidœ:œPrompt-6uwLfœ,œnameœ:œpromptœ,œoutput_typesœ:[œMessageœ]}-OpenAIModel-7WJ6a{œfieldNameœ:œinput_valueœ,œidœ:œOpenAIModel-7WJ6aœ,œinputTypesœ:[œMessageœ],œtypeœ:œstrœ}","className":"","selected":true},{"source":"CustomComponent-omGVe","sourceHandle":"{œdataTypeœ:œCustomComponentœ,œidœ:œCustomComponent-omGVeœ,œnameœ:œoutputœ,œoutput_typesœ:[œDataœ]}","target":"APIRequest-kjVCi","targetHandle":"{œfieldNameœ:œbodyœ,œidœ:œAPIRequest-kjVCiœ,œinputTypesœ:[œDataœ],œtypeœ:œNestedDictœ}","data":{"targetHandle":{"fieldName":"body","id":"APIRequest-kjVCi","inputTypes":["Data"],"type":"NestedDict"},"sourceHandle":{"dataType":"CustomComponent","id":"CustomComponent-omGVe","name":"output","output_types":["Data"]}},"id":"reactflow__edge-CustomComponent-omGVe{œdataTypeœ:œCustomComponentœ,œidœ:œCustomComponent-omGVeœ,œnameœ:œoutputœ,œoutput_typesœ:[œDataœ]}-APIRequest-kjVCi{œfieldNameœ:œbodyœ,œidœ:œAPIRequest-kjVCiœ,œinputTypesœ:[œDataœ],œtypeœ:œNestedDictœ}","className":"","selected":true},{"source":"OpenAIModel-4LBlQ","sourceHandle":"{œdataTypeœ:œOpenAIModelœ,œidœ:œOpenAIModel-4LBlQœ,œnameœ:œtext_outputœ,œoutput_typesœ:[œMessageœ]}","target":"Prompt-6uwLf","targetHandle":"{œfieldNameœ:œqueryœ,œidœ:œPrompt-6uwLfœ,œinputTypesœ:[œMessageœ,œTextœ],œtypeœ:œstrœ}","data":{"targetHandle":{"fieldName":"query","id":"Prompt-6uwLf","inputTypes":["Message","Text"],"type":"str"},"sourceHandle":{"dataType":"OpenAIModel","id":"OpenAIModel-4LBlQ","name":"text_output","output_types":["Message"]}},"id":"reactflow__edge-OpenAIModel-4LBlQ{œdataTypeœ:œOpenAIModelœ,œidœ:œOpenAIModel-4LBlQœ,œnameœ:œtext_outputœ,œoutput_typesœ:[œMessageœ]}-Prompt-6uwLf{œfieldNameœ:œqueryœ,œidœ:œPrompt-6uwLfœ,œinputTypesœ:[œMessageœ,œTextœ],œtypeœ:œstrœ}","className":"","selected":true},{"source":"Prompt-R3rtu","sourceHandle":"{œdataTypeœ:œPromptœ,œidœ:œPrompt-R3rtuœ,œnameœ:œpromptœ,œoutput_typesœ:[œMessageœ]}","target":"OpenAIModel-4LBlQ","targetHandle":"{œfieldNameœ:œinput_valueœ,œidœ:œOpenAIModel-4LBlQœ,œinputTypesœ:[œMessageœ],œtypeœ:œstrœ}","data":{"targetHandle":{"fieldName":"input_value","id":"OpenAIModel-4LBlQ","inputTypes":["Message"],"type":"str"},"sourceHandle":{"dataType":"Prompt","id":"Prompt-R3rtu","name":"prompt","output_types":["Message"]}},"id":"reactflow__edge-Prompt-R3rtu{œdataTypeœ:œPromptœ,œidœ:œPrompt-R3rtuœ,œnameœ:œpromptœ,œoutput_typesœ:[œMessageœ]}-OpenAIModel-4LBlQ{œfieldNameœ:œinput_valueœ,œidœ:œOpenAIModel-4LBlQœ,œinputTypesœ:[œMessageœ],œtypeœ:œstrœ}","selected":true,"className":""},{"source":"Prompt-1FJ5C","sourceHandle":"{œdataTypeœ:œPromptœ,œidœ:œPrompt-1FJ5Cœ,œnameœ:œpromptœ,œoutput_typesœ:[œMessageœ]}","target":"Prompt-R3rtu","targetHandle":"{œfieldNameœ:œuser_promptœ,œidœ:œPrompt-R3rtuœ,œinputTypesœ:[œMessageœ,œTextœ],œtypeœ:œstrœ}","data":{"targetHandle":{"fieldName":"user_prompt","id":"Prompt-R3rtu","inputTypes":["Message","Text"],"type":"str"},"sourceHandle":{"dataType":"Prompt","id":"Prompt-1FJ5C","name":"prompt","output_types":["Message"]}},"id":"reactflow__edge-Prompt-1FJ5C{œdataTypeœ:œPromptœ,œidœ:œPrompt-1FJ5Cœ,œnameœ:œpromptœ,œoutput_typesœ:[œMessageœ]}-Prompt-R3rtu{œfieldNameœ:œuser_promptœ,œidœ:œPrompt-R3rtuœ,œinputTypesœ:[œMessageœ,œTextœ],œtypeœ:œstrœ}","selected":true},{"source":"Prompt-1FJ5C","sourceHandle":"{œdataTypeœ:œPromptœ,œidœ:œPrompt-1FJ5Cœ,œnameœ:œpromptœ,œoutput_typesœ:[œMessageœ]}","target":"Prompt-6uwLf","targetHandle":"{œfieldNameœ:œinstructionsœ,œidœ:œPrompt-6uwLfœ,œinputTypesœ:[œMessageœ,œTextœ],œtypeœ:œstrœ}","data":{"targetHandle":{"fieldName":"instructions","id":"Prompt-6uwLf","inputTypes":["Message","Text"],"type":"str"},"sourceHandle":{"dataType":"Prompt","id":"Prompt-1FJ5C","name":"prompt","output_types":["Message"]}},"id":"reactflow__edge-Prompt-1FJ5C{œdataTypeœ:œPromptœ,œidœ:œPrompt-1FJ5Cœ,œnameœ:œpromptœ,œoutput_typesœ:[œMessageœ]}-Prompt-6uwLf{œfieldNameœ:œinstructionsœ,œidœ:œPrompt-6uwLfœ,œinputTypesœ:[œMessageœ,œTextœ],œtypeœ:œstrœ}","selected":true},{"source":"Prompt-XM6Zt","sourceHandle":"{œdataTypeœ:œPromptœ,œidœ:œPrompt-XM6Ztœ,œnameœ:œpromptœ,œoutput_typesœ:[œMessageœ]}","target":"Prompt-6uwLf","targetHandle":"{œfieldNameœ:œcontextœ,œidœ:œPrompt-6uwLfœ,œinputTypesœ:[œMessageœ,œTextœ],œtypeœ:œstrœ}","data":{"targetHandle":{"fieldName":"context","id":"Prompt-6uwLf","inputTypes":["Message","Text"],"type":"str"},"sourceHandle":{"dataType":"Prompt","id":"Prompt-XM6Zt","name":"prompt","output_types":["Message"]}},"id":"reactflow__edge-Prompt-XM6Zt{œdataTypeœ:œPromptœ,œidœ:œPrompt-XM6Ztœ,œnameœ:œpromptœ,œoutput_typesœ:[œMessageœ]}-Prompt-6uwLf{œfieldNameœ:œcontextœ,œidœ:œPrompt-6uwLfœ,œinputTypesœ:[œMessageœ,œTextœ],œtypeœ:œstrœ}","selected":true},{"source":"Prompt-XM6Zt","sourceHandle":"{œdataTypeœ:œPromptœ,œidœ:œPrompt-XM6Ztœ,œnameœ:œpromptœ,œoutput_typesœ:[œMessageœ]}","target":"Prompt-R3rtu","targetHandle":"{œfieldNameœ:œcontextœ,œidœ:œPrompt-R3rtuœ,œinputTypesœ:[œMessageœ,œTextœ],œtypeœ:œstrœ}","data":{"targetHandle":{"fieldName":"context","id":"Prompt-R3rtu","inputTypes":["Message","Text"],"type":"str"},"sourceHandle":{"dataType":"Prompt","id":"Prompt-XM6Zt","name":"prompt","output_types":["Message"]}},"id":"reactflow__edge-Prompt-XM6Zt{œdataTypeœ:œPromptœ,œidœ:œPrompt-XM6Ztœ,œnameœ:œpromptœ,œoutput_typesœ:[œMessageœ]}-Prompt-R3rtu{œfieldNameœ:œcontextœ,œidœ:œPrompt-R3rtuœ,œinputTypesœ:[œMessageœ,œTextœ],œtypeœ:œstrœ}","selected":true},{"source":"CreateHeaders-MKjxJ","sourceHandle":"{œdataTypeœ:œCreateHeadersœ,œidœ:œCreateHeaders-MKjxJœ,œnameœ:œoutputœ,œoutput_typesœ:[œDataœ]}","target":"APIRequest-kjVCi","targetHandle":"{œfieldNameœ:œheadersœ,œidœ:œAPIRequest-kjVCiœ,œinputTypesœ:[œDataœ],œtypeœ:œNestedDictœ}","data":{"targetHandle":{"fieldName":"headers","id":"APIRequest-kjVCi","inputTypes":["Data"],"type":"NestedDict"},"sourceHandle":{"dataType":"CreateHeaders","id":"CreateHeaders-MKjxJ","name":"output","output_types":["Data"]}},"id":"reactflow__edge-CreateHeaders-MKjxJ{œdataTypeœ:œCreateHeadersœ,œidœ:œCreateHeaders-MKjxJœ,œnameœ:œoutputœ,œoutput_typesœ:[œDataœ]}-APIRequest-kjVCi{œfieldNameœ:œheadersœ,œidœ:œAPIRequest-kjVCiœ,œinputTypesœ:[œDataœ],œtypeœ:œNestedDictœ}","selected":true}],"viewport":{"zoom":1,"x":0,"y":0}},"is_component":false,"name":"Dazzling Volhard","description":"","id":"A8EHc"},"outputs":[{"types":["LanguageModel"],"selected":"LanguageModel","name":"OpenAIModel-OHg1W_model_output","display_name":"Language Model","method":"build_model","value":"__UNDEFINED__","cache":true,"proxy":{"id":"OpenAIModel-4LBlQ","name":"model_output","nodeDisplayName":"OpenAI"}},{"types":["Message"],"selected":"Message","name":"OpenAIModel-BxXKu_text_output","display_name":"Text","method":"text_response","value":"__UNDEFINED__","cache":true,"proxy":{"id":"OpenAIModel-7WJ6a","name":"text_output","nodeDisplayName":"OpenAI"}},{"types":["LanguageModel"],"selected":"LanguageModel","name":"OpenAIModel-BxXKu_model_output","display_name":"Language Model","method":"build_model","value":"__UNDEFINED__","cache":true,"proxy":{"id":"OpenAIModel-7WJ6a","name":"model_output","nodeDisplayName":"OpenAI"}}]}},"selected":false,"width":384,"height":944,"positionAbsolute":{"x":-452.7851817216633,"y":1327.9770738829372},"dragging":false}],"edges":[{"source":"ChatInput-ZoJNe","sourceHandle":"{œdataTypeœ:œChatInputœ,œidœ:œChatInput-ZoJNeœ,œnameœ:œmessageœ,œoutput_typesœ:[œMessageœ]}","target":"groupComponent-mAH3x","targetHandle":"{œfieldNameœ:œuser_prompt_Prompt-mw3zbœ,œidœ:œgroupComponent-mAH3xœ,œinputTypesœ:[œMessageœ,œTextœ],œproxyœ:{œfieldœ:œuser_promptœ,œidœ:œPrompt-mw3zbœ},œtypeœ:œstrœ}","data":{"targetHandle":{"fieldName":"user_prompt_Prompt-mw3zb","id":"groupComponent-mAH3x","inputTypes":["Message","Text"],"proxy":{"field":"user_prompt","id":"Prompt-mw3zb"},"type":"str"},"sourceHandle":{"dataType":"ChatInput","id":"ChatInput-ZoJNe","name":"message","output_types":["Message"]}},"id":"reactflow__edge-ChatInput-ZoJNe{œdataTypeœ:œChatInputœ,œidœ:œChatInput-ZoJNeœ,œnameœ:œmessageœ,œoutput_typesœ:[œMessageœ]}-groupComponent-mAH3x{œfieldNameœ:œuser_prompt_Prompt-mw3zbœ,œidœ:œgroupComponent-mAH3xœ,œinputTypesœ:[œMessageœ,œTextœ],œproxyœ:{œfieldœ:œuser_promptœ,œidœ:œPrompt-mw3zbœ},œtypeœ:œstrœ}","className":""},{"source":"groupComponent-mAH3x","sourceHandle":"{œdataTypeœ:œGroupNodeœ,œidœ:œgroupComponent-mAH3xœ,œnameœ:œOpenAIModel-BxXKu_text_outputœ,œoutput_typesœ:[œMessageœ]}","target":"ChatOutput-BzHJd","targetHandle":"{œfieldNameœ:œinput_valueœ,œidœ:œChatOutput-BzHJdœ,œinputTypesœ:[œMessageœ],œtypeœ:œstrœ}","data":{"targetHandle":{"fieldName":"input_value","id":"ChatOutput-BzHJd","inputTypes":["Message"],"type":"str"},"sourceHandle":{"dataType":"GroupNode","id":"groupComponent-mAH3x","name":"OpenAIModel-BxXKu_text_output","output_types":["Message"]}},"id":"reactflow__edge-groupComponent-mAH3x{œdataTypeœ:œGroupNodeœ,œidœ:œgroupComponent-mAH3xœ,œnameœ:œOpenAIModel-BxXKu_text_outputœ,œoutput_typesœ:[œMessageœ]}-ChatOutput-BzHJd{œfieldNameœ:œinput_valueœ,œidœ:œChatOutput-BzHJdœ,œinputTypesœ:[œMessageœ],œtypeœ:œstrœ}","className":""},{"source":"TextInput-hAmSp","sourceHandle":"{œdataTypeœ:œTextInputœ,œidœ:œTextInput-hAmSpœ,œnameœ:œtextœ,œoutput_typesœ:[œMessageœ]}","target":"groupComponent-mAH3x","targetHandle":"{œfieldNameœ:œapi_key_OpenAIModel-BxXKuœ,œidœ:œgroupComponent-mAH3xœ,œinputTypesœ:[œMessageœ],œproxyœ:{œfieldœ:œapi_keyœ,œidœ:œOpenAIModel-BxXKuœ},œtypeœ:œstrœ}","data":{"targetHandle":{"fieldName":"api_key_OpenAIModel-BxXKu","id":"groupComponent-mAH3x","inputTypes":["Message"],"proxy":{"field":"api_key","id":"OpenAIModel-BxXKu"},"type":"str"},"sourceHandle":{"dataType":"TextInput","id":"TextInput-hAmSp","name":"text","output_types":["Message"]}},"id":"reactflow__edge-TextInput-hAmSp{œdataTypeœ:œTextInputœ,œidœ:œTextInput-hAmSpœ,œnameœ:œtextœ,œoutput_typesœ:[œMessageœ]}-groupComponent-mAH3x{œfieldNameœ:œapi_key_OpenAIModel-BxXKuœ,œidœ:œgroupComponent-mAH3xœ,œinputTypesœ:[œMessageœ],œproxyœ:{œfieldœ:œapi_keyœ,œidœ:œOpenAIModel-BxXKuœ},œtypeœ:œstrœ}","className":""},{"source":"TextInput-hAmSp","sourceHandle":"{œdataTypeœ:œTextInputœ,œidœ:œTextInput-hAmSpœ,œnameœ:œtextœ,œoutput_typesœ:[œMessageœ]}","target":"groupComponent-mAH3x","targetHandle":"{œfieldNameœ:œapi_key_OpenAIModel-OHg1Wœ,œidœ:œgroupComponent-mAH3xœ,œinputTypesœ:[œMessageœ],œproxyœ:{œfieldœ:œapi_keyœ,œidœ:œOpenAIModel-OHg1Wœ},œtypeœ:œstrœ}","data":{"targetHandle":{"fieldName":"api_key_OpenAIModel-OHg1W","id":"groupComponent-mAH3x","inputTypes":["Message"],"proxy":{"field":"api_key","id":"OpenAIModel-OHg1W"},"type":"str"},"sourceHandle":{"dataType":"TextInput","id":"TextInput-hAmSp","name":"text","output_types":["Message"]}},"id":"reactflow__edge-TextInput-hAmSp{œdataTypeœ:œTextInputœ,œidœ:œTextInput-hAmSpœ,œnameœ:œtextœ,œoutput_typesœ:[œMessageœ]}-groupComponent-mAH3x{œfieldNameœ:œapi_key_OpenAIModel-OHg1Wœ,œidœ:œgroupComponent-mAH3xœ,œinputTypesœ:[œMessageœ],œproxyœ:{œfieldœ:œapi_keyœ,œidœ:œOpenAIModel-OHg1Wœ},œtypeœ:œstrœ}","className":""},{"source":"TextInput-MSS8x","sourceHandle":"{œdataTypeœ:œTextInputœ,œidœ:œTextInput-MSS8xœ,œnameœ:œtextœ,œoutput_typesœ:[œMessageœ]}","target":"groupComponent-mAH3x","targetHandle":"{œfieldNameœ:œinput_value_CreateHeaders-Jw5wkœ,œidœ:œgroupComponent-mAH3xœ,œinputTypesœ:[œMessageœ],œproxyœ:{œfieldœ:œinput_valueœ,œidœ:œCreateHeaders-Jw5wkœ},œtypeœ:œstrœ}","data":{"targetHandle":{"fieldName":"input_value_CreateHeaders-Jw5wk","id":"groupComponent-mAH3x","inputTypes":["Message"],"proxy":{"field":"input_value","id":"CreateHeaders-Jw5wk"},"type":"str"},"sourceHandle":{"dataType":"TextInput","id":"TextInput-MSS8x","name":"text","output_types":["Message"]}},"id":"reactflow__edge-TextInput-MSS8x{œdataTypeœ:œTextInputœ,œidœ:œTextInput-MSS8xœ,œnameœ:œtextœ,œoutput_typesœ:[œMessageœ]}-groupComponent-mAH3x{œfieldNameœ:œinput_value_CreateHeaders-Jw5wkœ,œidœ:œgroupComponent-mAH3xœ,œinputTypesœ:[œMessageœ],œproxyœ:{œfieldœ:œinput_valueœ,œidœ:œCreateHeaders-Jw5wkœ},œtypeœ:œstrœ}","className":""},{"source":"TextInput-hAmSp","sourceHandle":"{œdataTypeœ:œTextInputœ,œidœ:œTextInput-hAmSpœ,œnameœ:œtextœ,œoutput_typesœ:[œMessageœ]}","target":"GroupNode-zl0DH","targetHandle":"{œfieldNameœ:œapi_key_OpenAIModel-BxXKuœ,œidœ:œGroupNode-zl0DHœ,œinputTypesœ:[œMessageœ],œproxyœ:{œfieldœ:œapi_keyœ,œidœ:œOpenAIModel-fJtPgœ},œtypeœ:œstrœ}","data":{"targetHandle":{"fieldName":"api_key_OpenAIModel-BxXKu","id":"GroupNode-zl0DH","inputTypes":["Message"],"proxy":{"field":"api_key","id":"OpenAIModel-fJtPg"},"type":"str"},"sourceHandle":{"dataType":"TextInput","id":"TextInput-hAmSp","name":"text","output_types":["Message"]}},"id":"reactflow__edge-TextInput-hAmSp{œdataTypeœ:œTextInputœ,œidœ:œTextInput-hAmSpœ,œnameœ:œtextœ,œoutput_typesœ:[œMessageœ]}-GroupNode-zl0DH{œfieldNameœ:œapi_key_OpenAIModel-BxXKuœ,œidœ:œGroupNode-zl0DHœ,œinputTypesœ:[œMessageœ],œproxyœ:{œfieldœ:œapi_keyœ,œidœ:œOpenAIModel-fJtPgœ},œtypeœ:œstrœ}","className":""},{"source":"TextInput-hAmSp","sourceHandle":"{œdataTypeœ:œTextInputœ,œidœ:œTextInput-hAmSpœ,œnameœ:œtextœ,œoutput_typesœ:[œMessageœ]}","target":"GroupNode-zl0DH","targetHandle":"{œfieldNameœ:œapi_key_OpenAIModel-OHg1Wœ,œidœ:œGroupNode-zl0DHœ,œinputTypesœ:[œMessageœ],œproxyœ:{œfieldœ:œapi_keyœ,œidœ:œOpenAIModel-joKQiœ},œtypeœ:œstrœ}","data":{"targetHandle":{"fieldName":"api_key_OpenAIModel-OHg1W","id":"GroupNode-zl0DH","inputTypes":["Message"],"proxy":{"field":"api_key","id":"OpenAIModel-joKQi"},"type":"str"},"sourceHandle":{"dataType":"TextInput","id":"TextInput-hAmSp","name":"text","output_types":["Message"]}},"id":"reactflow__edge-TextInput-hAmSp{œdataTypeœ:œTextInputœ,œidœ:œTextInput-hAmSpœ,œnameœ:œtextœ,œoutput_typesœ:[œMessageœ]}-GroupNode-zl0DH{œfieldNameœ:œapi_key_OpenAIModel-OHg1Wœ,œidœ:œGroupNode-zl0DHœ,œinputTypesœ:[œMessageœ],œproxyœ:{œfieldœ:œapi_keyœ,œidœ:œOpenAIModel-joKQiœ},œtypeœ:œstrœ}","className":""},{"source":"ChatInput-ZoJNe","sourceHandle":"{œdataTypeœ:œChatInputœ,œidœ:œChatInput-ZoJNeœ,œnameœ:œmessageœ,œoutput_typesœ:[œMessageœ]}","target":"GroupNode-zl0DH","targetHandle":"{œfieldNameœ:œuser_prompt_Prompt-mw3zbœ,œidœ:œGroupNode-zl0DHœ,œinputTypesœ:[œMessageœ,œTextœ],œproxyœ:{œfieldœ:œuser_promptœ,œidœ:œPrompt-1lyudœ},œtypeœ:œstrœ}","data":{"targetHandle":{"fieldName":"user_prompt_Prompt-mw3zb","id":"GroupNode-zl0DH","inputTypes":["Message","Text"],"proxy":{"field":"user_prompt","id":"Prompt-1lyud"},"type":"str"},"sourceHandle":{"dataType":"ChatInput","id":"ChatInput-ZoJNe","name":"message","output_types":["Message"]}},"id":"reactflow__edge-ChatInput-ZoJNe{œdataTypeœ:œChatInputœ,œidœ:œChatInput-ZoJNeœ,œnameœ:œmessageœ,œoutput_typesœ:[œMessageœ]}-GroupNode-zl0DH{œfieldNameœ:œuser_prompt_Prompt-mw3zbœ,œidœ:œGroupNode-zl0DHœ,œinputTypesœ:[œMessageœ,œTextœ],œproxyœ:{œfieldœ:œuser_promptœ,œidœ:œPrompt-1lyudœ},œtypeœ:œstrœ}","className":""},{"source":"GroupNode-zl0DH","sourceHandle":"{œdataTypeœ:œGroupNodeœ,œidœ:œGroupNode-zl0DHœ,œnameœ:œOpenAIModel-BxXKu_text_outputœ,œoutput_typesœ:[œMessageœ]}","target":"ChatOutput-KIwTd","targetHandle":"{œfieldNameœ:œinput_valueœ,œidœ:œChatOutput-KIwTdœ,œinputTypesœ:[œMessageœ],œtypeœ:œstrœ}","data":{"targetHandle":{"fieldName":"input_value","id":"ChatOutput-KIwTd","inputTypes":["Message"],"type":"str"},"sourceHandle":{"dataType":"GroupNode","id":"GroupNode-zl0DH","name":"OpenAIModel-BxXKu_text_output","output_types":["Message"]}},"id":"reactflow__edge-GroupNode-zl0DH{œdataTypeœ:œGroupNodeœ,œidœ:œGroupNode-zl0DHœ,œnameœ:œOpenAIModel-BxXKu_text_outputœ,œoutput_typesœ:[œMessageœ]}-ChatOutput-KIwTd{œfieldNameœ:œinput_valueœ,œidœ:œChatOutput-KIwTdœ,œinputTypesœ:[œMessageœ],œtypeœ:œstrœ}","className":""},{"source":"groupComponent-mAH3x","sourceHandle":"{œdataTypeœ:œGroupNodeœ,œidœ:œgroupComponent-mAH3xœ,œnameœ:œOpenAIModel-BxXKu_text_outputœ,œoutput_typesœ:[œMessageœ]}","target":"GroupNode-zl0DH","targetHandle":"{œfieldNameœ:œcontext_Prompt-HrBgEœ,œidœ:œGroupNode-zl0DHœ,œinputTypesœ:[œMessageœ,œTextœ],œproxyœ:{œfieldœ:œcontextœ,œidœ:œPrompt-CcCEoœ},œtypeœ:œstrœ}","data":{"targetHandle":{"fieldName":"context_Prompt-HrBgE","id":"GroupNode-zl0DH","inputTypes":["Message","Text"],"proxy":{"field":"context","id":"Prompt-CcCEo"},"type":"str"},"sourceHandle":{"dataType":"GroupNode","id":"groupComponent-mAH3x","name":"OpenAIModel-BxXKu_text_output","output_types":["Message"]}},"id":"reactflow__edge-groupComponent-mAH3x{œdataTypeœ:œGroupNodeœ,œidœ:œgroupComponent-mAH3xœ,œnameœ:œOpenAIModel-BxXKu_text_outputœ,œoutput_typesœ:[œMessageœ]}-GroupNode-zl0DH{œfieldNameœ:œcontext_Prompt-HrBgEœ,œidœ:œGroupNode-zl0DHœ,œinputTypesœ:[œMessageœ,œTextœ],œproxyœ:{œfieldœ:œcontextœ,œidœ:œPrompt-CcCEoœ},œtypeœ:œstrœ}","className":""},{"source":"TextInput-hAmSp","sourceHandle":"{œdataTypeœ:œTextInputœ,œidœ:œTextInput-hAmSpœ,œnameœ:œtextœ,œoutput_typesœ:[œMessageœ]}","target":"GroupNode-OjoFe","targetHandle":"{œfieldNameœ:œapi_key_OpenAIModel-BxXKuœ,œidœ:œGroupNode-OjoFeœ,œinputTypesœ:[œMessageœ],œproxyœ:{œfieldœ:œapi_keyœ,œidœ:œOpenAIModel-7WJ6aœ},œtypeœ:œstrœ}","data":{"targetHandle":{"fieldName":"api_key_OpenAIModel-BxXKu","id":"GroupNode-OjoFe","inputTypes":["Message"],"proxy":{"field":"api_key","id":"OpenAIModel-7WJ6a"},"type":"str"},"sourceHandle":{"dataType":"TextInput","id":"TextInput-hAmSp","name":"text","output_types":["Message"]}},"id":"reactflow__edge-TextInput-hAmSp{œdataTypeœ:œTextInputœ,œidœ:œTextInput-hAmSpœ,œnameœ:œtextœ,œoutput_typesœ:[œMessageœ]}-GroupNode-OjoFe{œfieldNameœ:œapi_key_OpenAIModel-BxXKuœ,œidœ:œGroupNode-OjoFeœ,œinputTypesœ:[œMessageœ],œproxyœ:{œfieldœ:œapi_keyœ,œidœ:œOpenAIModel-7WJ6aœ},œtypeœ:œstrœ}"},{"source":"TextInput-hAmSp","sourceHandle":"{œdataTypeœ:œTextInputœ,œidœ:œTextInput-hAmSpœ,œnameœ:œtextœ,œoutput_typesœ:[œMessageœ]}","target":"GroupNode-OjoFe","targetHandle":"{œfieldNameœ:œapi_key_OpenAIModel-OHg1Wœ,œidœ:œGroupNode-OjoFeœ,œinputTypesœ:[œMessageœ],œproxyœ:{œfieldœ:œapi_keyœ,œidœ:œOpenAIModel-4LBlQœ},œtypeœ:œstrœ}","data":{"targetHandle":{"fieldName":"api_key_OpenAIModel-OHg1W","id":"GroupNode-OjoFe","inputTypes":["Message"],"proxy":{"field":"api_key","id":"OpenAIModel-4LBlQ"},"type":"str"},"sourceHandle":{"dataType":"TextInput","id":"TextInput-hAmSp","name":"text","output_types":["Message"]}},"id":"reactflow__edge-TextInput-hAmSp{œdataTypeœ:œTextInputœ,œidœ:œTextInput-hAmSpœ,œnameœ:œtextœ,œoutput_typesœ:[œMessageœ]}-GroupNode-OjoFe{œfieldNameœ:œapi_key_OpenAIModel-OHg1Wœ,œidœ:œGroupNode-OjoFeœ,œinputTypesœ:[œMessageœ],œproxyœ:{œfieldœ:œapi_keyœ,œidœ:œOpenAIModel-4LBlQœ},œtypeœ:œstrœ}"},{"source":"TextInput-MSS8x","sourceHandle":"{œdataTypeœ:œTextInputœ,œidœ:œTextInput-MSS8xœ,œnameœ:œtextœ,œoutput_typesœ:[œMessageœ]}","target":"GroupNode-OjoFe","targetHandle":"{œfieldNameœ:œinput_value_CreateHeaders-Jw5wkœ,œidœ:œGroupNode-OjoFeœ,œinputTypesœ:[œMessageœ],œproxyœ:{œfieldœ:œinput_valueœ,œidœ:œCreateHeaders-MKjxJœ},œtypeœ:œstrœ}","data":{"targetHandle":{"fieldName":"input_value_CreateHeaders-Jw5wk","id":"GroupNode-OjoFe","inputTypes":["Message"],"proxy":{"field":"input_value","id":"CreateHeaders-MKjxJ"},"type":"str"},"sourceHandle":{"dataType":"TextInput","id":"TextInput-MSS8x","name":"text","output_types":["Message"]}},"id":"reactflow__edge-TextInput-MSS8x{œdataTypeœ:œTextInputœ,œidœ:œTextInput-MSS8xœ,œnameœ:œtextœ,œoutput_typesœ:[œMessageœ]}-GroupNode-OjoFe{œfieldNameœ:œinput_value_CreateHeaders-Jw5wkœ,œidœ:œGroupNode-OjoFeœ,œinputTypesœ:[œMessageœ],œproxyœ:{œfieldœ:œinput_valueœ,œidœ:œCreateHeaders-MKjxJœ},œtypeœ:œstrœ}","selected":false},{"source":"ChatInput-ZoJNe","sourceHandle":"{œdataTypeœ:œChatInputœ,œidœ:œChatInput-ZoJNeœ,œnameœ:œmessageœ,œoutput_typesœ:[œMessageœ]}","target":"GroupNode-OjoFe","targetHandle":"{œfieldNameœ:œuser_prompt_Prompt-mw3zbœ,œidœ:œGroupNode-OjoFeœ,œinputTypesœ:[œMessageœ,œTextœ],œproxyœ:{œfieldœ:œuser_promptœ,œidœ:œPrompt-1FJ5Cœ},œtypeœ:œstrœ}","data":{"targetHandle":{"fieldName":"user_prompt_Prompt-mw3zb","id":"GroupNode-OjoFe","inputTypes":["Message","Text"],"proxy":{"field":"user_prompt","id":"Prompt-1FJ5C"},"type":"str"},"sourceHandle":{"dataType":"ChatInput","id":"ChatInput-ZoJNe","name":"message","output_types":["Message"]}},"id":"reactflow__edge-ChatInput-ZoJNe{œdataTypeœ:œChatInputœ,œidœ:œChatInput-ZoJNeœ,œnameœ:œmessageœ,œoutput_typesœ:[œMessageœ]}-GroupNode-OjoFe{œfieldNameœ:œuser_prompt_Prompt-mw3zbœ,œidœ:œGroupNode-OjoFeœ,œinputTypesœ:[œMessageœ,œTextœ],œproxyœ:{œfieldœ:œuser_promptœ,œidœ:œPrompt-1FJ5Cœ},œtypeœ:œstrœ}"},{"source":"GroupNode-OjoFe","sourceHandle":"{œdataTypeœ:œGroupNodeœ,œidœ:œGroupNode-OjoFeœ,œnameœ:œOpenAIModel-BxXKu_text_outputœ,œoutput_typesœ:[œMessageœ]}","target":"ChatOutput-mjrhT","targetHandle":"{œfieldNameœ:œinput_valueœ,œidœ:œChatOutput-mjrhTœ,œinputTypesœ:[œMessageœ],œtypeœ:œstrœ}","data":{"targetHandle":{"fieldName":"input_value","id":"ChatOutput-mjrhT","inputTypes":["Message"],"type":"str"},"sourceHandle":{"dataType":"GroupNode","id":"GroupNode-OjoFe","name":"OpenAIModel-BxXKu_text_output","output_types":["Message"]}},"id":"reactflow__edge-GroupNode-OjoFe{œdataTypeœ:œGroupNodeœ,œidœ:œGroupNode-OjoFeœ,œnameœ:œOpenAIModel-BxXKu_text_outputœ,œoutput_typesœ:[œMessageœ]}-ChatOutput-mjrhT{œfieldNameœ:œinput_valueœ,œidœ:œChatOutput-mjrhTœ,œinputTypesœ:[œMessageœ],œtypeœ:œstrœ}"},{"source":"TextInput-MSS8x","sourceHandle":"{œdataTypeœ:œTextInputœ,œidœ:œTextInput-MSS8xœ,œnameœ:œtextœ,œoutput_typesœ:[œMessageœ]}","target":"GroupNode-zl0DH","targetHandle":"{œfieldNameœ:œinput_value_CreateHeaders-Jw5wkœ,œidœ:œGroupNode-zl0DHœ,œinputTypesœ:[œMessageœ],œproxyœ:{œfieldœ:œinput_valueœ,œidœ:œCreateHeaders-zkVPzœ},œtypeœ:œstrœ}","data":{"targetHandle":{"fieldName":"input_value_CreateHeaders-Jw5wk","id":"GroupNode-zl0DH","inputTypes":["Message"],"proxy":{"field":"input_value","id":"CreateHeaders-zkVPz"},"type":"str"},"sourceHandle":{"dataType":"TextInput","id":"TextInput-MSS8x","name":"text","output_types":["Message"]}},"id":"reactflow__edge-TextInput-MSS8x{œdataTypeœ:œTextInputœ,œidœ:œTextInput-MSS8xœ,œnameœ:œtextœ,œoutput_typesœ:[œMessageœ]}-GroupNode-zl0DH{œfieldNameœ:œinput_value_CreateHeaders-Jw5wkœ,œidœ:œGroupNode-zl0DHœ,œinputTypesœ:[œMessageœ],œproxyœ:{œfieldœ:œinput_valueœ,œidœ:œCreateHeaders-zkVPzœ},œtypeœ:œstrœ}"},{"source":"GroupNode-zl0DH","sourceHandle":"{œdataTypeœ:œGroupNodeœ,œidœ:œGroupNode-zl0DHœ,œnameœ:œOpenAIModel-BxXKu_text_outputœ,œoutput_typesœ:[œMessageœ]}","target":"GroupNode-OjoFe","targetHandle":"{œfieldNameœ:œcontext_Prompt-HrBgEœ,œidœ:œGroupNode-OjoFeœ,œinputTypesœ:[œMessageœ,œTextœ],œproxyœ:{œfieldœ:œcontextœ,œidœ:œPrompt-XM6Ztœ},œtypeœ:œstrœ}","data":{"targetHandle":{"fieldName":"context_Prompt-HrBgE","id":"GroupNode-OjoFe","inputTypes":["Message","Text"],"proxy":{"field":"context","id":"Prompt-XM6Zt"},"type":"str"},"sourceHandle":{"dataType":"GroupNode","id":"GroupNode-zl0DH","name":"OpenAIModel-BxXKu_text_output","output_types":["Message"]}},"id":"reactflow__edge-GroupNode-zl0DH{œdataTypeœ:œGroupNodeœ,œidœ:œGroupNode-zl0DHœ,œnameœ:œOpenAIModel-BxXKu_text_outputœ,œoutput_typesœ:[œMessageœ]}-GroupNode-OjoFe{œfieldNameœ:œcontext_Prompt-HrBgEœ,œidœ:œGroupNode-OjoFeœ,œinputTypesœ:[œMessageœ,œTextœ],œproxyœ:{œfieldœ:œcontextœ,œidœ:œPrompt-XM6Ztœ},œtypeœ:œstrœ}"}],"viewport":{"x":494.21003467238774,"y":-66.39848527652896,"zoom":0.17359608608169547}},"user_id":"027760a5-eeff-4518-a6d4-42fbacd6beeb","folder_id":"988bb63d-5695-4fb3-a60a-3290e88cd510"}